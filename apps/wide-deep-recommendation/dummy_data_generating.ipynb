{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide & Deep Recommendation for large scale data - Dummy Data Generation\n",
    "Due to the difficulty in downloading [Twitter Recsys Challenge 2021 dataset](https://recsys-twitter.com/data/show-downloads#), you can use this notebook to generate a pseudo dataset with the same structure to test the demo. After the dataset is generated, you can run [feature_engineering.ipynb](./feature_engineering.ipynb) and then [train.ipynb](./train.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import the necessary packages for cluster initialization and generating data on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType\n",
    "from bigdl.friesian.feature import FeatureTable\n",
    "from bigdl.orca import init_orca_context, stop_orca_context, OrcaContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please specify the size of the generated dataset and the path of HDFS/local directory here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records=1000000\n",
    "output_path=\"hdfs://hdfs_addr/recsys/dummy_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the environment on the YARN cluster. You simply need to prepare the Python environment on the driver node with [Anaconda](https://www.anaconda.com/products/individual) and BigDL will automatically distribute and prepare the environment for you across the cluster.\n",
    "Besides, you can specify the allocated resources for this application during the initialization, including the number of nodes, cores and the amount of memory to use, etc. BigDL provides detailed guidance to be easily deployed on [Hadoop/YARN](https://bigdl.readthedocs.io/en/latest/doc/UserGuide/hadoop.html) or [K8S](https://bigdl.readthedocs.io/en/latest/doc/UserGuide/k8s.html) clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing orca context\n",
      "Current pyspark location is : /root/anaconda3/envs/test/lib/python3.7/site-packages/pyspark/__init__.py\n",
      "Initializing SparkContext for yarn-client mode\n",
      "Start to pack current python env\n",
      "Collecting packages...\n",
      "Packing environment at '/root/anaconda3/envs/test' to '/tmp/tmp2l55p07m/python_env.tar.gz'\n",
      "[########################################] | 100% Completed | 14.6s\n",
      "Packing has been completed: /tmp/tmp2l55p07m/python_env.tar.gz\n",
      "pyspark_submit_args is: --master yarn --deploy-mode client --archives /tmp/tmp2l55p07m/python_env.tar.gz#python_env --driver-cores 4 --driver-memory 12g --num-executors 4 --executor-cores 8 --executor-memory 16g --driver-class-path /root/anaconda3/envs/test/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.0.0-jar-with-dependencies.jar:/root/anaconda3/envs/test/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-2.0.0-jar-with-dependencies.jar:/root/anaconda3/envs/test/lib/python3.7/site-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_2.4.6-2.0.0-jar-with-dependencies.jar pyspark-shell\n",
      "2022-07-12 10:59:57 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-07-12 10:59:57 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-12 11:01:00,641 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-07-12 11:01:00,642 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-07-12 11:01:00,643 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-07-12 11:01:00,643 Thread-5 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "22-07-12 11:01:00 [Thread-5] INFO  Engine$:121 - Auto detect executor number and executor cores number\n",
      "22-07-12 11:01:00 [Thread-5] INFO  Engine$:123 - Executor number is 4 and executor cores number is 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=1\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=224\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_MWAIT_HINTS=0\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=8M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=56\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_USE_YIELD=1\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %i thread %n bound to OS proc set {%A}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEBUG=disabled\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='1'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=8M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-07-12 11:01:01 [Thread-5] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 28\n",
      "2022-07-12 11:01:01 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.\n",
      "22-07-12 11:01:01 [Thread-5] INFO  Engine$:446 - Find existing spark context. Checking the spark conf...\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  Sample\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor\n",
      "cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity\n",
      "BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity\n"
     ]
    }
   ],
   "source": [
    "OrcaContext.log_output = True\n",
    "\n",
    "executor_cores = 8\n",
    "num_executor = 4\n",
    "executor_memory = \"16g\"\n",
    "driver_cores = 4\n",
    "driver_memory = \"12g\"\n",
    "conf = {\"spark.network.timeout\": \"10000000\",\n",
    "        \"spark.sql.broadcastTimeout\": \"7200\",\n",
    "        \"spark.sql.shuffle.partitions\": \"2000\",\n",
    "        \"spark.locality.wait\": \"0s\",\n",
    "        \"spark.sql.crossJoin.enabled\": \"true\",\n",
    "        \"spark.task.cpus\": \"1\",\n",
    "        \"spark.executor.heartbeatInterval\": \"200s\",\n",
    "        \"spark.driver.maxResultSize\": \"40G\",\n",
    "        \"spark.eventLog.enabled\": \"true\",\n",
    "        \"spark.app.name\": \"recsys-dummy-data-generation\",\n",
    "        \"spark.debug.maxToStringFields\":\"100\"}\n",
    "sc = init_orca_context(\"yarn\", cores=executor_cores,\n",
    "                       num_nodes=num_executor, memory=executor_memory,\n",
    "                       driver_cores=driver_cores, driver_memory=driver_memory,\n",
    "                       conf=conf)\n",
    "spark = OrcaContext.get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the struct and the random range for generating pseudo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"test_tokens\", StringType(), True),\n",
    "                     StructField(\"hashtags\", StringType(), True),\n",
    "                     StructField(\"tweet_id\", StringType(), True),\n",
    "                     StructField(\"present_media\", StringType(), True),\n",
    "                     StructField(\"present_links\", StringType(), True),\n",
    "                     StructField(\"present_domains\", StringType(), True),\n",
    "                     StructField(\"tweet_type\", StringType(), True),\n",
    "                     StructField(\"language\", StringType(), True),\n",
    "                     StructField(\"tweet_timestamp\", LongType(), True),\n",
    "                     StructField(\"engaged_with_user_id\", StringType(), True),\n",
    "                     StructField(\"engaged_with_user_follower_count\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"engaged_with_user_following_count\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"engaged_with_user_is_verified\",\n",
    "                                 BooleanType(), True),\n",
    "                     StructField(\n",
    "                         \"engaged_with_user_account_creation\", LongType(), True),\n",
    "                     StructField(\"enaging_user_id\", StringType(), True),\n",
    "                     StructField(\"enaging_user_follower_count\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"enaging_user_following_count\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"enaging_user_is_verified\",\n",
    "                                 BooleanType(), True),\n",
    "                     StructField(\"enaging_user_account_creation\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"engagee_follows_engager\",\n",
    "                                 StringType(), True),\n",
    "                     StructField(\"reply_timestamp\", LongType(), True),\n",
    "                     StructField(\"retweet_timestamp\", LongType(), True),\n",
    "                     StructField(\"retweet_with_comment_timestamp\",\n",
    "                                 LongType(), True),\n",
    "                     StructField(\"like_timestamp\", LongType(), True)])\n",
    "id_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\",\n",
    "           \"9\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\",\n",
    "           \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\",\n",
    "           \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "media_list = [\"Photo\", \"Video\", \"GIF\"]\n",
    "tweet_list = [\"Retweet\", \"Quote\", \"TopLevel\"]\n",
    "language_list = ['DE8A3755FCEDC549A408D7B1EB1A2C9F', '9D831A0F3603A54732CCBDBF291D17B7', 'D922D8FEA3EFAD3200455120B75BCEB8', 'F9D8F1DB5A398E1225A2C42E34A51DF6', 'A5CFB818D79497B482B7225887DBD3AD', '2573A3CF633EBE6932A1E1010D5CD213', 'C1E99BF67DDA2227007DE8038FE32470', '477ED2ED930405BF1DBF13F9BF973434', '8729EBF694C3DAF61208A209C2A542C8', '10C6C994C2AD434F9D49D4BE9CFBC613', '89CE0912454AFE0A1B959569C37A5B8F', '914074E75CB398B5A2D81E1A51818CAA', '9A78FC330083E72BE0DD1EA92656F3B5', 'E84BE2C963852FB065EE827F41A0A304', '23686A079CA538645BF6118A1EF51C8B', '7D11A7AA105DAB4D6799AF863369DB9C', '9FCF19233EAD65EA6E32C2E6DC03A444', '313ECD3A1E5BB07406E4249475C2D6D6', '159541FA269CA8A9CDB93658CAEC4CA2', 'CDE47D81F953D800F760F1DE8AA754BA', '2F548E5BE0D7F678E72DDE31DFBEF8E7', 'F33767F7D7080003F403FDAB34FEB755', '5F152815982885A996841493F2757D91', 'B0FA488F2911701DD8EC5B1EA5E322D8', '5B210378BE9FFA3C90818C43B29B466B', '1F73BB863A39DB62B4A55B7E558DB1E8', '3EA57373381A56822CBBC736169D0145', '00304D7356D6C64481190D708D8F739C', 'BAC6A3C2E18C26A77C99B41ECE1C738D', 'DA13A5C3763C212D9D68FC69102DE5E5', '7E18F69967284BB0601E88A114B8F7A9', '5A0759FB938B1D9B1E08B7A3A14F1042',\n",
    "                 'A3E4360031A7E05E9279F4D504EE18DD', '3AB05D6A4045A6C37D3E4566CFDFFE26', '5B6973BEB05212E396F3F2DC6A31B71C', 'BF04E736C599E9DE22F39F1DC157E1F1', '838A92D9F7EB57FB4A8B0C953A80C7EB', 'C41F6D723AB5D14716D856DF9C000DED', '37342508F52BF4B62CCE3BA25460F9EB', 'DC5C9FB3F0B3B740BAEE4F6049C2C7F1', '310ECD7D1E42216E3C1B31EFDDFC72A7', '99CA116BF6AA65D70F3C78BEBADC51F0', '3DF931B225B690508A63FD24133FA0E2', 'E7F038DE3EAD397AEC9193686C911677', '3228B1FB4BC92E81EF2FE35BDA86C540', 'F70598172AC4514B1E6818EA361AD580', '59BE899EB83AAA19878738040F6828F0', 'B4DC2F82961F1263E90DF7A942CCE0B2', '488B32D24BD4BB44172EB981C1BCA6FA', 'B8B04128918BBF54E2E178BFF1ABA833', '678E280656F6A0C0C23D5DFD46B85C14', '4B55C45CD308068E4D0913DEF1043AD6', '440116720BC3A7957E216A77EE5C18CF', '0BB2C843174730BA7D958C98B763A797', '7F4FAB1EB12CD95EDCD9DB2A6634EFCE', '105008E45831ADE8AF1DB888319F422A', '6B90065EA806B8523C0A6E56D7A961B2', '41776FB50B812A6775C2F8DEC92A9779', 'D7C16BC3C9A5A633D6A3043A567C95A6', '4CA37504EF8BA4352B03DCBA50E98A45', '7168CE9B777B76E4069A538DC5F28B6F', 'E6936751CBF4F921F7DE1AEF33A16ED0', '8C64085F46CD49FA5C80E72A35845185', '6744F8519308FD72D8C47BD45186303C', 'CF304ED3CFC1ADD26720B97B39900FFD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_record(random_seed):\n",
    "    random.seed(random_seed)\n",
    "    test_tokens = \"\\t\".join([str(random.randint(1, 1000))\n",
    "                             for i in range(random.randint(1, 10))])\n",
    "    hashtags = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
    "                          for i in range(random.randint(0, 50))])\n",
    "    tweet_id = \"\".join(random.choices(id_list, k=32))\n",
    "    present_media = \"\\t\".join(random.choices(\n",
    "        media_list, k=random.randint(0, 9)))\n",
    "    present_links = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
    "                               for i in range(random.randint(0, 10))])\n",
    "    present_domains = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
    "                                 for i in range(random.randint(0, 10))])\n",
    "    tweet_type = random.choices(tweet_list)[0]\n",
    "    language = random.choices(language_list)[0]\n",
    "    tweet_timestamp = random.randint(946656000, 1609430400)\n",
    "    engaged_with_user_id = \"\".join(random.choices(id_list, k=32))\n",
    "    engaged_with_user_follower_count = random.randint(0, 10000)\n",
    "    engaged_with_user_following_count = random.randint(0, 10000)\n",
    "    engaged_with_user_is_verified = bool(random.getrandbits(1))\n",
    "    engaged_with_user_account_creation = random.randint(946656000, 1609430400)\n",
    "    enaging_user_id = \"\".join(random.choices(id_list, k=32))\n",
    "    enaging_user_follower_count = random.randint(0, 10000)\n",
    "    enaging_user_following_count = random.randint(0, 10000)\n",
    "    enaging_user_is_verified = bool(random.getrandbits(1))\n",
    "    enaging_user_account_creation = random.randint(946656000, 1609430400)\n",
    "    engagee_follows_engager = bool(random.getrandbits(1))\n",
    "    reply = bool(random.getrandbits(1))\n",
    "    reply_timestamp = random.randint(946656000, 1609430400) if reply else None\n",
    "    retweet = bool(random.getrandbits(1))\n",
    "    retweet_timestamp = random.randint(\n",
    "        946656000, 1609430400) if retweet else None\n",
    "    comment = bool(random.getrandbits(1))\n",
    "    retweet_with_comment_timestamp = random.randint(\n",
    "        946656000, 1609430400) if comment else None\n",
    "    like = bool(random.getrandbits(1))\n",
    "    like_timestamp = random.randint(946656000, 1609430400) if like else None\n",
    "    return (test_tokens, hashtags, tweet_id, present_media, present_links, present_domains,\n",
    "            tweet_type, language, tweet_timestamp, engaged_with_user_id,\n",
    "            engaged_with_user_follower_count, engaged_with_user_following_count,\n",
    "            engaged_with_user_is_verified, engaged_with_user_account_creation,\n",
    "            enaging_user_id, enaging_user_follower_count, enaging_user_following_count,\n",
    "            enaging_user_is_verified, enaging_user_account_creation,\n",
    "            engagee_follows_engager, reply_timestamp, retweet_timestamp,\n",
    "            retweet_with_comment_timestamp, like_timestamp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Spark RDD to generate a data list and then convert it to a FeatureTable for storing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+---------------+--------------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+---------------------------+----------------------------+------------------------+-----------------------------+-----------------------+---------------+-----------------+------------------------------+--------------+\n",
      "|         test_tokens|            hashtags|            tweet_id|       present_media|       present_links|     present_domains|tweet_type|            language|tweet_timestamp|engaged_with_user_id|engaged_with_user_follower_count|engaged_with_user_following_count|engaged_with_user_is_verified|engaged_with_user_account_creation|     enaging_user_id|enaging_user_follower_count|enaging_user_following_count|enaging_user_is_verified|enaging_user_account_creation|engagee_follows_engager|reply_timestamp|retweet_timestamp|retweet_with_comment_timestamp|like_timestamp|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+---------------+--------------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+---------------------------+----------------------------+------------------------+-----------------------------+-----------------------+---------------+-----------------+------------------------------+--------------+\n",
      "|777\t912\t431\t42\t26...|ESAHLWIARM9WZTWBQ...|XYHYTSQ6JFY66N53I...|                    |5NXKCYKEMS80JXOMM...|D0MH81BSY3V1P0FVE...|     Quote|7168CE9B777B76E40...|     1160723533|2GUIMEJX8EH07Z1Y1...|                            5235|                             3303|                        false|                        1492129905|IKOHIG6J492ITVGCR...|                       8558|                        1434|                   false|                   1268674495|                   true|     1078464176|       1010003473|                          null|          null|\n",
      "|         583\t868\t822|24RHD7HWELRP9SL3B...|W18VI6Y7F8IBY2L6M...|GIF\tGIF\tGIF\tVideo...|WRL59TAREJ90RSGCI...|42BQKZLWKHE22NU06...|   Retweet|6744F8519308FD72D...|     1602729385|6SZVN5PFGZKXEDPAP...|                            9525|                             5239|                         true|                        1582623035|SBOTW5JUX1BM2F1CL...|                       3640|                        3832|                   false|                   1603097251|                  false|           null|             null|                    1261991340|          null|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+---------------+--------------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+---------------------------+----------------------------+------------------------+-----------------------------+-----------------------+---------------+-----------------+------------------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size =  799128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set size =  200872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save data finished\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(num_records))\n",
    "dummy_data_rdd = rdd.map(generate_record)\n",
    "df = FeatureTable(spark.createDataFrame(dummy_data_rdd, schema))\n",
    "print(df.show(2))\n",
    "\n",
    "train_df, test_df = df.random_split([0.8, 0.2])\n",
    "print('train set size = ', train_df.size())\n",
    "print('test set size = ', test_df.size())\n",
    "\n",
    "train_df.write_parquet(os.path.join(output_path, 'train'))\n",
    "test_df.write_parquet(os.path.join(output_path, 'test'))\n",
    "print('Save data finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping orca context\n"
     ]
    }
   ],
   "source": [
    "stop_orca_context()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('doc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "222bb267707f992669cc93d2571c9bf629e27e35fcbc8dd653f1f324074335d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
