{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/intel-analytics/BigDL/blob/main/python/chronos/colab-notebook/howto/how_to_export_torchscript_files.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the TorchScript model files to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When a forecaster is developed, we can save the forecaster and the whole forecasting pipeline (including data preprocessing, forecasting and data postprocessing) as torchscript by calling `export_torchscript_file`. In this guidance, **we demonstrate how to export these torchscript model files to disk** in detail.\n",
    "\n",
    "We will take `TCNForecaster` and nyc_taxi dataset as an example in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we begin, we need to install Chronos if it isn’t already available, we choose to use pytorch as deep learning backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre --upgrade bigdl-chronos[pytorch]\n",
    "# fix conflict with google colab\n",
    "!pip uninstall -y torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📝**Note**\n",
    "> \n",
    "> - Although Chronos supports inferencing on a cluster, the method to export model files can only be used when forecaster is a non-distributed version.\n",
    "> - Only pytorch backend deep learning forecasters support jit acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecaster preparation\n",
    "\n",
    "Before the exporting process, a forecaster should be created and trained. The training process is introduced in the previous guidance [Train forcaster on single node](https://bigdl.readthedocs.io/en/latest/doc/Chronos/Howto/how_to_train_forecaster_on_one_node.html) in detail, therefore we directly create and train a `TCNForecaster` based on the nyc taxi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# scaler perparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# data preparation\n",
    "def get_data():\n",
    "    from bigdl.chronos.data import get_public_dataset\n",
    "    \n",
    "\n",
    "    # load the nyc taxi datset\n",
    "    tsdata_train, tsdata_val, tsdata_test = get_public_dataset(name='nyc_taxi')\n",
    "\n",
    "    # export test data to csv for future usage when deploying forecasting pipeline\n",
    "    # make sure to set `index=False` to make the saved data have the same structure as original data\n",
    "    tsdata_test.df.to_csv(\"deployment_data.csv\", index=False)\n",
    "    \n",
    "    tsdata_train.scale(scaler, fit=True)\n",
    "\n",
    "    return tsdata_train, tsdata_val, tsdata_test\n",
    "\n",
    "# trained forecaster preparation\n",
    "def get_trained_forecaster(train_data):\n",
    "    from bigdl.chronos.forecaster import TCNForecaster\n",
    "    # create a TCNForecaster\n",
    "    forecaster = TCNForecaster(past_seq_len=48,\n",
    "                               future_seq_len=1,\n",
    "                               input_feature_num=1,\n",
    "                               output_feature_num=1)\n",
    "    \n",
    "    # train the forecaster on the taining data\n",
    "    forecaster.fit(train_data)\n",
    "    return forecaster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the torchscript model files\n",
    "\n",
    "When a trained forecaster is ready and forecaster is a non-distributed version, we provide with `export_torchscript_file` method to export the torchscript model files to disk. There are 2 parameters you may want to specify: `dirname` is the location to save the torchscript files, if omitted, the model will be saved at \"fp32_torchscript\" directory under current directory, and `quantized_dirname` is the location to save the quantized torchscript model files. But the quantization of jit model is not supported yet, so we set it to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# get data for training and testing and validating\n",
    "tsdata_train, tsdata_val, tsdata_test = get_data()\n",
    "train_data = tsdata_train.to_torch_data_loader(roll=True, lookback=48, horizon=1)\n",
    "\n",
    "# get a trained forecaster\n",
    "forecaster = get_trained_forecaster(train_data)\n",
    "\n",
    "# create a directory to save torchscript files\n",
    "dirname = Path(\"torchscript_files\")\n",
    "dirname.mkdir(exist_ok=True)\n",
    "ckpt_name = dirname / \"fp32_torch_script\"\n",
    "\n",
    "# export the torchscript files\n",
    "forecaster.export_torchscript_file(dirname=ckpt_name, quantized_dirname=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📝**Note**\n",
    "> \n",
    "> - When `export_torchscript_file` is called, the forecaster will automatically build an jit session with default settings. So you can directly call this method without calling `predict_with_jit` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files exported will be saved at `torchscript_files` directory. \n",
    "\n",
    "There are 2 files in each subdirectory:\n",
    "\n",
    "- `nano_model_meta.yml`: meta information of the saved model checkpoint\n",
    "- `ckpt.pth`: JIT model checkpoint for general use, describes model structure\n",
    "\n",
    "You only need to take `ckpt.pth` file for futher usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the whole forecasting pipeline\n",
    "\n",
    "You could also save the whole forecasting pipeline to torchscript files, so it could be used without Python environment, and it's a great help when you want to deploy the forecasting pipeline in other languages like C++. We provide a parameter called `save_pipeline`, if it is set to `True`, the pipeline will be saved at \"`dirname`/chronos_forecasting_pipeline.pt\" by default, and you need to specify another 2 parameters (`tsdata` and `drop_dt_col`), please check [API doc](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Chronos/forecasters.html#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.export_torchscript_file) to know more about these parameters and current limitations of this feature. The following code shows how to export the forecasting pipeline to disk and how to use it during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export forecasting pipeline\n",
    "forecaster.export_torchscript_file(dirname=ckpt_name,\n",
    "                                   quantized_dirname=None,\n",
    "                                   save_pipeline=True,\n",
    "                                   tsdata=tsdata_train,\n",
    "                                   drop_dt_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deployment example in Python\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# load data from csv file\n",
    "deployment_df = pd.read_csv(\"deployment_data.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# drop the datetime column because we specified `drop_dtcol=True` when exporting the pipeline\n",
    "# now the data structure is same as data used in developing\n",
    "deployment_df.drop(columns=\"timestamp\", inplace=True)\n",
    "\n",
    "# create input tensor\n",
    "input_tensor = torch.from_numpy(deployment_df.values).type(torch.float64)\n",
    "\n",
    "# load the forecasting pipeline\n",
    "forecasting_pipeline_path = ckpt_name / \"chronos_forecasting_pipeline.pt\"\n",
    "forecasting_pipeline = torch.jit.load(forecasting_pipeline_path)\n",
    "\n",
    "# run pipeline\n",
    "output_tensor = forecasting_pipeline.forward(input_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the result, let's compare the output with the result using Chronos API. How to process data during deployment is shown in [How to process data in production environment](https://zhaojie-doc.readthedocs.io/en/export_forecasting_pipeline_howto/doc/Chronos/Howto/how_to_process_data_in_production_environment.html), you could refer to it for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the result with original pipeline using Chronos API\n",
    "\n",
    "# deployment using Chronos API\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "from bigdl.chronos.data import TSDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_df = pd.read_csv(\"deployment_data.csv\", parse_dates=[\"timestamp\"])\n",
    "tsdata_test = TSDataset.from_pandas(test_df,\n",
    "                                    dt_col=\"timestamp\",\n",
    "                                    target_col=\"value\",\n",
    "                                    repair=False,\n",
    "                                    deploy_mode=True)\n",
    "\n",
    "# preprocessing\n",
    "# \"scaler\" should be the scaler used in development\n",
    "tsdata_test.scale(scaler) \\\n",
    "           .roll(lookback=48, horizon=1)\n",
    "input_data = tsdata_test.to_numpy()\n",
    "\n",
    "# forecasting\n",
    "forecasting_output = forecaster.predict_with_jit(input_data)\n",
    "\n",
    "# postprocessing\n",
    "postprocess_output = tsdata_test.unscale_numpy(forecasting_output)\n",
    "\n",
    "# compare the results\n",
    "assert_array_almost_equal(output_tensor.numpy(), postprocess_output, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment in C++\n",
    "\n",
    "Since a more common case when deploying the pipeline is in C++, we provide an example code snip to show the core deployment workflow in C++ using libtorch APIs. You could refer to [installation guide](https://pytorch.org/cppdocs/installing.html) to install libtorch, and more information of APIs is available at [libtorch API doc](https://pytorch.org/cppdocs/api/library_root.html).\n",
    "\n",
    "```C++\n",
    "// core deployment workflow example in C++\n",
    "\n",
    "#include <torch/torch.h>\n",
    "#include <torch/script.h>\n",
    "\n",
    "// Create input tensor from your data, you should implement this function.\n",
    "// The data to create input tensor should have the same format as the data used in developing.\n",
    "// If you sepcified drop_dt_col=True when exporting the pipeline, you should skip the\n",
    "// datatime column here to keep the same structure as the developing data.\n",
    "torch::Tensor input_tensor = create_input_tensor(data);\n",
    "\n",
    "// load the forecasting pipeline\n",
    "torch::jit::script::Module forecasting;\n",
    "forecasting = torch::jit::load(forecasting_path);\n",
    "\n",
    "// run pipeline\n",
    "torch::Tensor output = forecasting.forward(input_tensor).toTensor();\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('chronos-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edcf3a1384e0c635c6a1374bbfb4bfc5de419d28ba5059a4c674cfed4e784c9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
