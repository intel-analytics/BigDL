{"cells":[{"cell_type":"code","source":["import urllib.request\nimport zipfile\n\n# download and unzip dataset\nurl = \"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n# make sure directory (/dbfs/FileStore/datasets) is already created in your dbfs\ndata_save_path = \"/dbfs/FileStore/datasets/ml-latest-small.zip\"\nurllib.request.urlretrieve(url, data_save_path)\n\ndist_path = \"/dbfs/FileStore/datasets/\"\nwith zipfile.ZipFile(data_save_path, 'r') as zip_ref:\n    zip_ref.extractall(dist_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7203c07a-a98e-458b-88af-794805f28756"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import math\nimport argparse\nimport os\nimport random\n\nfrom bigdl.orca import init_orca_context, stop_orca_context, OrcaContext\nfrom bigdl.orca.learn.tf2.estimator import Estimator\nfrom pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n\n\ndef build_model(num_users, num_items, class_num, layers=[20, 10], include_mf=True, mf_embed=20):\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, concatenate, multiply\n\n    num_layer = len(layers)\n    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n\n    mlp_embed_user = Embedding(input_dim=num_users, output_dim=int(layers[0] / 2),\n                               input_length=1)(user_input)\n    mlp_embed_item = Embedding(input_dim=num_items, output_dim=int(layers[0] / 2),\n                               input_length=1)(item_input)\n\n    user_latent = Flatten()(mlp_embed_user)\n    item_latent = Flatten()(mlp_embed_item)\n\n    mlp_latent = concatenate([user_latent, item_latent], axis=1)\n    for idx in range(1, num_layer):\n        layer = Dense(layers[idx], activation='relu',\n                      name='layer%d' % idx)\n        mlp_latent = layer(mlp_latent)\n\n    if include_mf:\n        mf_embed_user = Embedding(input_dim=num_users,\n                                  output_dim=mf_embed,\n                                  input_length=1)(user_input)\n        mf_embed_item = Embedding(input_dim=num_items,\n                                  output_dim=mf_embed,\n                                  input_length=1)(item_input)\n        mf_user_flatten = Flatten()(mf_embed_user)\n        mf_item_flatten = Flatten()(mf_embed_item)\n\n        mf_latent = multiply([mf_user_flatten, mf_item_flatten])\n        concated_model = concatenate([mlp_latent, mf_latent], axis=1)\n        prediction = Dense(class_num, activation='softmax', name='prediction')(concated_model)\n    else:\n        prediction = Dense(class_num, activation='softmax', name='prediction')(mlp_latent)\n\n    model = tf.keras.Model([user_input, item_input], prediction)\n    return model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ae48eb2-56e3-4329-9b58-87a28dd774d5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lr = 0.001\nepochs = 5\nbatch_size = 8000\nbackend = \"ray\" # ray or spark\ndata_dir = './'\nmodel_dir = \"/dbfs/FileStore/model/ncf_train/\"\nsave_path = model_dir + \"ncf.h5\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8716b1b-8bda-4a31-8e9a-a95d3f55e3f4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc = init_orca_context(cluster_mode=\"spark-submit\")\n\nspark = OrcaContext.get_spark_session()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04b59868-3a7b-4a43-b5d1-4fa8a92c56c5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["schema = StructType([StructField(\"user\", IntegerType(), False),\n                     StructField(\"item\", IntegerType(), False),\n                     StructField(\"label\", FloatType(), False)])\n\ndata = spark.read.csv(\"dbfs:/FileStore/datasets/ml-latest-small/ratings.csv\", schema, header=True)\n\nmax_user_id = data.agg({\"user\": \"max\"}).collect()[0][\"max(user)\"]\nmax_item_id = data.agg({\"item\": \"max\"}).collect()[0][\"max(item)\"]\n\nnum_users = max_user_id + 1\nnum_items = max_item_id + 1\nclass_num = data.select('label').distinct().count()\n\ntrain, test = data.randomSplit([0.8, 0.2], seed=1)\n\nconfig = {\"lr\": lr}\n\n\ndef model_creator(config):\n    import tensorflow as tf\n\n    model = build_model(num_users, num_items, class_num)\n    print(model.summary())\n    optimizer = tf.keras.optimizers.Adam(config[\"lr\"])\n    model.compile(optimizer=optimizer,\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['sparse_categorical_crossentropy', 'accuracy'])\n    return model\n\n\nsteps_per_epoch = math.ceil(train.count() / batch_size)\nval_steps = math.ceil(test.count() / batch_size)\n\nestimator = Estimator.from_keras(model_creator=model_creator,\n                                 verbose=False,\n                                 config=config,\n                                 backend=backend,\n                                 model_dir=model_dir)\nestimator.fit(train,\n              batch_size=batch_size,\n              epochs=epochs,\n              feature_cols=['user', 'item'],\n              label_cols=['label'],\n              steps_per_epoch=steps_per_epoch,\n              validation_data=test,\n              validation_steps=val_steps)\n\npredictions = estimator.predict(test,\n                                batch_size=batch_size,\n                                feature_cols=['user', 'item'])\nprint(\"Predictions on validation dataset:\")\npredictions.show(5, truncate=False)\n\nprint(\"Saving model to: \", save_path)\nestimator.save(save_path)\n\n# load with estimator.load(save_path)\n# estimator.load(save_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff56791a-2ee4-454f-9b41-57a031c38138"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["stop_orca_context()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10d80c3e-e5be-4b8c-bea9-b4fb98f5780e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"tf_keras_ncf_ new","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3661296061709701}},"nbformat":4,"nbformat_minor":0}
