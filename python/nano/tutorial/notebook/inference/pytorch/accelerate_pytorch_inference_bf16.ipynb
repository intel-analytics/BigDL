{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/accelerate_pytorch_inference_bf16.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Accelerate PyTorch Inference with BF16 mixed precision"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 📝 **Quick Note**\n",
        ">\n",
        "> * `bf16`: `InferenceOptimizer.quantize(model, precision='bf16')`.\n",
        "> * `bf16 + ipex`: `InferenceOptimizer.quantize(model, precision='bf16', use_ipex=True)`\n",
        "> * `bf16 + jit`: `InferenceOptimizer.quantize(model, precision='bf16', accelerator=\"jit\")`\n",
        "> * `bf16 + channels_last`: `InferenceOptimizer.quantize(model, precision='bf16', channels_last=True)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To accelerate the model in bf16 precision, the following dependencies need to be installed first："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for BigDL-Nano\n",
        "!pip install --pre --upgrade bigdl-nano[pytorch]  # install the nightly-bulit version\n",
        "# !source bigdl-nano-init\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 📝 **Note**\n",
        ">\n",
        "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take an [ResNet-18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) pretrained on ImageNet dataset as an example. First, we load the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "model_ft = resnet18(pretrained=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Accelerate the model in bf16 precision, we need import `InferenceOptimizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bigdl.nano.pytorch import InferenceOptimizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 📝 **Note**\n",
        ">\n",
        "> Platforms without hardware acceleration for BFloat16 could lead to bad BFloat16 inference performance. In other word, only Cooper Lake and Sapphire Rapids Xeon processors could reveal the extreme performance.\n",
        ">\n",
        "> All of following methods could be combined as users' wish. For example, `BF16+IPEX+jit+channels_last` is also supported. Automatically searching for the best configurations could be found [here](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/Inference/PyTorch/inference_optimizer_optimize.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BF16\n",
        "\n",
        "Users will have a model that could utilize the mixed precision instructions(e.g., AVX512_bf16, AMX_bf16) with the assistance of `with InferenceOptimizer.get_context(bf16_model):`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.rand(2, 3, 224, 224)\n",
        "bf16_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                         precision='bf16')\n",
        "with InferenceOptimizer.get_context(bf16_model):\n",
        "    y_hat = bf16_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BF16 + IPEX\n",
        "Users will have a model that is optimized by Intel® Extension for PyTorch (within eager mode) and utilize the mixed precision instructions(e.g., AVX512_bf16, AMX_bf16) with the assistance of `with InferenceOptimizer.get_context(bf16_model):`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ipex_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                         precision='bf16',\n",
        "                                         use_ipex=True)\n",
        "with InferenceOptimizer.get_context(ipex_model):\n",
        "    y_hat = ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BF16 + jit\n",
        "\n",
        "Users will have a model that is traced by torch.jit and utilize the mixed precision instructions(e.g., AVX512_bf16, AMX_bf16) with the assistance of `with InferenceOptimizer.get_context(bf16_model):`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jit_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                        precision='bf16',\n",
        "                                        accelerator=\"jit\",\n",
        "                                        input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_model):\n",
        "    y_hat = jit_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 📝 **Note**\n",
        ">\n",
        "> `input_sample` is the parameter for OpenVINO accelerator to know the **shape** of the model input. So both the batch size and the specific values are not important to `input_sample`. If we want our test dataset to consist of images with $224 \\times 224$ pixels, we could use `torch.rand(1, 3, 224, 224)` for `input_sample` here.\n",
        ">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BF16 + channels_last\n",
        "Users will have a model with alternative way of ordering NCHW and utilize the mixed precision instructions(e.g., AVX512_bf16, AMX_bf16) with the assistance of `with InferenceOptimizer.get_context(bf16_model):`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "channels_last_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                                  precision='bf16',\n",
        "                                                  channels_last=True)\n",
        "with InferenceOptimizer.get_context(channels_last_model):\n",
        "    y_hat = channels_last_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accelereate inference using combined method\n",
        "You can use any of the methods mentioned above to combine each other to try to acclerate your model. However, the effect is not like stacking buffs.\n",
        "It is not that the more methods you use, the better. You should try many times to find the best combination of methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# bf16 + IPEX + JIT + channels_last\n",
        "jit_ipex_model = InferenceOptimizer.quantize(model_ft,\n",
        "                                             precision='bf16',\n",
        "                                             accelerator=\"jit\",\n",
        "                                             use_ipex=True,\n",
        "                                             channels_last=True,\n",
        "                                             input_sample=torch.rand(1, 3, 224, 224))\n",
        "with InferenceOptimizer.get_context(jit_ipex_model):\n",
        "    y_hat = jit_ipex_model(x)\n",
        "    predictions = y_hat.argmax(dim=1)\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> 📚 **Related Readings**\n",
        ">\n",
        "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
        "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "chronos",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f7cbcfcf124497a723b2fc91b0dad8cd6ed41af955928289a9d3478af9690021"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
