{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/inference_optimizer_optimize.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Acceleration Method with the Minimum Inference Latency using InferenceOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates how to apply InferenceOptimizer to quickly find acceleration method with the minimum inference latency under specific restrictions or without restrictions for a trained model. \n",
    "In this example, we first train ResNet18 model on the [cats and dogs dataset](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip). Then, by calling `optimize()`, we can obtain all available accelaration combinations provided by BigDL-Nano for inference. By calling `get_best_model()` , we could get the best model under specific restrictions or without restrictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference using Bigdl-nano InferenceOptimizer, the following packages need to be installed first. We recommend you to use [Miniconda](https://docs.conda.io/en/latest/miniconda.html) to prepare the environment and install the following packages in a conda environment. \n",
    "\n",
    "You can create a conda environment by executing:\n",
    "\n",
    "```\n",
    "# \"nano\" is conda environment name, you can use any name you like.\n",
    "conda create -n nano python=3.7 setuptools=58.0.4  \n",
    "conda activate nano\n",
    "!pip install --pre --upgrade bigdl-nano[pytorch,inference]  # install the nightly-bulit version\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize environment variables with script `bigdl-nano-init` installed with bigdl-nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source bigdl-nano-init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, prepare model and dataset. We use a pretrained ResNet18 model and train the model on [cats and dogs dataset](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from bigdl.nano.pytorch import Trainer\n",
    "\n",
    "def accuracy(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return Accuracy()(pred, target)\n",
    "\n",
    "def prepare_model_and_dataset(model_ft, val_size):\n",
    "    DATA_URL = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    if not Path(\"data\").exists():\n",
    "        # download dataset\n",
    "        download_and_extract_archive(url=DATA_URL, download_root=\"data\", remove_finished=True)\n",
    "\n",
    "    data_path = Path(\"data/cats_and_dogs_filtered\")\n",
    "    train_dataset = ImageFolder(data_path.joinpath(\"train\"), transform=train_transform)\n",
    "    val_dataset = ImageFolder(data_path.joinpath(\"validation\"), transform=val_transform)\n",
    "\n",
    "    indices = torch.randperm(len(val_dataset))\n",
    "    val_dataset = Subset(val_dataset, indices=indices[:val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=8, shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    \n",
    "    model_ft.fc = torch.nn.Linear(num_ftrs, 2)\n",
    "    loss_ft = torch.nn.CrossEntropyLoss()\n",
    "    optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=1e-3)\n",
    "\n",
    "    # compile model\n",
    "    model = Trainer.compile(model_ft, loss=loss_ft, optimizer=optimizer_ft, metrics=[accuracy])\n",
    "    trainer = Trainer(max_epochs=1)\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "    \n",
    "    return model, train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "_, train_dataset, val_dataset = prepare_model_and_dataset(model, val_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;_The full definition of function_ `prepare_model_and_dataset` _could be found in the_ [runnable example](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/pytorch/inference_optimizer_optimize.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find acceleration method with the minimum inference latency, you could import `InferenceOptimizer` and call `optimize` method. The `optimize` method will run all possible acceleration combinations and output the result, it will take about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nano.pytorch import InferenceOptimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define metric for accuracy calculation\n",
    "def accuracy(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    return Accuracy()(pred, target)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "\n",
    "# To obtain the latency of single sample, set batch_size=1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1)\n",
    "val_dataloader = DataLoader(val_dataset)\n",
    "\n",
    "optimizer.optimize(model=model,\n",
    "                   training_data=train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=accuracy,\n",
    "                   direction=\"max\",\n",
    "                   thread_num=1,\n",
    "                   latency_sample_num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example output of `optimizer.optimize` is shown below.\n",
    "\n",
    "```\n",
    "==========================Optimization Results==========================\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "|             method             |        status        | latency(ms)  |       accuracy       |\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "|            original            |      successful      |    41.304    |         0.86         |\n",
    "|           fp32_ipex            |      successful      |    38.624    |    not recomputed    |\n",
    "|              bf16              |   lack dependency    |     None     |         None         |\n",
    "|           bf16_ipex            |   lack dependency    |     None     |         None         |\n",
    "|              int8              |      successful      |    23.108    |        0.852         |\n",
    "|            jit_fp32            |    early stopped     |    75.324    |         None         |\n",
    "|         jit_fp32_ipex          |      successful      |    65.829    |    not recomputed    |\n",
    "|  jit_fp32_ipex_channels_last   |    early stopped     |    90.795    |         None         |\n",
    "|         openvino_fp32          |      successful      |    40.322    |    not recomputed    |\n",
    "|         openvino_int8          |      successful      |    3.871     |        0.834         |\n",
    "|        onnxruntime_fp32        |      successful      |    30.08     |    not recomputed    |\n",
    "|    onnxruntime_int8_qlinear    |      successful      |    18.662    |        0.846         |\n",
    "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
    " -------------------------------- ---------------------- -------------- ----------------------\n",
    "Optimization cost 74.2s in total.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📝 **Note**\n",
    "> \n",
    "> When specifying `training_data` parameter, make sure to set batch size of the training data to the same batch size you may want to use in real deploy environment, as the batch size may impact on latency.\n",
    ">\n",
    "> For more information, please refer to the [API Documentation](https://bigdl.readthedocs.io/en/latest/doc/PythonAPI/Nano/pytorch.html#bigdl.nano.pytorch.InferenceOptimizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could call `get_best_model` method to obtain the best model under specific restrictions or without restrictions. Here we get the model with minimal latency when accuracy drop less than 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"When accuracy drop less than 5%, the model with minimal latency is: \", option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you could use the best model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with InferenceOptimizer.get_context(acc_model):\n",
    "    x = next(iter(train_dataloader))[0]\n",
    "    output = acc_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export the best model, you could simply call `save` method and pass the path to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./best_model\"\n",
    "InferenceOptimizer.save(acc_model, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model files will be saved at `./best_model` directory. For each type in the `option` of best model, you only need to take the following files for further usage.\n",
    "\n",
    "- **OpenVINO**\n",
    "    \n",
    "    `ov_saved_model.bin`: Contains the weights and biases binary data of model\n",
    "    \n",
    "    `ov_saved_model.xml`: Model checkpoint for general use, describes model structure\n",
    "\n",
    "- **onnxruntime**\n",
    "\n",
    "    `onnx_saved_model.onnx`: Represents model checkpoint for general use, describes model structure\n",
    "    \n",
    "- **int8**\n",
    "\n",
    "    `best_model.pt`: Represents model optimized by Intel® Neural Compressor\n",
    "\n",
    "- **ipex | channel_last | jit**\n",
    "    \n",
    "    `ckpt.pt`: If `jit` in option, it stores model optimized using just-in-time compilation, otherwise, it stores original model weight by `torch.save(model.state_dict())`.\n",
    "\n",
    "- **Others**\n",
    "    \n",
    "    `saved_weight.pt`: Saved by `torch.save(model.state_dict())`.\n",
    "    \n",
    "    If `bf16` in option, the model weights obtained are bf16 dtype, otherwise, the model weights obtained are fp32 dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📚 **Related Readings**\n",
    "> \n",
    "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
    "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('ruonan_nano')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d347a5dca25745bedb029e46e41f7d6c8c9b5181ecb97033e2e81a7538459254"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
