{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[View the runnable example on GitHub](https://github.com/intel-analytics/BigDL/tree/main/python/nano/tutorial/notebook/inference/openvino/openvino_inference_async.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO Asynchronous Inference using Nano API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `async_predict` method in `OpenVINOModel` class in Nano to do asynchronous inference on an OpenVINO model. It only takes a few lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run asynchronous inference on OpenVINO model with Nano, the following dependencies need to be installed first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for BigDL-Nano\n",
    "!pip install --pre --upgrade bigdl-nano # install the nightly-built version\n",
    "# !source bigdl-nano-init\n",
    "\n",
    "# for OpenVINO\n",
    "!pip install openvino-dev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📝 **Note**\n",
    ">\n",
    "> We recommend to run the commands above, especially `source bigdl-nano-init` before jupyter kernel is started, or some of the optimizations may not take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a [resnet18-xnor-binary-onnx-0001](https://docs.openvino.ai/latest/omz_models_model_resnet18_xnor_binary_onnx_0001.html) model pretrained on ImageNet dataset from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) as an example. First, we download the model using [omz_downloader](https://docs.openvino.ai/latest/omz_tools_downloader.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name resnet18-xnor-binary-onnx-0001 -o ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the model using `OpenVINOModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nano.openvino import OpenVINOModel\n",
    "\n",
    "ov_model = OpenVINOModel(\"model/intel/resnet18-xnor-binary-onnx-0001/FP16-INT1/resnet18-xnor-binary-onnx-0001.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run asynchronous inference on OpenVINO model, **the only change you need to make is to prepare a list of input data and call** `ov_model.async_predict(input_data, num_requests)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = [np.random.randn(1, 3, 224, 224) for i in range(5)]\n",
    "async_results = ov_model.async_predict(input_data=input_data, num_requests=5)\n",
    "for res in async_results:\n",
    "    predictions = res.argmax(axis=1)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📝 **Note**\n",
    "> \n",
    "> `async_predict` accepts multiple groups of input data in a list, and each group of data will be inferenced using an asynchronous infer request, and a list containing the result of each infer request will be retured. If you have multiple groups of input data to inference, `async_predict` will achieve better performance than sync inference using `ov_model(x)`.\n",
    ">\n",
    "> You can specify the number of asynchronous infer request in `num_requests`, if `num_requests` is set to 0, the value will be set automatically to the optimal number.\n",
    ">\n",
    "> In the code above, we have 5 groups of input data and create 5 asynchronous infer requests. When `async_predict` is called, each asynchronous infer request will run inference in a parallel pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 📚 **Related Readings**\n",
    "> \n",
    "> - [How to install BigDL-Nano](https://bigdl.readthedocs.io/en/latest/doc/Nano/Overview/nano.html#install)\n",
    "> - [How to install BigDL-Nano in Google Colab](https://bigdl.readthedocs.io/en/latest/doc/Nano/Howto/install_in_colab.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nano-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcba839278e72a05a80ee7542e90a94f8a25e052dc735aca32801a2a5657bf32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
