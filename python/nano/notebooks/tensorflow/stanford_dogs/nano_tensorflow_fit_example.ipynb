{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef33b643",
   "metadata": {},
   "source": [
    "## BigDL-Nano example on Stanford Dogs dataset\n",
    "---\n",
    "This example illustrates how to apply bigdl-nano optimizations on a image recognition case based on Tensorflow Keras framework. The basic image recognition module is pre-trained EfficientNetB0 from tensorflow.keras.applications and fine-tune it on [Stanford Dogs](http://vision.stanford.edu/aditya86/ImageNetDogs/) image recognition dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ce663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from bigdl.nano.tf.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "381495df",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=224\n",
    "BATCH_SIZE=int(os.environ.get('BATCH_SIZE', 64))\n",
    "DATASET_NAME=\"stanford_dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49885ff",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "---\n",
    "Here we load data from tensorflow_datasets (hereafter TFDS). Stanford Dogs dataset is provided in TFDS as stanford_dogs. It features 20,580 images that belong to 120 classes of dog breeds (12,000 for training and 8,580 for testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a88cd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 03:29:42.207687: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info=tfds.load(\n",
    "    DATASET_NAME, data_dir=\"tensorflow_datasets/\" ,split=[\"train\", \"train\"], with_info=True, as_supervised=True\n",
    ")\n",
    "NUM_CLASSES = ds_info.features[\"label\"].num_classes\n",
    "FIT_STEPS = int(os.environ.get('FIT_STEPS', len(ds_train)/BATCH_SIZE))\n",
    "VAL_STEPS = int(os.environ.get('VAL_STEPS')) if 'VAL_STEPS' in os.environ else None\n",
    "FREEZE_EPOCHS = int(os.environ.get('FREEZE_EPOCHS', 25)) # @param {type: \"slider\", min:8, max:80}\n",
    "UNFREEZE_EPOCHS = int(os.environ.get('UNFREEZE_EPOCHS', 10)) # @param {type: \"slider\", min:8, max:50}\n",
    "NUM_PROCESSES = int(os.environ.get('NUM_PROCESSES', 4))\n",
    "\n",
    "size = (IMG_SIZE, IMG_SIZE)\n",
    "ds_train = ds_train.map(lambda image, label: (tf.image.resize(image, size), label))\n",
    "ds_test = ds_test.map(lambda image, label: (tf.image.resize(image, size), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863af62",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "---\n",
    "To augment the dataset it can beneficial to make augmenter functions: a function that receives an image (a tf.Tensor) and a label and returns a new augmented image and label. By defining functions for each augmentation operation we can easily attach them to datasets and control when they are evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9764283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, label):\n",
    "    return (tf.image.rot90(image, tf.random_uniform_initializer(minval=0, maxval=4)(shape=[], dtype=tf.int32)), label)\n",
    "\n",
    "def flip(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    return (image, label)\n",
    "\n",
    "def contrast(image, label):\n",
    "    return (tf.image.random_contrast(image, 0.7, 1.3), label)\n",
    "\n",
    "def color(image, label):\n",
    "    image = tf.image.random_hue(image, 0.08)\n",
    "    image = tf.image.random_saturation(image, 0.6, 1.6)\n",
    "    image = tf.image.random_brightness(image, 0.05)\n",
    "    image = tf.image.random_contrast(image, 0.7, 1.3)\n",
    "    return (image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe26aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.repeat(50)\n",
    "augmentations = [rotate, flip, contrast, color]\n",
    "\n",
    "for f in augmentations:\n",
    "    ds_train = ds_train.map(f, num_parallel_calls=4)\n",
    "\n",
    "ds_train = ds_train.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439df50",
   "metadata": {},
   "source": [
    "### Prepare inputs\n",
    "---\n",
    "Once we verify the input data and augmentation are working correctly, we prepare dataset for training. The input data are resized to uniform IMG_SIZE. The labels are put into one-hot (a.k.a. categorical) encoding. The dataset is batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fc82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_preprocess(image, label):\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    input_preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "ds_train = ds_train.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(input_preprocess)\n",
    "ds_test = ds_test.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "ds_train = ds_train.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290760a8",
   "metadata": {},
   "source": [
    "### Transfer learning from pre-trained weights\n",
    "---\n",
    "Here we initialize the model with pre-trained ImageNet weights, and we fine-tune it on Stanford Dogs dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb14afbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, learning_rate=1e-2):\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = EfficientNetB0(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "    # Compile\n",
    "    model = Model(inputs, outputs, name=\"EfficientNet\")\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers[-20:]:\n",
    "        if not isinstance(layer, layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca9fea",
   "metadata": {},
   "source": [
    "### Train\n",
    "---\n",
    "Use Model.fit from bigdl.nano.tf.keras for BigDl-Nano tf.keras.\n",
    "\n",
    "This function override tf.keras.Model.fit to add more parameters.\n",
    "\n",
    "\n",
    "Additional parameters:\n",
    "```\n",
    "        :param num_processes:  when num_processes is not None, it specifies how many sub-processes\n",
    "                               to launch to run pseudo-distributed training; when num_processes is None,\n",
    "                               training will run in the current process.\n",
    "                               \n",
    "        :param backend: Use backend 'multiprocessing', 'horovod', 'ray', defaults to None.\n",
    "                        when num_processes is not None, it specifies which backend to use when\n",
    "                       launching sub-processes to run psedu-distributed training; \n",
    "                       when num_processes is None, this parameter takes no effect.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6a071",
   "metadata": {},
   "source": [
    "### Single process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a6bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073741824 bytes == 0x5559d20b4000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa57f702c92 0x7fa57f71b6d8 0x7fa57f76fe3e 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6/187 [..............................] - ETA: 1:10 - loss: 5.4018 - accuracy: 0.0885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x555a1531a000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61/187 [========>.....................] - ETA: 47s - loss: 4.1153 - accuracy: 0.3010"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 4294967296 bytes == 0x555a997de000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/187 [==========================>...] - ETA: 5s - loss: 3.5676 - accuracy: 0.3714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x555ba21de000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 124s 624ms/step - loss: 3.4490 - accuracy: 0.3842 - val_loss: 0.7359 - val_accuracy: 0.7765\n",
      "Epoch 2/25\n",
      " 14/187 [=>............................] - ETA: 1:03 - loss: 2.0583 - accuracy: 0.5301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x555db857e000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 116s 624ms/step - loss: 1.7786 - accuracy: 0.5477 - val_loss: 0.5495 - val_accuracy: 0.8305\n",
      "Epoch 3/25\n",
      "187/187 [==============================] - 114s 611ms/step - loss: 1.4776 - accuracy: 0.5876 - val_loss: 0.5131 - val_accuracy: 0.8407\n",
      "Epoch 4/25\n",
      " 88/187 [=============>................] - ETA: 36s - loss: 1.3816 - accuracy: 0.6133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 34359738368 bytes == 0x5561d9c90000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 113s 604ms/step - loss: 1.3806 - accuracy: 0.6128 - val_loss: 0.4955 - val_accuracy: 0.8424\n",
      "Epoch 5/25\n",
      "187/187 [==============================] - 112s 600ms/step - loss: 1.3206 - accuracy: 0.6279 - val_loss: 0.4596 - val_accuracy: 0.8516\n",
      "Epoch 6/25\n",
      "187/187 [==============================] - 114s 609ms/step - loss: 1.3009 - accuracy: 0.6269 - val_loss: 0.4430 - val_accuracy: 0.8559\n",
      "Epoch 7/25\n",
      "187/187 [==============================] - 113s 604ms/step - loss: 1.2757 - accuracy: 0.6397 - val_loss: 0.4261 - val_accuracy: 0.8605\n",
      "Epoch 8/25\n",
      "187/187 [==============================] - 112s 601ms/step - loss: 1.2479 - accuracy: 0.6482 - val_loss: 0.4161 - val_accuracy: 0.8640\n",
      "Epoch 9/25\n",
      " 43/187 [=====>........................] - ETA: 53s - loss: 1.2393 - accuracy: 0.6403"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 68719476736 bytes == 0x556a1bc90000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 114s 611ms/step - loss: 1.2493 - accuracy: 0.6385 - val_loss: 0.3974 - val_accuracy: 0.8736\n",
      "Epoch 10/25\n",
      "187/187 [==============================] - 113s 604ms/step - loss: 1.2304 - accuracy: 0.6528 - val_loss: 0.3709 - val_accuracy: 0.8747\n",
      "Epoch 11/25\n",
      "187/187 [==============================] - 112s 601ms/step - loss: 1.2482 - accuracy: 0.6461 - val_loss: 0.4429 - val_accuracy: 0.8572\n",
      "Epoch 12/25\n",
      "187/187 [==============================] - 113s 603ms/step - loss: 1.2149 - accuracy: 0.6584 - val_loss: 0.3909 - val_accuracy: 0.8722\n",
      "Epoch 13/25\n",
      "187/187 [==============================] - 113s 603ms/step - loss: 1.2142 - accuracy: 0.6510 - val_loss: 0.3705 - val_accuracy: 0.8786\n",
      "Epoch 14/25\n",
      "187/187 [==============================] - 112s 602ms/step - loss: 1.2133 - accuracy: 0.6555 - val_loss: 0.3943 - val_accuracy: 0.8698\n",
      "Epoch 15/25\n",
      "187/187 [==============================] - 113s 606ms/step - loss: 1.2250 - accuracy: 0.6579 - val_loss: 0.3859 - val_accuracy: 0.8719\n",
      "Epoch 16/25\n",
      "187/187 [==============================] - 111s 597ms/step - loss: 1.2077 - accuracy: 0.6585 - val_loss: 0.3725 - val_accuracy: 0.8747\n",
      "Epoch 17/25\n",
      "187/187 [==============================] - 112s 602ms/step - loss: 1.1927 - accuracy: 0.6581 - val_loss: 0.4493 - val_accuracy: 0.8527\n",
      "Epoch 18/25\n",
      "138/187 [=====================>........] - ETA: 17s - loss: 1.2303 - accuracy: 0.6556"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 137438953472 bytes == 0x557a9fc90000 @  0x7fa596addd3f 0x7fa596b140c0 0x7fa596b17082 0x7fa596b17243 0x7fa587148402 0x7fa57b4e2eb0 0x7fa57b5030b5 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x559aa7c90000 @  0x7fa596addd3f 0x7fa596b13ba6 0x7fa596b16a5a 0x7fa57b4fd836 0x7fa57b502f72 0x7fa57b5069ea 0x7fa57b506f69 0x7fa57b5072d1 0x7fa57b4fbce3 0x7fa576bc1051 0x7fa576a1c38d 0x7fa5767af087 0x7fa5767af91e 0x7fa5767afb1d 0x7fa5821a0bf5 0x7fa576bc2d7c 0x7fa576b4ccec 0x7fa57bf0f76e 0x7fa57bf0c1f3 0x7fa57729e313 0x7fa596a73609 0x7fa596998133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 113s 604ms/step - loss: 1.2136 - accuracy: 0.6588 - val_loss: 0.3783 - val_accuracy: 0.8768\n",
      "Epoch 19/25\n",
      "187/187 [==============================] - 112s 598ms/step - loss: 1.2199 - accuracy: 0.6616 - val_loss: 0.3519 - val_accuracy: 0.8844\n",
      "Epoch 20/25\n",
      "187/187 [==============================] - 111s 596ms/step - loss: 1.2443 - accuracy: 0.6574 - val_loss: 0.4113 - val_accuracy: 0.8639\n",
      "Epoch 21/25\n",
      "187/187 [==============================] - 112s 599ms/step - loss: 1.2092 - accuracy: 0.6603 - val_loss: 0.3787 - val_accuracy: 0.8752\n",
      "Epoch 22/25\n",
      "187/187 [==============================] - 111s 597ms/step - loss: 1.2223 - accuracy: 0.6600 - val_loss: 0.4108 - val_accuracy: 0.8590\n",
      "Epoch 23/25\n",
      "187/187 [==============================] - 112s 600ms/step - loss: 1.2001 - accuracy: 0.6695 - val_loss: 0.3748 - val_accuracy: 0.8765\n",
      "Epoch 24/25\n",
      "187/187 [==============================] - 112s 602ms/step - loss: 1.2118 - accuracy: 0.6621 - val_loss: 0.3680 - val_accuracy: 0.8759\n",
      "Epoch 25/25\n",
      "187/187 [==============================] - 113s 607ms/step - loss: 1.1863 - accuracy: 0.6691 - val_loss: 0.3973 - val_accuracy: 0.8708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 04:16:55.724107: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa538cd2a58>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_single = build_model(num_classes=NUM_CLASSES)\n",
    "\n",
    "start = time()\n",
    "model_single.fit(ds_train,\n",
    "                 epochs=FREEZE_EPOCHS,\n",
    "                 steps_per_epoch=FIT_STEPS,\n",
    "                 validation_data=ds_test,\n",
    "                 validation_steps=VAL_STEPS,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e33464b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - 142s 728ms/step - loss: 0.9584 - accuracy: 0.7172 - val_loss: 0.2450 - val_accuracy: 0.9236\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 137s 733ms/step - loss: 0.8080 - accuracy: 0.7550 - val_loss: 0.2186 - val_accuracy: 0.9311\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 136s 728ms/step - loss: 0.7679 - accuracy: 0.7705 - val_loss: 0.2000 - val_accuracy: 0.9360\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 136s 729ms/step - loss: 0.6943 - accuracy: 0.7903 - val_loss: 0.1861 - val_accuracy: 0.9413\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 136s 727ms/step - loss: 0.6795 - accuracy: 0.7868 - val_loss: 0.1737 - val_accuracy: 0.9459\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 135s 723ms/step - loss: 0.6311 - accuracy: 0.7996 - val_loss: 0.1630 - val_accuracy: 0.9477\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 136s 728ms/step - loss: 0.5861 - accuracy: 0.8187 - val_loss: 0.1462 - val_accuracy: 0.9547\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 136s 728ms/step - loss: 0.5625 - accuracy: 0.8234 - val_loss: 0.1437 - val_accuracy: 0.9554\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 134s 721ms/step - loss: 0.5333 - accuracy: 0.8295 - val_loss: 0.1347 - val_accuracy: 0.9594\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 134s 718ms/step - loss: 0.5121 - accuracy: 0.8387 - val_loss: 0.1264 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 04:39:37.991699: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9b942dc780>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfreeze_model(model_single)\n",
    "model_single.fit(ds_train,\n",
    "                 epochs=UNFREEZE_EPOCHS,\n",
    "                 steps_per_epoch=FIT_STEPS,\n",
    "                 validation_data=ds_test,\n",
    "                 validation_steps=VAL_STEPS,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1966b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 54s 288ms/step - loss: 0.1264 - accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "fit_time_model_single = time() - start\n",
    "acc_model_single = model_single.evaluate(ds_test, steps=VAL_STEPS, verbose=1)\n",
    "model_single.save(\"EfficientNetB0.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee11a679",
   "metadata": {},
   "source": [
    "### Multiple processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e6587b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 1073741824 bytes == 0x55a042126000 @  0x7f56a5f2dd3f 0x7f56a5f640c0 0x7f56a5f67082 0x7f56a5f67243 0x7f5696598402 0x7f568a932eb0 0x7f568a9530b5 0x7f568a9569ea 0x7f568a956f69 0x7f568a9572d1 0x7f568a94bce3 0x7f5686011051 0x7f5685e6c38d 0x7f5685bff087 0x7f5685bff91e 0x7f5685bffb1d 0x7f568eb52c92 0x7f568eb6b6d8 0x7f568ebbfe3e 0x7f5686012d7c 0x7f5685f9ccec 0x7f568b35f76e 0x7f568b35c1f3 0x7f56866ee313 0x7f56a5ec3609 0x7f56a5de8133\n",
      "2022-06-09 11:08:50.070695: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:08:57.448326: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm1o1ebux/temp_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpm1o1ebux/temp_model/assets\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "2022-06-09 11:09:16.218087: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:09:16.233678: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:51495, 1 -> localhost:45381, 2 -> localhost:52357, 3 -> localhost:46199}\n",
      "2022-06-09 11:09:16.233862: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:51495\n",
      "2022-06-09 11:09:16.318999: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:09:16.326727: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:51495, 1 -> localhost:45381, 2 -> localhost:52357, 3 -> localhost:46199}\n",
      "2022-06-09 11:09:16.326905: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:45381\n",
      "2022-06-09 11:09:16.466794: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:09:16.475141: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:51495, 1 -> localhost:45381, 2 -> localhost:52357, 3 -> localhost:46199}\n",
      "2022-06-09 11:09:16.475474: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:52357\n",
      "2022-06-09 11:09:16.649721: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:09:16.657345: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:51495, 1 -> localhost:45381, 2 -> localhost:52357, 3 -> localhost:46199}\n",
      "2022-06-09 11:09:16.657547: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:46199\n",
      "2022-06-09 11:09:31.287788: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:09:31.287931: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:09:31.288342: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:09:31.289294: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:09:31.400065: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:09:31.400872: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:09:31.410007: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:09:31.410048: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x555ef944e000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43a16a 0x7fdb59a3c941 0x7fdb59a3d061 0x7fdb5aeb1a1a 0x7fdb4f55cdbf 0x7fdb5af7c941 0x7fdb5492c953 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x560944f28000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f224219bbf7 0x7f22440c13ee 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x55a02ef2c000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b3606bf7 0x7f30b552c3ee 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x56285f934000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd07b820c92 0x7fd07b8396d8 0x7fd07b88de3e 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 73/187 [==========>...................] - ETA: 30s - loss: 4.1473 - accuracy: 0.2964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55a0717c2000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b230dbfd 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n",
      "tcmalloc: large alloc 2147483648 bytes == 0x555f3be64000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43938d 0x7fdb4f1cc087 0x7fdb4f1cc91e 0x7fdb4f1ccb1d 0x7fdb551acbfd 0x7fdb4f5dfd7c 0x7fdb4f5666d5 0x7fdb4f55a24e 0x7fdb5492c76e 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77/187 [===========>..................] - ETA: 29s - loss: 4.1336 - accuracy: 0.2981"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x560987896000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f22468b3bf5 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n",
      "tcmalloc: large alloc 2147483648 bytes == 0x5628a2268000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd0788adbfd 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 0s - loss: 3.4914 - accuracy: 0.3679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:10:30.441369: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:10:30.446354: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:10:30.447180: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:10:30.449801: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:10:30.532706: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:10:30.536970: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:10:30.549191: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:10:30.549788: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x56292a9d6000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd0788adbfd 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x55a0f991e000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b230dbfd 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x555fc36dc000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43938d 0x7fdb4f1cc087 0x7fdb4f1cc91e 0x7fdb4f1ccb1d 0x7fdb551acbfd 0x7fdb4f5dfd7c 0x7fdb4f5666d5 0x7fdb4f55a24e 0x7fdb5492c76e 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x560a1009a000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f22468b3bf5 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 100s 475ms/step - loss: 3.4914 - accuracy: 0.3679 - val_loss: 0.8264 - val_accuracy: 0.7544\n",
      "Epoch 2/25\n",
      "187/187 [==============================] - 40s 216ms/step - loss: 1.9580 - accuracy: 0.5146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:11:51.359371: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:11:51.382019: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:11:51.388239: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:11:51.400375: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25\n",
      "171/187 [==========================>...] - ETA: 3s - loss: 1.6189 - accuracy: 0.5607"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x562a33036000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd0788adbfd 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n",
      "tcmalloc: large alloc 8589934592 bytes == 0x55a2023f8000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b230dbfd 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/187 [===========================>..] - ETA: 2s - loss: 1.6142 - accuracy: 0.5609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x5560cc2c6000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43938d 0x7fdb4f1cc087 0x7fdb4f1cc91e 0x7fdb4f1ccb1d 0x7fdb551acbfd 0x7fdb4f5dfd7c 0x7fdb4f5666d5 0x7fdb4f55a24e 0x7fdb5492c76e 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n",
      "tcmalloc: large alloc 8589934592 bytes == 0x560b182be000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f2240ea2bfd 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 0s - loss: 1.6060 - accuracy: 0.5617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:12:28.846619: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:12:28.853529: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:12:28.853933: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:12:28.867361: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 200ms/step - loss: 1.6060 - accuracy: 0.5617\n",
      "Epoch 4/25\n",
      "187/187 [==============================] - 37s 195ms/step - loss: 1.4903 - accuracy: 0.5838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:13:05.472438: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:05.488014: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:05.489062: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:05.499020: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.4504 - accuracy: 0.5953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:13:42.614447: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:42.615966: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:42.633893: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:13:42.640631: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 198ms/step - loss: 1.4504 - accuracy: 0.5953\n",
      "Epoch 6/25\n",
      "187/187 [==============================] - 36s 195ms/step - loss: 1.4251 - accuracy: 0.6038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:14:19.090800: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:19.094398: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:19.099850: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:19.107244: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.4074 - accuracy: 0.6098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:14:55.712019: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:55.714474: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:55.725052: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:14:55.738830: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 196ms/step - loss: 1.4074 - accuracy: 0.6098\n",
      "Epoch 8/25\n",
      "124/187 [==================>...........] - ETA: 12s - loss: 1.4213 - accuracy: 0.6043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x55a412bf8000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b230dbfd 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n",
      "tcmalloc: large alloc 17179869184 bytes == 0x562c43842000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd0788adbfd 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/187 [===================>..........] - ETA: 11s - loss: 1.4238 - accuracy: 0.6039"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x5562dcd4a000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43938d 0x7fdb4f1cc087 0x7fdb4f1cc91e 0x7fdb4f1ccb1d 0x7fdb551acbfd 0x7fdb4f5dfd7c 0x7fdb4f5666d5 0x7fdb4f55a24e 0x7fdb5492c76e 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n",
      "tcmalloc: large alloc 17179869184 bytes == 0x560d28f3e000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f2240ea2bfd 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 196ms/step - loss: 1.3929 - accuracy: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:15:32.589974: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:15:32.599575: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:15:32.602657: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:15:32.606071: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3712 - accuracy: 0.6109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:16:10.225943: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:10.225955: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:10.226327: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:10.229356: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 38s 201ms/step - loss: 1.3712 - accuracy: 0.6109\n",
      "Epoch 10/25\n",
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3402 - accuracy: 0.6222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:16:46.963628: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:46.963738: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:46.981270: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:16:46.983969: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3776 - accuracy: 0.6191"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:17:23.895704: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:17:23.896784: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:17:23.900698: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:17:23.905244: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3776 - accuracy: 0.6191\n",
      "Epoch 12/25\n",
      "187/187 [==============================] - 37s 196ms/step - loss: 1.3604 - accuracy: 0.6151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:18:00.689883: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:00.697375: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:00.712102: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:00.712775: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3434 - accuracy: 0.6298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:18:37.202864: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:37.209067: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:37.220208: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:18:37.238306: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 36s 195ms/step - loss: 1.3434 - accuracy: 0.6298\n",
      "Epoch 14/25\n",
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3543 - accuracy: 0.6198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:19:14.005823: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:14.012703: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:14.013637: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:14.014476: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3628 - accuracy: 0.6225"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:19:50.898136: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:50.902992: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:50.903014: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:19:50.903767: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3628 - accuracy: 0.6225\n",
      "Epoch 16/25\n",
      "187/187 [==============================] - 37s 195ms/step - loss: 1.3322 - accuracy: 0.6300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:20:27.522218: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:20:27.526275: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:20:27.529009: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:20:27.543278: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3480 - accuracy: 0.6301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:21:04.118407: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:04.123521: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:04.124298: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:04.132224: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 196ms/step - loss: 1.3480 - accuracy: 0.6301\n",
      "Epoch 18/25\n",
      " 34/187 [====>.........................] - ETA: 28s - loss: 1.3095 - accuracy: 0.6489"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 34359738368 bytes == 0x55a833bf8000 @  0x7f30c2cead3f 0x7f30c2d210c0 0x7f30c2d24082 0x7f30c2d24243 0x7f30bccc6402 0x7f30b1060eb0 0x7f30b10810b5 0x7f30b10849ea 0x7f30b1084f69 0x7f30b10852d1 0x7f30b1079ce3 0x7f30ac73f051 0x7f30ac59a38d 0x7f30ac32d087 0x7f30ac32d91e 0x7f30ac32db1d 0x7f30b230dbfd 0x7f30ac740d7c 0x7f30ac6c76d5 0x7f30ac6bb24e 0x7f30b1a8d76e 0x7f30b1a8a1f3 0x7f30ace1c313 0x7f30c2c80609 0x7f30c2ba5133\n",
      "tcmalloc: large alloc 34359738368 bytes == 0x563064842000 @  0x7fd08928ad3f 0x7fd0892c10c0 0x7fd0892c4082 0x7fd0892c4243 0x7fd083266402 0x7fd077600eb0 0x7fd0776210b5 0x7fd0776249ea 0x7fd077624f69 0x7fd0776252d1 0x7fd077619ce3 0x7fd072cdf051 0x7fd072b3a38d 0x7fd0728cd087 0x7fd0728cd91e 0x7fd0728cdb1d 0x7fd0788adbfd 0x7fd072ce0d7c 0x7fd072c676d5 0x7fd072c5b24e 0x7fd07802d76e 0x7fd07802a1f3 0x7fd0733bc313 0x7fd089220609 0x7fd089145133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38/187 [=====>........................] - ETA: 28s - loss: 1.3167 - accuracy: 0.6501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 34359738368 bytes == 0x5566fdd4a000 @  0x7fdb65b89d3f 0x7fdb65bc00c0 0x7fdb65bc3082 0x7fdb65bc3243 0x7fdb5fb65402 0x7fdb53effeb0 0x7fdb53f200b5 0x7fdb53f239ea 0x7fdb53f23f69 0x7fdb53f242d1 0x7fdb53f18ce3 0x7fdb4f5de051 0x7fdb4f43938d 0x7fdb4f1cc087 0x7fdb4f1cc91e 0x7fdb4f1ccb1d 0x7fdb551acbfd 0x7fdb4f5dfd7c 0x7fdb4f5666d5 0x7fdb4f55a24e 0x7fdb5492c76e 0x7fdb549291f3 0x7fdb4fcbb313 0x7fdb65b1f609 0x7fdb65a44133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42/187 [=====>........................] - ETA: 29s - loss: 1.3157 - accuracy: 0.6499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 34359738368 bytes == 0x561149f3e000 @  0x7f225187fd3f 0x7f22518b60c0 0x7f22518b9082 0x7f22518b9243 0x7f224b85b402 0x7f223fbf5eb0 0x7f223fc160b5 0x7f223fc199ea 0x7f223fc19f69 0x7f223fc1a2d1 0x7f223fc0ece3 0x7f223b2d4051 0x7f223b12f38d 0x7f223aec2087 0x7f223aec291e 0x7f223aec2b1d 0x7f2240ea2bfd 0x7f223b2d5d7c 0x7f223b25c6d5 0x7f223b25024e 0x7f224062276e 0x7f224061f1f3 0x7f223b9b1313 0x7f2251815609 0x7f225173a133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 198ms/step - loss: 1.3445 - accuracy: 0.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:21:41.168772: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:41.178052: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:41.183951: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:21:41.196484: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3458 - accuracy: 0.6312"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:22:17.877075: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:17.878973: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:17.883912: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:17.885891: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 196ms/step - loss: 1.3458 - accuracy: 0.6312\n",
      "Epoch 20/25\n",
      "187/187 [==============================] - 37s 195ms/step - loss: 1.3461 - accuracy: 0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:22:54.430373: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:54.435653: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:54.446896: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:22:54.465152: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3645 - accuracy: 0.6265"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:23:31.035961: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:23:31.043032: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:23:31.045048: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:23:31.045421: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 37s 196ms/step - loss: 1.3645 - accuracy: 0.6265\n",
      "Epoch 22/25\n",
      "187/187 [==============================] - 37s 200ms/step - loss: 1.3643 - accuracy: 0.6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:24:08.536649: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:08.538006: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:08.541515: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:08.542961: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3213 - accuracy: 0.6374"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:24:46.187122: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:46.194799: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:46.194875: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:24:46.195292: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 38s 201ms/step - loss: 1.3213 - accuracy: 0.6374\n",
      "Epoch 24/25\n",
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3795 - accuracy: 0.6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:25:23.233629: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:25:23.233728: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:25:23.235472: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:25:23.236030: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25\n",
      "187/187 [==============================] - ETA: 0s - loss: 1.3422 - accuracy: 0.6319"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:26:00.117775: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:26:00.120057: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:26:00.130669: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:26:00.134641: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:26:00.159587: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:26:00.162580: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:26:00.175013: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:26:00.179277: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "187/187 [==============================] - 37s 197ms/step - loss: 1.3422 - accuracy: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:26:09.235908: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:26:09.288925: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:26:09.379701: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:26:09.545982: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5648286dd8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_multiple = build_model(num_classes=NUM_CLASSES, learning_rate=1e-2)\n",
    "\n",
    "start = time()\n",
    "model_multiple.fit(ds_train,\n",
    "                   epochs=FREEZE_EPOCHS,\n",
    "                   validation_data=ds_test,\n",
    "                   steps_per_epoch=FIT_STEPS,\n",
    "                   validation_steps=VAL_STEPS,\n",
    "                   verbose=1,\n",
    "                   num_processes=NUM_PROCESSES,\n",
    "                   backend=\"multiprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901e9a64",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:27:18.083041: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx6b3acmy/temp_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpx6b3acmy/temp_model/assets\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "2022-06-09 11:27:45.785284: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:27:45.794876: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:54159, 1 -> localhost:55681, 2 -> localhost:60883, 3 -> localhost:43175}\n",
      "2022-06-09 11:27:45.795096: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:54159\n",
      "2022-06-09 11:27:45.856329: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:27:45.864435: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:54159, 1 -> localhost:55681, 2 -> localhost:60883, 3 -> localhost:43175}\n",
      "2022-06-09 11:27:45.864613: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:55681\n",
      "2022-06-09 11:27:45.979576: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:27:45.987194: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:54159, 1 -> localhost:55681, 2 -> localhost:60883, 3 -> localhost:43175}\n",
      "2022-06-09 11:27:45.987364: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:60883\n",
      "2022-06-09 11:27:46.253758: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-09 11:27:46.262094: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> localhost:54159, 1 -> localhost:55681, 2 -> localhost:60883, 3 -> localhost:43175}\n",
      "2022-06-09 11:27:46.262280: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:43175\n",
      "2022-06-09 11:28:01.143965: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:28:01.144037: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:28:01.161446: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:28:01.162667: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:28:01.271153: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:28:01.272846: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:28:01.274465: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:28:01.284710: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x55d754988000 @  0x7f3d5df7bd3f 0x7f3d5dfb20c0 0x7f3d5dfb5082 0x7f3d5dfb5243 0x7f3d57f57402 0x7f3d4c2f1eb0 0x7f3d4c3120b5 0x7f3d4c3159ea 0x7f3d4c315f69 0x7f3d4c3162d1 0x7f3d4c30ace3 0x7f3d479d0051 0x7f3d4782b38d 0x7f3d475be087 0x7f3d475be91e 0x7f3d475beb1d 0x7f3d52fafbf5 0x7f3d479d1d7c 0x7f3d479586d5 0x7f3d4794c24e 0x7f3d4cd1e76e 0x7f3d4cd1b1f3 0x7f3d480ad313 0x7f3d5df11609 0x7f3d5de36133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x55c4cdfac000 @  0x7fa160e26d3f 0x7fa160e5d0c0 0x7fa160e60082 0x7fa160e60243 0x7fa15ae02402 0x7fa14f19ceb0 0x7fa14f1bd0b5 0x7fa14f1c09ea 0x7fa14f1c0f69 0x7fa14f1c12d1 0x7fa14f1b5ce3 0x7fa14a87b051 0x7fa14a6d716a 0x7fa154cd9941 0x7fa154cda061 0x7fa15614ea1a 0x7fa14a7f9dbf 0x7fa156219941 0x7fa14fbc9953 0x7fa14fbc61f3 0x7fa14af58313 0x7fa160dbc609 0x7fa160ce1133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x55d70211a000 @  0x7fd434a42d3f 0x7fd434a790c0 0x7fd434a7c082 0x7fd434a7c243 0x7fd42ea1e402 0x7fd422db8eb0 0x7fd422dd90b5 0x7fd422ddc9ea 0x7fd422ddcf69 0x7fd422ddd2d1 0x7fd422dd1ce3 0x7fd41e497051 0x7fd41e2f238d 0x7fd41e085087 0x7fd41e08591e 0x7fd41e085b1d 0x7fd426fd8c92 0x7fd426ff16d8 0x7fd427045e3e 0x7fd41e498d7c 0x7fd41e41f6d5 0x7fd41e41324e 0x7fd4237e576e 0x7fd4237e21f3 0x7fd41eb74313 0x7fd4349d8609 0x7fd4348fd133\n",
      "tcmalloc: large alloc 1073741824 bytes == 0x55a02c362000 @  0x7f0968b63d3f 0x7f0968b9a0c0 0x7f0968b9d082 0x7f0968b9d243 0x7f0962b3f402 0x7f0956ed9eb0 0x7f0956efa0b5 0x7f0956efd9ea 0x7f0956efdf69 0x7f0956efe2d1 0x7f0956ef2ce3 0x7f09525b8051 0x7f095241416a 0x7f095ca16941 0x7f095ca17061 0x7f095de8ba1a 0x7f0952536dbf 0x7f095df56941 0x7f0957906953 0x7f09579031f3 0x7f0952c95313 0x7f0968af9609 0x7f0968a1e133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 73/187 [==========>...................] - ETA: 33s - loss: 1.2107 - accuracy: 0.6629"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55a06f3ea000 @  0x7f0968b63d3f 0x7f0968b9a0c0 0x7f0968b9d082 0x7f0968b9d243 0x7f0962b3f402 0x7f0956ed9eb0 0x7f0956efa0b5 0x7f0956efd9ea 0x7f0956efdf69 0x7f0956efe2d1 0x7f0956ef2ce3 0x7f09525b8051 0x7f095241338d 0x7f09521a6087 0x7f09521a691e 0x7f09521a6b1d 0x7f0958186bfd 0x7f09525b9d7c 0x7f09525406d5 0x7f095253424e 0x7f095790676e 0x7f09579031f3 0x7f0952c95313 0x7f0968af9609 0x7f0968a1e133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77/187 [===========>..................] - ETA: 32s - loss: 1.2064 - accuracy: 0.6638"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55c5116d4000 @  0x7fa160e26d3f 0x7fa160e5d0c0 0x7fa160e60082 0x7fa160e60243 0x7fa15ae02402 0x7fa14f19ceb0 0x7fa14f1bd0b5 0x7fa14f1c09ea 0x7fa14f1c0f69 0x7fa14f1c12d1 0x7fa14f1b5ce3 0x7fa14a87b051 0x7fa14a6d638d 0x7fa14a469087 0x7fa14a46991e 0x7fa14a469b1d 0x7fa150449bfd 0x7fa14a87cd7c 0x7fa14a8036d5 0x7fa14a7f724e 0x7fa14fbc976e 0x7fa14fbc61f3 0x7fa14af58313 0x7fa160dbc609 0x7fa160ce1133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 81/187 [===========>..................] - ETA: 31s - loss: 1.1980 - accuracy: 0.6644"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55d797806000 @  0x7f3d5df7bd3f 0x7f3d5dfb20c0 0x7f3d5dfb5082 0x7f3d5dfb5243 0x7f3d57f57402 0x7f3d4c2f1eb0 0x7f3d4c3120b5 0x7f3d4c3159ea 0x7f3d4c315f69 0x7f3d4c3162d1 0x7f3d4c30ace3 0x7f3d479d0051 0x7f3d4782b38d 0x7f3d475be087 0x7f3d475be91e 0x7f3d475beb1d 0x7f3d4d59ebfd 0x7f3d479d1d7c 0x7f3d479586d5 0x7f3d4794c24e 0x7f3d4cd1e76e 0x7f3d4cd1b1f3 0x7f3d480ad313 0x7f3d5df11609 0x7f3d5de36133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85/187 [============>.................] - ETA: 29s - loss: 1.1831 - accuracy: 0.6678"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55d744dfe000 @  0x7fd434a42d3f 0x7fd434a790c0 0x7fd434a7c082 0x7fd434a7c243 0x7fd42ea1e402 0x7fd422db8eb0 0x7fd422dd90b5 0x7fd422ddc9ea 0x7fd422ddcf69 0x7fd422ddd2d1 0x7fd422dd1ce3 0x7fd41e497051 0x7fd41e2f238d 0x7fd41e085087 0x7fd41e08591e 0x7fd41e085b1d 0x7fd429a76bf5 0x7fd41e498d7c 0x7fd41e41f6d5 0x7fd41e41324e 0x7fd4237e576e 0x7fd4237e21f3 0x7fd41eb74313 0x7fd4349d8609 0x7fd4348fd133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - ETA: 0s - loss: 1.0811 - accuracy: 0.6890"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:29:05.017371: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:29:05.017570: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:29:05.020400: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:29:05.024417: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "2022-06-09 11:29:05.099478: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:29:05.107402: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:29:05.110067: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:29:05.139768: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x55c599496000 @  0x7fa160e26d3f 0x7fa160e5d0c0 0x7fa160e60082 0x7fa160e60243 0x7fa15ae02402 0x7fa14f19ceb0 0x7fa14f1bd0b5 0x7fa14f1c09ea 0x7fa14f1c0f69 0x7fa14f1c12d1 0x7fa14f1b5ce3 0x7fa14a87b051 0x7fa14a6d638d 0x7fa14a469087 0x7fa14a46991e 0x7fa14a469b1d 0x7fa150449bfd 0x7fa14a87cd7c 0x7fa14a8036d5 0x7fa14a7f724e 0x7fa14fbc976e 0x7fa14fbc61f3 0x7fa14af58313 0x7fa160dbc609 0x7fa160ce1133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x55a0f7140000 @  0x7f0968b63d3f 0x7f0968b9a0c0 0x7f0968b9d082 0x7f0968b9d243 0x7f0962b3f402 0x7f0956ed9eb0 0x7f0956efa0b5 0x7f0956efd9ea 0x7f0956efdf69 0x7f0956efe2d1 0x7f0956ef2ce3 0x7f09525b8051 0x7f095241338d 0x7f09521a6087 0x7f09521a691e 0x7f09521a6b1d 0x7f095db97bf5 0x7f09525b9d7c 0x7f09525406d5 0x7f095253424e 0x7f095790676e 0x7f09579031f3 0x7f0952c95313 0x7f0968af9609 0x7f0968a1e133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x55d81fb5a000 @  0x7f3d5df7bd3f 0x7f3d5dfb20c0 0x7f3d5dfb5082 0x7f3d5dfb5243 0x7f3d57f57402 0x7f3d4c2f1eb0 0x7f3d4c3120b5 0x7f3d4c3159ea 0x7f3d4c315f69 0x7f3d4c3162d1 0x7f3d4c30ace3 0x7f3d479d0051 0x7f3d4782b38d 0x7f3d475be087 0x7f3d475be91e 0x7f3d475beb1d 0x7f3d4d59ebfd 0x7f3d479d1d7c 0x7f3d479586d5 0x7f3d4794c24e 0x7f3d4cd1e76e 0x7f3d4cd1b1f3 0x7f3d480ad313 0x7f3d5df11609 0x7f3d5de36133\n",
      "tcmalloc: large alloc 4294967296 bytes == 0x55d7cc864000 @  0x7fd434a42d3f 0x7fd434a790c0 0x7fd434a7c082 0x7fd434a7c243 0x7fd42ea1e402 0x7fd422db8eb0 0x7fd422dd90b5 0x7fd422ddc9ea 0x7fd422ddcf69 0x7fd422ddd2d1 0x7fd422dd1ce3 0x7fd41e497051 0x7fd41e2f238d 0x7fd41e085087 0x7fd41e08591e 0x7fd41e085b1d 0x7fd429a76bf5 0x7fd41e498d7c 0x7fd41e41f6d5 0x7fd41e41324e 0x7fd4237e576e 0x7fd4237e21f3 0x7fd41eb74313 0x7fd4349d8609 0x7fd4348fd133\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 187 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 101s 482ms/step - loss: 1.0811 - accuracy: 0.6890 - val_loss: 0.2880 - val_accuracy: 0.9062\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 49s 265ms/step - loss: 0.9610 - accuracy: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:30:31.609281: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:30:31.609724: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:30:31.612063: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:30:31.617549: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "163/187 [=========================>....] - ETA: 6s - loss: 0.9250 - accuracy: 0.7256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x55c6a1a9a000 @  0x7fa160e26d3f 0x7fa160e5d0c0 0x7fa160e60082 0x7fa160e60243 0x7fa15ae02402 0x7fa14f19ceb0 0x7fa14f1bd0b5 0x7fa14f1c09ea 0x7fa14f1c0f69 0x7fa14f1c12d1 0x7fa14f1b5ce3 0x7fa14a87b051 0x7fa14a6d638d 0x7fa14a469087 0x7fa14a46991e 0x7fa14a469b1d 0x7fa150449bfd 0x7fa14a87cd7c 0x7fa14a8036d5 0x7fa14a7f724e 0x7fa14fbc976e 0x7fa14fbc61f3 0x7fa14af58313 0x7fa160dbc609 0x7fa160ce1133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/187 [=========================>....] - ETA: 5s - loss: 0.9270 - accuracy: 0.7245"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x55a1ff2f4000 @  0x7f0968b63d3f 0x7f0968b9a0c0 0x7f0968b9d082 0x7f0968b9d243 0x7f0962b3f402 0x7f0956ed9eb0 0x7f0956efa0b5 0x7f0956efd9ea 0x7f0956efdf69 0x7f0956efe2d1 0x7f0956ef2ce3 0x7f09525b8051 0x7f095241338d 0x7f09521a6087 0x7f09521a691e 0x7f09521a6b1d 0x7f0958186bfd 0x7f09525b9d7c 0x7f09525406d5 0x7f095253424e 0x7f095790676e 0x7f09579031f3 0x7f0952c95313 0x7f0968af9609 0x7f0968a1e133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/187 [==========================>...] - ETA: 4s - loss: 0.9263 - accuracy: 0.7246"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x55d8d51d4000 @  0x7fd434a42d3f 0x7fd434a790c0 0x7fd434a7c082 0x7fd434a7c243 0x7fd42ea1e402 0x7fd422db8eb0 0x7fd422dd90b5 0x7fd422ddc9ea 0x7fd422ddcf69 0x7fd422ddd2d1 0x7fd422dd1ce3 0x7fd41e497051 0x7fd41e2f238d 0x7fd41e085087 0x7fd41e08591e 0x7fd41e085b1d 0x7fd424065bfd 0x7fd41e498d7c 0x7fd41e41f6d5 0x7fd41e41324e 0x7fd4237e576e 0x7fd4237e21f3 0x7fd41eb74313 0x7fd4349d8609 0x7fd4348fd133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/187 [===========================>..] - ETA: 3s - loss: 0.9234 - accuracy: 0.7254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 8589934592 bytes == 0x55d927b5a000 @  0x7f3d5df7bd3f 0x7f3d5dfb20c0 0x7f3d5dfb5082 0x7f3d5dfb5243 0x7f3d57f57402 0x7f3d4c2f1eb0 0x7f3d4c3120b5 0x7f3d4c3159ea 0x7f3d4c315f69 0x7f3d4c3162d1 0x7f3d4c30ace3 0x7f3d479d0051 0x7f3d4782b38d 0x7f3d475be087 0x7f3d475be91e 0x7f3d475beb1d 0x7f3d4d59ebfd 0x7f3d479d1d7c 0x7f3d479586d5 0x7f3d4794c24e 0x7f3d4cd1e76e 0x7f3d4cd1b1f3 0x7f3d480ad313 0x7f3d5df11609 0x7f3d5de36133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 48s 258ms/step - loss: 0.9250 - accuracy: 0.7249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:31:19.948801: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:31:19.952388: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:31:19.956004: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:31:19.977287: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "187/187 [==============================] - 45s 238ms/step - loss: 0.8632 - accuracy: 0.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:32:04.594118: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:04.595407: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:04.604111: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:04.620519: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "187/187 [==============================] - 45s 240ms/step - loss: 0.8206 - accuracy: 0.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:32:49.467411: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:49.474532: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:49.479945: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:32:49.482036: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "187/187 [==============================] - 45s 242ms/step - loss: 0.7806 - accuracy: 0.7660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:33:34.694148: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:33:34.697094: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:33:34.698121: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:33:34.699919: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "187/187 [==============================] - 45s 243ms/step - loss: 0.7321 - accuracy: 0.7766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:34:20.117136: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:34:20.117749: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:34:20.131116: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:34:20.131225: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "116/187 [=================>............] - ETA: 16s - loss: 0.7162 - accuracy: 0.7804"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x55c8b229a000 @  0x7fa160e26d3f 0x7fa160e5d0c0 0x7fa160e60082 0x7fa160e60243 0x7fa15ae02402 0x7fa14f19ceb0 0x7fa14f1bd0b5 0x7fa14f1c09ea 0x7fa14f1c0f69 0x7fa14f1c12d1 0x7fa14f1b5ce3 0x7fa14a87b051 0x7fa14a6d638d 0x7fa14a469087 0x7fa14a46991e 0x7fa14a469b1d 0x7fa150449bfd 0x7fa14a87cd7c 0x7fa14a8036d5 0x7fa14a7f724e 0x7fa14fbc976e 0x7fa14fbc61f3 0x7fa14af58313 0x7fa160dbc609 0x7fa160ce1133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/187 [==================>...........] - ETA: 16s - loss: 0.7176 - accuracy: 0.7798"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x55a40ffe4000 @  0x7f0968b63d3f 0x7f0968b9a0c0 0x7f0968b9d082 0x7f0968b9d243 0x7f0962b3f402 0x7f0956ed9eb0 0x7f0956efa0b5 0x7f0956efd9ea 0x7f0956efdf69 0x7f0956efe2d1 0x7f0956ef2ce3 0x7f09525b8051 0x7f095241338d 0x7f09521a6087 0x7f09521a691e 0x7f09521a6b1d 0x7f0958186bfd 0x7f09525b9d7c 0x7f09525406d5 0x7f095253424e 0x7f095790676e 0x7f09579031f3 0x7f0952c95313 0x7f0968af9609 0x7f0968a1e133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124/187 [==================>...........] - ETA: 15s - loss: 0.7138 - accuracy: 0.7811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x55dae6096000 @  0x7fd434a42d3f 0x7fd434a790c0 0x7fd434a7c082 0x7fd434a7c243 0x7fd42ea1e402 0x7fd422db8eb0 0x7fd422dd90b5 0x7fd422ddc9ea 0x7fd422ddcf69 0x7fd422ddd2d1 0x7fd422dd1ce3 0x7fd41e497051 0x7fd41e2f238d 0x7fd41e085087 0x7fd41e08591e 0x7fd41e085b1d 0x7fd424065bfd 0x7fd41e498d7c 0x7fd41e41f6d5 0x7fd41e41324e 0x7fd4237e576e 0x7fd4237e21f3 0x7fd41eb74313 0x7fd4349d8609 0x7fd4348fd133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/187 [===================>..........] - ETA: 14s - loss: 0.7167 - accuracy: 0.7808"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 17179869184 bytes == 0x55db38acc000 @  0x7f3d5df7bd3f 0x7f3d5dfb20c0 0x7f3d5dfb5082 0x7f3d5dfb5243 0x7f3d57f57402 0x7f3d4c2f1eb0 0x7f3d4c3120b5 0x7f3d4c3159ea 0x7f3d4c315f69 0x7f3d4c3162d1 0x7f3d4c30ace3 0x7f3d479d0051 0x7f3d4782b38d 0x7f3d475be087 0x7f3d475be91e 0x7f3d475beb1d 0x7f3d4d59ebfd 0x7f3d479d1d7c 0x7f3d479586d5 0x7f3d4794c24e 0x7f3d4cd1e76e 0x7f3d4cd1b1f3 0x7f3d480ad313 0x7f3d5df11609 0x7f3d5de36133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 46s 245ms/step - loss: 0.7140 - accuracy: 0.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:35:06.053245: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:06.059118: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:06.060861: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:06.072665: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.7888"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:35:52.018861: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:52.021350: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:52.025506: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:35:52.026554: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 46s 246ms/step - loss: 0.6822 - accuracy: 0.7888\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 45s 240ms/step - loss: 0.6680 - accuracy: 0.7949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:36:36.809924: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:36:36.810250: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:36:36.814418: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:36:36.817421: W tensorflow/core/framework/dataset.cc:744] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2022-06-09 11:36:36.849881: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:36:36.851053: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:36:36.852536: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:36:36.859441: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2022-06-09 11:36:45.598629: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:36:45.856147: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:36:46.007326: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-06-09 11:36:46.051001: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f563355eb70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfreeze_model(model_multiple)\n",
    "\n",
    "model_multiple.fit(ds_train,\n",
    "                  epochs=UNFREEZE_EPOCHS,\n",
    "                   steps_per_epoch=FIT_STEPS,\n",
    "                  validation_data=ds_test,\n",
    "                   validation_steps=VAL_STEPS,\n",
    "                  verbose=1,\n",
    "                  num_processes=NUM_PROCESSES,\n",
    "                  backend=\"multiprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbe1d422",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20/187 [==>...........................] - ETA: 45s - loss: 0.1617 - accuracy: 0.9477"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 2147483648 bytes == 0x55a12eb7e000 @  0x7f56a5f2dd3f 0x7f56a5f640c0 0x7f56a5f67082 0x7f56a5f67243 0x7f5696598402 0x7f568a932eb0 0x7f568a9530b5 0x7f568a9569ea 0x7f568a956f69 0x7f568a9572d1 0x7f568a94bce3 0x7f5686011051 0x7f5685e6c38d 0x7f5685bff087 0x7f5685bff91e 0x7f5685bffb1d 0x7f56915f0bf5 0x7f5686012d7c 0x7f5685f9ccec 0x7f568b35f76e 0x7f568b35c1f3 0x7f56866ee313 0x7f56a5ec3609 0x7f56a5de8133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76/187 [===========>..................] - ETA: 27s - loss: 0.1697 - accuracy: 0.9474"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tcmalloc: large alloc 4294967296 bytes == 0x55a1b337e000 @  0x7f56a5f2dd3f 0x7f56a5f640c0 0x7f56a5f67082 0x7f56a5f67243 0x7f5696598402 0x7f568a932eb0 0x7f568a9530b5 0x7f568a9569ea 0x7f568a956f69 0x7f568a9572d1 0x7f568a94bce3 0x7f5686011051 0x7f5685e6c38d 0x7f5685bff087 0x7f5685bff91e 0x7f5685bffb1d 0x7f56915f0bf5 0x7f5686012d7c 0x7f5685f9ccec 0x7f568b35f76e 0x7f568b35c1f3 0x7f56866ee313 0x7f56a5ec3609 0x7f56a5de8133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 45s 237ms/step - loss: 0.1595 - accuracy: 0.9509\n"
     ]
    }
   ],
   "source": [
    "fit_time_model_multiple = time() - start\n",
    "acc_model_multiple = model_multiple.evaluate(ds_test, steps=VAL_STEPS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062daaad",
   "metadata": {},
   "source": [
    "### Multiple processes with horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651d7242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 11:47:21.371783: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0_67z5ql/temp_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0_67z5ql/temp_model/assets\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "[3]<stderr>:2022-06-09 11:47:53.830895: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[2]<stderr>:2022-06-09 11:47:53.831199: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[3]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[2]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1]<stderr>:2022-06-09 11:47:53.841787: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[0]<stderr>:2022-06-09 11:47:53.845143: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[3]<stderr>:2022-06-09 11:48:03.419889: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[3]<stderr>:2022-06-09 11:48:03.438107: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[2]<stderr>:2022-06-09 11:48:03.502919: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[2]<stderr>:2022-06-09 11:48:03.522058: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[1]<stderr>:2022-06-09 11:48:03.866572: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[1]<stderr>:2022-06-09 11:48:03.885587: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[0]<stderr>:2022-06-09 11:48:03.898395: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[0]<stderr>:2022-06-09 11:48:03.917057: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x55e76e50a000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8aa2077c92 0x7f8aa20906d8 0x7f8aa20e4e3e 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n",
      "[2]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x556d23f72000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6df0dbbc92 0x7f6df0dd46d8 0x7f6df0e28e3e 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n",
      "[1]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x55e5fcaee000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46516a 0x7f49f4a67941 0x7f49f4a68061 0x7f49f5edca1a 0x7f49ea587dbf 0x7f49f5fa7941 0x7f49ef957953 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n",
      "[0]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x55878d666000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559f067c92 0x7f559f0806d8 0x7f559f0d4e3e 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 58/187 [========>.....................] - ETA: 23s - loss: 4.1773 - accuracy: 0.2885     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x5587cff64000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559c0f4bfd 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 62/187 [========>.....................] - ETA: 22s - loss: 4.1994 - accuracy: 0.288\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x556d666f6000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6dede48bfd 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n",
      "[1]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x55e63f49e000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46438d 0x7f49ea1f7087 0x7f49ea1f791e 0x7f49ea1f7b1d 0x7f49f01d7bfd 0x7f49ea60ad7c 0x7f49ea594cec 0x7f49ef95776e 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 66/187 [=========>....................] - ETA: 22s - loss: 4.1652 - accuracy: 0.294\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x55e7b0dc0000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8a9f104bfd 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:186/187 [============================>.] - ETA: 0s - loss: 3.3330 - accuracy: 0.3810\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x5588577b0000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559c0f4bfd 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n",
      "[3]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x55e838300000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8a9f104bfd 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n",
      "[1]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x55e6c6640000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46438d 0x7f49ea1f7087 0x7f49ea1f791e 0x7f49ea1f7b1d 0x7f49f01d7bfd 0x7f49ea60ad7c 0x7f49ea594cec 0x7f49ef95776e 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n",
      "[2]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x556dedb56000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6dede48bfd 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 71s 335ms/step - loss: 3.3333 - accuracy: 0.3867 - val_loss: 0.7920 - val_accuracy: 0.7707\n",
      "[0]<stdout>:Epoch 2/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 309ms/step - loss: 1.8928 - accuracy: 0.5201 - val_loss: 0.5536 - val_accuracy: 0.8325\n",
      "[0]<stdout>:Epoch 3/25\n",
      "[0]<stdout>:156/187 [========================>.....] - ETA: 5s - loss: 1.5365 - accuracy: 0.57008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x55e7cf058000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46438d 0x7f49ea1f7087 0x7f49ea1f791e 0x7f49ea1f7b1d 0x7f49f01d7bfd 0x7f49ea60ad7c 0x7f49ea594cec 0x7f49ef95776e 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:160/187 [========================>.....] - ETA: 4s - loss: 1.5342 - accuracy: 0.57\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x55e940e5e000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8a9f104bfd 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n",
      "[0]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x5589602d8000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559c0f4bfd 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:164/187 [=========================>....] - ETA: 4s - loss: 1.5333 - accuracy: 0.57\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x556ef6356000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6dede48bfd 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 58s 310ms/step - loss: 1.5274 - accuracy: 0.5752 - val_loss: 0.5065 - val_accuracy: 0.8373\n",
      "[0]<stdout>:Epoch 4/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 311ms/step - loss: 1.4690 - accuracy: 0.5892 - val_loss: 0.4458 - val_accuracy: 0.8621\n",
      "[0]<stdout>:Epoch 5/25\n",
      "[0]<stdout>:187/187 [==============================] - 62s 319ms/step - loss: 1.3482 - accuracy: 0.6116 - val_loss: 0.4373 - val_accuracy: 0.8662\n",
      "[0]<stdout>:Epoch 6/25\n",
      "[0]<stdout>:187/187 [==============================] - 61s 320ms/step - loss: 1.4129 - accuracy: 0.6093 - val_loss: 0.4282 - val_accuracy: 0.8556\n",
      "[0]<stdout>:Epoch 7/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 311ms/step - loss: 1.3736 - accuracy: 0.6063 - val_loss: 0.4072 - val_accuracy: 0.8733\n",
      "[0]<stdout>:Epoch 8/25\n",
      "[0]<stdout>:109/187 [================>.............] - ETA: 15s - loss: 1.3181 - accuracy: 0.6428\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x55e9dfce2000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46438d 0x7f49ea1f7087 0x7f49ea1f791e 0x7f49ea1f7b1d 0x7f49f01d7bfd 0x7f49ea60ad7c 0x7f49ea594cec 0x7f49ef95776e 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:113/187 [=================>............] - ETA: 14s - loss: 1.3392 - accuracy: 0.637\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x558b70ad8000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559c0f4bfd 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n",
      "[3]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x55eb51d70000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8a9f104bfd 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:121/187 [==================>...........] - ETA: 12s - loss: 1.3428 - accuracy: 0.637\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x557106e66000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6dede48bfd 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 64s 334ms/step - loss: 1.3255 - accuracy: 0.6350 - val_loss: 0.4273 - val_accuracy: 0.8662\n",
      "[0]<stdout>:Epoch 9/25\n",
      "[0]<stdout>:187/187 [==============================] - 60s 318ms/step - loss: 1.3675 - accuracy: 0.6227 - val_loss: 0.4407 - val_accuracy: 0.8533\n",
      "[0]<stdout>:Epoch 10/25\n",
      "[0]<stdout>:187/187 [==============================] - 61s 324ms/step - loss: 1.3702 - accuracy: 0.6190 - val_loss: 0.4195 - val_accuracy: 0.8573\n",
      "[0]<stdout>:Epoch 11/25\n",
      "[0]<stdout>:187/187 [==============================] - 61s 323ms/step - loss: 1.2785 - accuracy: 0.6397 - val_loss: 0.4502 - val_accuracy: 0.8533\n",
      "[0]<stdout>:Epoch 12/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 317ms/step - loss: 1.2698 - accuracy: 0.6410 - val_loss: 0.4154 - val_accuracy: 0.8699\n",
      "[0]<stdout>:Epoch 13/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 312ms/step - loss: 1.3437 - accuracy: 0.6207 - val_loss: 0.4162 - val_accuracy: 0.8614\n",
      "[0]<stdout>:Epoch 14/25\n",
      "[0]<stdout>:187/187 [==============================] - 58s 305ms/step - loss: 1.3179 - accuracy: 0.6287 - val_loss: 0.4168 - val_accuracy: 0.8672\n",
      "[0]<stdout>:Epoch 15/25\n",
      "[0]<stdout>:187/187 [==============================] - 58s 309ms/step - loss: 1.2863 - accuracy: 0.6377 - val_loss: 0.3759 - val_accuracy: 0.8781\n",
      "[0]<stdout>:Epoch 16/25\n",
      "[0]<stdout>:187/187 [==============================] - 61s 315ms/step - loss: 1.2998 - accuracy: 0.6327 - val_loss: 0.3910 - val_accuracy: 0.8709\n",
      "[0]<stdout>:Epoch 17/25\n",
      "[0]<stdout>:187/187 [==============================] - 58s 305ms/step - loss: 1.3137 - accuracy: 0.6293 - val_loss: 0.4001 - val_accuracy: 0.8696\n",
      "[0]<stdout>:Epoch 18/25\n",
      "[0]<stdout>: 19/187 [==>...........................] - ETA: 30s - loss: 1.3614 - accuracy: 0.6315 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 34359738368 bytes == 0x55ee00ce2000 @  0x7f4a00ba0d3f 0x7f4a00bd70c0 0x7f4a00bda082 0x7f4a00bda243 0x7f49fab90402 0x7f49eef2aeb0 0x7f49eef4b0b5 0x7f49eef4e9ea 0x7f49eef4ef69 0x7f49eef4f2d1 0x7f49eef43ce3 0x7f49ea609051 0x7f49ea46438d 0x7f49ea1f7087 0x7f49ea1f791e 0x7f49ea1f7b1d 0x7f49f01d7bfd 0x7f49ea60ad7c 0x7f49ea594cec 0x7f49ef95776e 0x7f49ef9541f3 0x7f49eace6313 0x7f4a00b36609 0x7f4a00a5b133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 23/187 [==>...........................] - ETA: 31s - loss: 1.3858 - accuracy: 0.625\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 34359738368 bytes == 0x55ef72d70000 @  0x7f8aafacdd3f 0x7f8aafb040c0 0x7f8aafb07082 0x7f8aafb07243 0x7f8aa9abd402 0x7f8a9de57eb0 0x7f8a9de780b5 0x7f8a9de7b9ea 0x7f8a9de7bf69 0x7f8a9de7c2d1 0x7f8a9de70ce3 0x7f8a99536051 0x7f8a9939138d 0x7f8a99124087 0x7f8a9912491e 0x7f8a99124b1d 0x7f8a9f104bfd 0x7f8a99537d7c 0x7f8a994c1cec 0x7f8a9e88476e 0x7f8a9e8811f3 0x7f8a99c13313 0x7f8aafa63609 0x7f8aaf988133\n",
      "[0]<stderr>:tcmalloc: large alloc 34359738368 bytes == 0x558f91ad8000 @  0x7f55acabdd3f 0x7f55acaf40c0 0x7f55acaf7082 0x7f55acaf7243 0x7f55a6aad402 0x7f559ae47eb0 0x7f559ae680b5 0x7f559ae6b9ea 0x7f559ae6bf69 0x7f559ae6c2d1 0x7f559ae60ce3 0x7f5596526051 0x7f559638138d 0x7f5596114087 0x7f559611491e 0x7f5596114b1d 0x7f559c0f4bfd 0x7f5596527d7c 0x7f55964b1cec 0x7f559b87476e 0x7f559b8711f3 0x7f5596c03313 0x7f55aca53609 0x7f55ac978133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 27/187 [===>..........................] - ETA: 32s - loss: 1.3795 - accuracy: 0.620\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 34359738368 bytes == 0x557527e66000 @  0x7f6dfe811d3f 0x7f6dfe8480c0 0x7f6dfe84b082 0x7f6dfe84b243 0x7f6df8801402 0x7f6decb9beb0 0x7f6decbbc0b5 0x7f6decbbf9ea 0x7f6decbbff69 0x7f6decbc02d1 0x7f6decbb4ce3 0x7f6de827a051 0x7f6de80d538d 0x7f6de7e68087 0x7f6de7e6891e 0x7f6de7e68b1d 0x7f6dede48bfd 0x7f6de827bd7c 0x7f6de8205cec 0x7f6ded5c876e 0x7f6ded5c51f3 0x7f6de8957313 0x7f6dfe7a7609 0x7f6dfe6cc133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 64s 318ms/step - loss: 1.2791 - accuracy: 0.6494 - val_loss: 0.4043 - val_accuracy: 0.8702\n",
      "[0]<stdout>:Epoch 19/25\n",
      "[0]<stdout>:187/187 [==============================] - 62s 329ms/step - loss: 1.3400 - accuracy: 0.6190 - val_loss: 0.4060 - val_accuracy: 0.8672\n",
      "[0]<stdout>:Epoch 20/25\n",
      "[0]<stdout>:187/187 [==============================] - 60s 320ms/step - loss: 1.2716 - accuracy: 0.6487 - val_loss: 0.4254 - val_accuracy: 0.8696\n",
      "[0]<stdout>:Epoch 21/25\n",
      "[0]<stdout>:187/187 [==============================] - 60s 317ms/step - loss: 1.3254 - accuracy: 0.6267 - val_loss: 0.4306 - val_accuracy: 0.8570\n",
      "[0]<stdout>:Epoch 22/25\n",
      "[0]<stdout>:187/187 [==============================] - 62s 322ms/step - loss: 1.2847 - accuracy: 0.6447 - val_loss: 0.4079 - val_accuracy: 0.8624\n",
      "[0]<stdout>:Epoch 23/25\n",
      "[0]<stdout>:187/187 [==============================] - 60s 312ms/step - loss: 1.3003 - accuracy: 0.6434 - val_loss: 0.3634 - val_accuracy: 0.8736\n",
      "[0]<stdout>:Epoch 24/25\n",
      "[0]<stdout>:187/187 [==============================] - 59s 310ms/step - loss: 1.3222 - accuracy: 0.6334 - val_loss: 0.3737 - val_accuracy: 0.8750\n",
      "[0]<stdout>:Epoch 25/25\n",
      "[0]<stdout>:186/187 [============================>.] - ETA: 0s - loss: 1.2904 - accuracy: 0.64892\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:2022-06-09 12:13:18.778839: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - ETA: 0s - loss: 1.2888 - accuracy: 0.6434\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "[0]<stdout>:187/187 [==============================] - 62s 316ms/step - loss: 1.2888 - accuracy: 0.6434 - val_loss: 0.3735 - val_accuracy: 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:2022-06-09 12:13:20.700714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[2]<stderr>:2022-06-09 12:13:20.893522: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[3]<stderr>:2022-06-09 12:13:21.217032: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[1]<stderr>:2022-06-09 12:13:27.727152: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[2]<stderr>:2022-06-09 12:13:29.165183: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[0]<stderr>:2022-06-09 12:13:29.288803: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[3]<stderr>:2022-06-09 12:13:29.750036: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[1]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[1]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[1]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[1]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[2]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[2]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[2]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[2]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[3]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[3]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[3]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[3]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[0]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[0]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[0]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[0]<stderr>:  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5627c325c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_multiple_horovod = build_model(num_classes=NUM_CLASSES, learning_rate=1e-2)\n",
    "\n",
    "start = time()\n",
    "model_multiple_horovod.fit(ds_train,\n",
    "                           epochs=FREEZE_EPOCHS,\n",
    "                           validation_data=ds_test,\n",
    "                           steps_per_epoch=FIT_STEPS,\n",
    "                           validation_steps=VAL_STEPS,\n",
    "                           verbose=1,\n",
    "                           num_processes=NUM_PROCESSES,\n",
    "                           backend=\"horovod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "860530f1",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 12:14:38.789260: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps4khrapt/temp_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps4khrapt/temp_model/assets\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n",
      "[3]<stderr>:2022-06-09 12:15:09.702898: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[3]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[1]<stderr>:2022-06-09 12:15:09.703814: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[1]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[0]<stderr>:2022-06-09 12:15:09.708192: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[0]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[2]<stderr>:2022-06-09 12:15:09.712911: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "[2]<stderr>:To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[3]<stderr>:2022-06-09 12:15:19.628657: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[3]<stderr>:2022-06-09 12:15:19.647373: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[1]<stderr>:2022-06-09 12:15:19.660876: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[1]<stderr>:2022-06-09 12:15:19.679165: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[2]<stderr>:2022-06-09 12:15:19.732030: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[0]<stderr>:2022-06-09 12:15:19.747138: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[2]<stderr>:2022-06-09 12:15:19.750627: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n",
      "[0]<stderr>:2022-06-09 12:15:19.766297: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:537] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x562a35e96000 @  0x7faadfebbd3f 0x7faadfef20c0 0x7faadfef5082 0x7faadfef5243 0x7faad9eab402 0x7faace245eb0 0x7faace2660b5 0x7faace2699ea 0x7faace269f69 0x7faace26a2d1 0x7faace25ece3 0x7faac9924051 0x7faac977f38d 0x7faac9512087 0x7faac951291e 0x7faac9512b1d 0x7faad2465c92 0x7faad247e6d8 0x7faad24d2e3e 0x7faac9925d7c 0x7faac98afcec 0x7faacec7276e 0x7faacec6f1f3 0x7faaca001313 0x7faadfe51609 0x7faadfd76133\n",
      "[1]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x55dfd1478000 @  0x7f8040ff3d3f 0x7f804102a0c0 0x7f804102d082 0x7f804102d243 0x7f803afe3402 0x7f802f37deb0 0x7f802f39e0b5 0x7f802f3a19ea 0x7f802f3a1f69 0x7f802f3a22d1 0x7f802f396ce3 0x7f802aa5c051 0x7f802a8b738d 0x7f802a64a087 0x7f802a64a91e 0x7f802a64ab1d 0x7f803359dc92 0x7f80335b66d8 0x7f803360ae3e 0x7f802aa5dd7c 0x7f802a9e7cec 0x7f802fdaa76e 0x7f802fda71f3 0x7f802b139313 0x7f8040f89609 0x7f8040eae133\n",
      "[2]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x556a14076000 @  0x7f3ee0c08d3f 0x7f3ee0c3f0c0 0x7f3ee0c42082 0x7f3ee0c42243 0x7f3edabf8402 0x7f3ecef92eb0 0x7f3ecefb30b5 0x7f3ecefb69ea 0x7f3ecefb6f69 0x7f3ecefb72d1 0x7f3ecefabce3 0x7f3eca671051 0x7f3eca4cc38d 0x7f3eca25f087 0x7f3eca25f91e 0x7f3eca25fb1d 0x7f3ed31b2c92 0x7f3ed31cb6d8 0x7f3ed321fe3e 0x7f3eca672d7c 0x7f3eca5fccec 0x7f3ecf9bf76e 0x7f3ecf9bc1f3 0x7f3ecad4e313 0x7f3ee0b9e609 0x7f3ee0ac3133\n",
      "[0]<stderr>:tcmalloc: large alloc 1073741824 bytes == 0x564a2a12a000 @  0x7f69c3087d3f 0x7f69c30be0c0 0x7f69c30c1082 0x7f69c30c1243 0x7f69bd077402 0x7f69b1411eb0 0x7f69b14320b5 0x7f69b14359ea 0x7f69b1435f69 0x7f69b14362d1 0x7f69b142ace3 0x7f69acaf0051 0x7f69ac94b38d 0x7f69ac6de087 0x7f69ac6de91e 0x7f69ac6deb1d 0x7f69b5631c92 0x7f69b564a6d8 0x7f69b569ee3e 0x7f69acaf1d7c 0x7f69aca7bcec 0x7f69b1e3e76e 0x7f69b1e3b1f3 0x7f69ad1cd313 0x7f69c301d609 0x7f69c2f42133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 58/187 [========>.....................] - ETA: 31s - loss: 1.1852 - accuracy: 0.6738 \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x556a569cc000 @  0x7f3ee0c08d3f 0x7f3ee0c3f0c0 0x7f3ee0c42082 0x7f3ee0c42243 0x7f3edabf8402 0x7f3ecef92eb0 0x7f3ecefb30b5 0x7f3ecefb69ea 0x7f3ecefb6f69 0x7f3ecefb72d1 0x7f3ecefabce3 0x7f3eca671051 0x7f3eca4cc38d 0x7f3eca25f087 0x7f3eca25f91e 0x7f3eca25fb1d 0x7f3ed023fbfd 0x7f3eca672d7c 0x7f3eca5fccec 0x7f3ecf9bf76e 0x7f3ecf9bc1f3 0x7f3ecad4e313 0x7f3ee0b9e609 0x7f3ee0ac3133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 62/187 [========>.....................] - ETA: 30s - loss: 1.1850 - accuracy: 0.667\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x562a78740000 @  0x7faadfebbd3f 0x7faadfef20c0 0x7faadfef5082 0x7faadfef5243 0x7faad9eab402 0x7faace245eb0 0x7faace2660b5 0x7faace2699ea 0x7faace269f69 0x7faace26a2d1 0x7faace25ece3 0x7faac9924051 0x7faac977f38d 0x7faac9512087 0x7faac951291e 0x7faac9512b1d 0x7faacf4f2bfd 0x7faac9925d7c 0x7faac98afcec 0x7faacec7276e 0x7faacec6f1f3 0x7faaca001313 0x7faadfe51609 0x7faadfd76133\n",
      "[0]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x564a6cb22000 @  0x7f69c3087d3f 0x7f69c30be0c0 0x7f69c30c1082 0x7f69c30c1243 0x7f69bd077402 0x7f69b1411eb0 0x7f69b14320b5 0x7f69b14359ea 0x7f69b1435f69 0x7f69b14362d1 0x7f69b142ace3 0x7f69acaf0051 0x7f69ac94b38d 0x7f69ac6de087 0x7f69ac6de91e 0x7f69ac6deb1d 0x7f69b26bebfd 0x7f69acaf1d7c 0x7f69aca7bcec 0x7f69b1e3e76e 0x7f69b1e3b1f3 0x7f69ad1cd313 0x7f69c301d609 0x7f69c2f42133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>: 66/187 [=========>....................] - ETA: 29s - loss: 1.1822 - accuracy: 0.667\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 2147483648 bytes == 0x55e013da2000 @  0x7f8040ff3d3f 0x7f804102a0c0 0x7f804102d082 0x7f804102d243 0x7f803afe3402 0x7f802f37deb0 0x7f802f39e0b5 0x7f802f3a19ea 0x7f802f3a1f69 0x7f802f3a22d1 0x7f802f396ce3 0x7f802aa5c051 0x7f802a8b738d 0x7f802a64a087 0x7f802a64a91e 0x7f802a64ab1d 0x7f803603bbf5 0x7f802aa5dd7c 0x7f802a9e7cec 0x7f802fdaa76e 0x7f802fda71f3 0x7f802b139313 0x7f8040f89609 0x7f8040eae133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:186/187 [============================>.] - ETA: 0s - loss: 1.0604 - accuracy: 0.6944\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x55e09b640000 @  0x7f8040ff3d3f 0x7f804102a0c0 0x7f804102d082 0x7f804102d243 0x7f803afe3402 0x7f802f37deb0 0x7f802f39e0b5 0x7f802f3a19ea 0x7f802f3a1f69 0x7f802f3a22d1 0x7f802f396ce3 0x7f802aa5c051 0x7f802a8b738d 0x7f802a64a087 0x7f802a64a91e 0x7f802a64ab1d 0x7f803062abfd 0x7f802aa5dd7c 0x7f802a9e7cec 0x7f802fdaa76e 0x7f802fda71f3 0x7f802b139313 0x7f8040f89609 0x7f8040eae133\n",
      "[2]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x556ade08a000 @  0x7f3ee0c08d3f 0x7f3ee0c3f0c0 0x7f3ee0c42082 0x7f3ee0c42243 0x7f3edabf8402 0x7f3ecef92eb0 0x7f3ecefb30b5 0x7f3ecefb69ea 0x7f3ecefb6f69 0x7f3ecefb72d1 0x7f3ecefabce3 0x7f3eca671051 0x7f3eca4cc38d 0x7f3eca25f087 0x7f3eca25f91e 0x7f3eca25fb1d 0x7f3ed023fbfd 0x7f3eca672d7c 0x7f3eca5fccec 0x7f3ecf9bf76e 0x7f3ecf9bc1f3 0x7f3ecad4e313 0x7f3ee0b9e609 0x7f3ee0ac3133\n",
      "[0]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x564af3d6e000 @  0x7f69c3087d3f 0x7f69c30be0c0 0x7f69c30c1082 0x7f69c30c1243 0x7f69bd077402 0x7f69b1411eb0 0x7f69b14320b5 0x7f69b14359ea 0x7f69b1435f69 0x7f69b14362d1 0x7f69b142ace3 0x7f69acaf0051 0x7f69ac94b38d 0x7f69ac6de087 0x7f69ac6de91e 0x7f69ac6deb1d 0x7f69b26bebfd 0x7f69acaf1d7c 0x7f69aca7bcec 0x7f69b1e3e76e 0x7f69b1e3b1f3 0x7f69ad1cd313 0x7f69c301d609 0x7f69c2f42133\n",
      "[3]<stderr>:tcmalloc: large alloc 4294967296 bytes == 0x562affd66000 @  0x7faadfebbd3f 0x7faadfef20c0 0x7faadfef5082 0x7faadfef5243 0x7faad9eab402 0x7faace245eb0 0x7faace2660b5 0x7faace2699ea 0x7faace269f69 0x7faace26a2d1 0x7faace25ece3 0x7faac9924051 0x7faac977f38d 0x7faac9512087 0x7faac951291e 0x7faac9512b1d 0x7faacf4f2bfd 0x7faac9925d7c 0x7faac98afcec 0x7faacec7276e 0x7faacec6f1f3 0x7faaca001313 0x7faadfe51609 0x7faadfd76133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 89s 426ms/step - loss: 1.0581 - accuracy: 0.6935 - val_loss: 0.2583 - val_accuracy: 0.9161\n",
      "[0]<stdout>:Epoch 2/10\n",
      "[0]<stdout>:187/187 [==============================] - 76s 406ms/step - loss: 0.9438 - accuracy: 0.7213 - val_loss: 0.2376 - val_accuracy: 0.9232\n",
      "[0]<stdout>:Epoch 3/10\n",
      "[0]<stdout>:152/187 [=======================>......] - ETA: 8s - loss: 0.8795 - accuracy: 0.73662\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x55e1a41c2000 @  0x7f8040ff3d3f 0x7f804102a0c0 0x7f804102d082 0x7f804102d243 0x7f803afe3402 0x7f802f37deb0 0x7f802f39e0b5 0x7f802f3a19ea 0x7f802f3a1f69 0x7f802f3a22d1 0x7f802f396ce3 0x7f802aa5c051 0x7f802a8b738d 0x7f802a64a087 0x7f802a64a91e 0x7f802a64ab1d 0x7f803062abfd 0x7f802aa5dd7c 0x7f802a9e7cec 0x7f802fdaa76e 0x7f802fda71f3 0x7f802b139313 0x7f8040f89609 0x7f8040eae133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:156/187 [========================>.....] - ETA: 7s - loss: 0.8788 - accuracy: 0.73\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x564bfcbbe000 @  0x7f69c3087d3f 0x7f69c30be0c0 0x7f69c30c1082 0x7f69c30c1243 0x7f69bd077402 0x7f69b1411eb0 0x7f69b14320b5 0x7f69b14359ea 0x7f69b1435f69 0x7f69b14362d1 0x7f69b142ace3 0x7f69acaf0051 0x7f69ac94b38d 0x7f69ac6de087 0x7f69ac6de91e 0x7f69ac6deb1d 0x7f69b26bebfd 0x7f69acaf1d7c 0x7f69aca7bcec 0x7f69b1e3e76e 0x7f69b1e3b1f3 0x7f69ad1cd313 0x7f69c301d609 0x7f69c2f42133\n",
      "[2]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x556be6afa000 @  0x7f3ee0c08d3f 0x7f3ee0c3f0c0 0x7f3ee0c42082 0x7f3ee0c42243 0x7f3edabf8402 0x7f3ecef92eb0 0x7f3ecefb30b5 0x7f3ecefb69ea 0x7f3ecefb6f69 0x7f3ecefb72d1 0x7f3ecefabce3 0x7f3eca671051 0x7f3eca4cc38d 0x7f3eca25f087 0x7f3eca25f91e 0x7f3eca25fb1d 0x7f3ed023fbfd 0x7f3eca672d7c 0x7f3eca5fccec 0x7f3ecf9bf76e 0x7f3ecf9bc1f3 0x7f3ecad4e313 0x7f3ee0b9e609 0x7f3ee0ac3133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:164/187 [=========================>....] - ETA: 5s - loss: 0.8867 - accuracy: 0.73\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 8589934592 bytes == 0x562c0888a000 @  0x7faadfebbd3f 0x7faadfef20c0 0x7faadfef5082 0x7faadfef5243 0x7faad9eab402 0x7faace245eb0 0x7faace2660b5 0x7faace2699ea 0x7faace269f69 0x7faace26a2d1 0x7faace25ece3 0x7faac9924051 0x7faac977f38d 0x7faac9512087 0x7faac951291e 0x7faac9512b1d 0x7faacf4f2bfd 0x7faac9925d7c 0x7faac98afcec 0x7faacec7276e 0x7faacec6f1f3 0x7faaca001313 0x7faadfe51609 0x7faadfd76133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 78s 415ms/step - loss: 0.8887 - accuracy: 0.7346 - val_loss: 0.2173 - val_accuracy: 0.9338\n",
      "[0]<stdout>:Epoch 4/10\n",
      "[0]<stdout>:187/187 [==============================] - 79s 420ms/step - loss: 0.8456 - accuracy: 0.7453 - val_loss: 0.2024 - val_accuracy: 0.9402\n",
      "[0]<stdout>:Epoch 5/10\n",
      "[0]<stdout>:187/187 [==============================] - 79s 422ms/step - loss: 0.7651 - accuracy: 0.7684 - val_loss: 0.1951 - val_accuracy: 0.9436\n",
      "[0]<stdout>:Epoch 6/10\n",
      "[0]<stdout>:187/187 [==============================] - 78s 415ms/step - loss: 0.7075 - accuracy: 0.7818 - val_loss: 0.1870 - val_accuracy: 0.9426\n",
      "[0]<stdout>:Epoch 7/10\n",
      "[0]<stdout>:187/187 [==============================] - 78s 418ms/step - loss: 0.7181 - accuracy: 0.7861 - val_loss: 0.1765 - val_accuracy: 0.9490\n",
      "[0]<stdout>:Epoch 8/10\n",
      "[0]<stdout>:105/187 [===============>..............] - ETA: 19s - loss: 0.6908 - accuracy: 0.7912\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x55e3b4cd2000 @  0x7f8040ff3d3f 0x7f804102a0c0 0x7f804102d082 0x7f804102d243 0x7f803afe3402 0x7f802f37deb0 0x7f802f39e0b5 0x7f802f3a19ea 0x7f802f3a1f69 0x7f802f3a22d1 0x7f802f396ce3 0x7f802aa5c051 0x7f802a8b738d 0x7f802a64a087 0x7f802a64a91e 0x7f802a64ab1d 0x7f803062abfd 0x7f802aa5dd7c 0x7f802a9e7cec 0x7f802fdaa76e 0x7f802fda71f3 0x7f802b139313 0x7f8040f89609 0x7f8040eae133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:109/187 [================>.............] - ETA: 19s - loss: 0.6974 - accuracy: 0.789\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x556df760a000 @  0x7f3ee0c08d3f 0x7f3ee0c3f0c0 0x7f3ee0c42082 0x7f3ee0c42243 0x7f3edabf8402 0x7f3ecef92eb0 0x7f3ecefb30b5 0x7f3ecefb69ea 0x7f3ecefb6f69 0x7f3ecefb72d1 0x7f3ecefabce3 0x7f3eca671051 0x7f3eca4cc38d 0x7f3eca25f087 0x7f3eca25f91e 0x7f3eca25fb1d 0x7f3ed023fbfd 0x7f3eca672d7c 0x7f3eca5fccec 0x7f3ecf9bf76e 0x7f3ecf9bc1f3 0x7f3ecad4e313 0x7f3ee0b9e609 0x7f3ee0ac3133\n",
      "[0]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x564e0d3be000 @  0x7f69c3087d3f 0x7f69c30be0c0 0x7f69c30c1082 0x7f69c30c1243 0x7f69bd077402 0x7f69b1411eb0 0x7f69b14320b5 0x7f69b14359ea 0x7f69b1435f69 0x7f69b14362d1 0x7f69b142ace3 0x7f69acaf0051 0x7f69ac94b38d 0x7f69ac6de087 0x7f69ac6de91e 0x7f69ac6deb1d 0x7f69b26bebfd 0x7f69acaf1d7c 0x7f69aca7bcec 0x7f69b1e3e76e 0x7f69b1e3b1f3 0x7f69ad1cd313 0x7f69c301d609 0x7f69c2f42133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:117/187 [=================>............] - ETA: 17s - loss: 0.7013 - accuracy: 0.788\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[3]<stderr>:tcmalloc: large alloc 17179869184 bytes == 0x562e1948c000 @  0x7faadfebbd3f 0x7faadfef20c0 0x7faadfef5082 0x7faadfef5243 0x7faad9eab402 0x7faace245eb0 0x7faace2660b5 0x7faace2699ea 0x7faace269f69 0x7faace26a2d1 0x7faace25ece3 0x7faac9924051 0x7faac977f38d 0x7faac9512087 0x7faac951291e 0x7faac9512b1d 0x7faacf4f2bfd 0x7faac9925d7c 0x7faac98afcec 0x7faacec7276e 0x7faacec6f1f3 0x7faaca001313 0x7faadfe51609 0x7faadfd76133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - 79s 420ms/step - loss: 0.7044 - accuracy: 0.7931 - val_loss: 0.1730 - val_accuracy: 0.9484\n",
      "[0]<stdout>:Epoch 9/10\n",
      "[0]<stdout>:187/187 [==============================] - 78s 415ms/step - loss: 0.6612 - accuracy: 0.7914 - val_loss: 0.1640 - val_accuracy: 0.9477\n",
      "[0]<stdout>:Epoch 10/10\n",
      "[0]<stdout>:186/187 [============================>.] - ETA: 0s - loss: 0.6339 - accuracy: 0.80660\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1]<stderr>:2022-06-09 12:28:24.106138: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[2]<stderr>:2022-06-09 12:28:24.789655: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]<stdout>:187/187 [==============================] - ETA: 0s - loss: 0.6321 - accuracy: 0.8021\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "[0]<stdout>:187/187 [==============================] - 78s 413ms/step - loss: 0.6321 - accuracy: 0.8021 - val_loss: 0.1503 - val_accuracy: 0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]<stderr>:2022-06-09 12:28:31.470305: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[3]<stderr>:2022-06-09 12:28:31.847628: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "[1]<stderr>:2022-06-09 12:28:34.064152: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[2]<stderr>:2022-06-09 12:28:35.450440: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[0]<stderr>:2022-06-09 12:28:40.127942: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[3]<stderr>:2022-06-09 12:28:40.497524: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "[1]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[1]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[1]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[1]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[2]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[2]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[2]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[2]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[0]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[0]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[0]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[0]<stderr>:  return generic_utils.serialize_keras_object(obj)\n",
      "[3]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[3]<stderr>:  layer_config = serialize_layer_fn(layer)\n",
      "[3]<stderr>:/opt/conda/envs/nanoWithTf/lib/python3.7/site-packages/keras/saving/saved_model/layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "[3]<stderr>:  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5606bdfba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfreeze_model(model_multiple_horovod)\n",
    "\n",
    "model_multiple_horovod.fit(ds_train,\n",
    "                  epochs=UNFREEZE_EPOCHS,\n",
    "                  steps_per_epoch=FIT_STEPS,\n",
    "                  validation_data=ds_test,\n",
    "                  validation_steps=VAL_STEPS,\n",
    "                  verbose=1,\n",
    "                  num_processes=NUM_PROCESSES,\n",
    "                  backend=\"horovod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00e49de",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 43s 227ms/step - loss: 0.1593 - accuracy: 0.9518\n"
     ]
    }
   ],
   "source": [
    "fit_time_model_multiple_horovod = time() - start\n",
    "acc_model_multiple_horovod = model_multiple_horovod.evaluate(ds_test, steps=VAL_STEPS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51834636",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|        Precision     | Fit Time(s)       | Accuracy(%) |\n",
      "|         Single       |       4159.40       |    96.11    |\n",
      "|        Multiple      |       1746.63       |    95.09    |\n",
      "| Multiple With Horovod|       2548.35       |    95.18    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|        Precision     | Fit Time(s)       | Accuracy(%) |\n",
    "|         Single       |       {:5.2f}       |    {:5.2f}    |\n",
    "|        Multiple      |       {:5.2f}       |    {:5.2f}    |\n",
    "| Multiple With Horovod|       {:5.2f}       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    fit_time_model_single, acc_model_single[1] * 100,\n",
    "    fit_time_model_multiple, acc_model_multiple[1] * 100,\n",
    "    fit_time_model_multiple_horovod, acc_model_multiple_horovod[1] * 100\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32dbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nanoWithTf]",
   "language": "python",
   "name": "conda-env-nanoWithTf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
