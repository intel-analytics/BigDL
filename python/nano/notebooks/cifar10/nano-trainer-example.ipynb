{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Bigdl-nano Resnet example on CIFAR10 dataset\n",
    "---\n",
    "This example illustrates how to apply bigdl-nano optimizations on a image recognition case based on pytorch-lightning framework. The basic image recognition module is implemented with Lightning and trained on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) image recognition Benchmark dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from bigdl.nano.pytorch.trainer import Trainer\n",
    "from bigdl.nano.pytorch.vision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CIFAR10 Data Module\n",
    "---\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(data_path, batch_size, num_workers):\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "    cifar10_dm = CIFAR10DataModule(\n",
    "        data_dir=data_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        train_transforms=train_transforms,\n",
    "        test_transforms=test_transforms,\n",
    "        val_transforms=test_transforms\n",
    "    )\n",
    "    return cifar10_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Resnet\n",
    "---\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lightning Module\n",
    "---\n",
    "Check out the [configure_optimizers](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#configure-optimizers) method to use custom Learning Rate schedulers. The OneCycleLR with SGD will get you to around 92-93% accuracy in 20-30 epochs and 93-94% accuracy in 40-50 epochs. Feel free to experiment with different LR schedules from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, learning_rate=0.05, batch_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // self.hparams.batch_size\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "seed_everything(7)\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "data_module = prepare_data(PATH_DATASETS, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train\n",
    "Use Trainer from bigdl.nano.pytorch.trainer for BigDL-Nano pytorch.\n",
    "\n",
    "This Trainer extends PyTorch Lightning Trainer by adding various options to accelerate pytorch training.\n",
    "\n",
    "```\n",
    "    :param num_processes: number of processes in distributed training. default: 4.\n",
    "    :param use_ipex: whether we use ipex as accelerator for trainer. default: True.\n",
    "    :param cpu_for_each_process: A list of length `num_processes`, each containing a list of\n",
    "            indices of cpus each process will be using. default: None, and the cpu will be\n",
    "            automatically and evenly distributed among processes.\n",
    "```\n",
    "The next few cells show examples of different parameters.\n",
    "#### Single Process\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528977f22717438d974006b6cc8bfad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 7\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b409435fde5341df821c3d7aaad118c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848657477c5d4e4db8cb5fa7e6d5e982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9161999821662903, 'test_loss': 0.25411105155944824}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.05)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_single_none\")\n",
    "basic_trainer = Trainer(num_processes = 1,\n",
    "                  use_ipex = False,\n",
    "                  progress_bar_refresh_rate=10,\n",
    "                  max_epochs=30,\n",
    "                  logger=TensorBoardLogger(\"lightning_logs/\", name=\"basic\"),\n",
    "                  callbacks=[LearningRateMonitor(logging_interval=\"step\"), checkpoint_callback])\n",
    "start = time()\n",
    "basic_trainer.fit(model, datamodule=data_module)\n",
    "basic_fit_time = time() - start\n",
    "outputs = basic_trainer.test(model, datamodule=data_module)\n",
    "basic_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single Process with IPEX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48a194524ec43e5ae643b1fe0800ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed6bb1db0ce467d86557416babea493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747322824d234e489c76f9abb3a73448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9154000282287598, 'test_loss': 0.2526312470436096}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.05)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_single_ipex\")\n",
    "single_ipex_trainer = Trainer(num_processes=1,\n",
    "                        use_ipex = True,\n",
    "                        distributed_backend=\"subprocess\",\n",
    "                        progress_bar_refresh_rate=10,\n",
    "                        max_epochs=30,\n",
    "                        logger=TensorBoardLogger(\"lightning_logs/\", name=\"single_ipex\"),\n",
    "                        callbacks=[LearningRateMonitor(logging_interval=\"step\"), checkpoint_callback])\n",
    "start = time()\n",
    "single_ipex_trainer.fit(model, datamodule=data_module)\n",
    "single_ipex_fit_time = time() - start\n",
    "outputs = single_ipex_trainer.test(model, datamodule=data_module)\n",
    "single_ipex_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multiple Processes with IPEX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:75: LightningDeprecationWarning: Argument `num_nodes` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "  \"Argument `num_nodes` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:81: LightningDeprecationWarning: Argument `sync_batchnorm` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "  \"Argument `sync_batchnorm` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:685: UserWarning: Specified `Precision` and `TrainingType` plugins will be ignored, since an `Accelerator` instance was provided.\n",
      "  \"Specified `Precision` and `TrainingType` plugins will be ignored,\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "Epoch 0:   0%|          | 0/392 [00:00<00:00, 5874.38it/s]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n",
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  82%|████████▏ | 320/392 [01:17<00:17,  4.14it/s, loss=1.61, v_num=9]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  84%|████████▍ | 330/392 [01:18<00:14,  4.24it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  87%|████████▋ | 340/392 [01:18<00:12,  4.33it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  89%|████████▉ | 350/392 [01:19<00:09,  4.41it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  92%|█████████▏| 360/392 [01:20<00:07,  4.49it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  94%|█████████▍| 370/392 [01:21<00:04,  4.57it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  97%|█████████▋| 380/392 [01:22<00:02,  4.65it/s, loss=1.61, v_num=9]\n",
      "Epoch 0:  99%|█████████▉| 390/392 [01:22<00:00,  4.72it/s, loss=1.61, v_num=9]\n",
      "Epoch 0: 100%|██████████| 392/392 [01:23<00:00,  4.72it/s, loss=1.61, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  82%|████████▏ | 320/392 [01:17<00:17,  4.12it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  84%|████████▍ | 330/392 [01:18<00:14,  4.22it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  87%|████████▋ | 340/392 [01:19<00:12,  4.30it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  89%|████████▉ | 350/392 [01:19<00:09,  4.39it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  92%|█████████▏| 360/392 [01:20<00:07,  4.48it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  94%|█████████▍| 370/392 [01:21<00:04,  4.58it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  97%|█████████▋| 380/392 [01:21<00:02,  4.67it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1:  99%|█████████▉| 390/392 [01:22<00:00,  4.76it/s, loss=1.38, v_num=9, val_loss=1.600, val_acc=0.437]\n",
      "Epoch 1: 100%|██████████| 392/392 [01:22<00:00,  4.76it/s, loss=1.35, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  82%|████████▏ | 320/392 [01:13<00:16,  4.36it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  84%|████████▍ | 330/392 [01:14<00:13,  4.46it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  87%|████████▋ | 340/392 [01:15<00:11,  4.54it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  89%|████████▉ | 350/392 [01:15<00:09,  4.63it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  92%|█████████▏| 360/392 [01:16<00:06,  4.71it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  94%|█████████▍| 370/392 [01:17<00:04,  4.80it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  97%|█████████▋| 380/392 [01:17<00:02,  4.90it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2:  99%|█████████▉| 390/392 [01:18<00:00,  4.99it/s, loss=1.17, v_num=9, val_loss=1.340, val_acc=0.517]\n",
      "Epoch 2: 100%|██████████| 392/392 [01:18<00:00,  4.99it/s, loss=1.2, v_num=9, val_loss=1.220, val_acc=0.580] \n",
      "Epoch 3:  82%|████████▏ | 320/392 [01:05<00:14,  4.88it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  84%|████████▍ | 330/392 [01:06<00:12,  4.99it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  87%|████████▋ | 340/392 [01:07<00:10,  5.08it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  89%|████████▉ | 350/392 [01:07<00:08,  5.19it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  92%|█████████▏| 360/392 [01:08<00:06,  5.29it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  94%|█████████▍| 370/392 [01:08<00:04,  5.38it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  97%|█████████▋| 380/392 [01:09<00:02,  5.47it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3:  99%|█████████▉| 390/392 [01:10<00:00,  5.57it/s, loss=1.07, v_num=9, val_loss=1.220, val_acc=0.580]\n",
      "Epoch 3: 100%|██████████| 392/392 [01:10<00:00,  5.56it/s, loss=1.03, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  82%|████████▏ | 320/392 [01:22<00:18,  3.87it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]    \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  84%|████████▍ | 330/392 [01:23<00:15,  3.96it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  87%|████████▋ | 340/392 [01:24<00:12,  4.06it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  89%|████████▉ | 350/392 [01:24<00:10,  4.15it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  92%|█████████▏| 360/392 [01:25<00:07,  4.24it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  94%|█████████▍| 370/392 [01:25<00:05,  4.33it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  97%|█████████▋| 380/392 [01:26<00:02,  4.42it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4:  99%|█████████▉| 390/392 [01:26<00:00,  4.50it/s, loss=1, v_num=9, val_loss=1.180, val_acc=0.609]\n",
      "Epoch 4: 100%|██████████| 392/392 [01:27<00:00,  4.49it/s, loss=1.02, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  82%|████████▏ | 320/392 [01:17<00:17,  4.14it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  84%|████████▍ | 330/392 [01:18<00:14,  4.24it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  87%|████████▋ | 340/392 [01:18<00:12,  4.33it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  89%|████████▉ | 350/392 [01:19<00:09,  4.42it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  92%|█████████▏| 360/392 [01:20<00:07,  4.50it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  94%|█████████▍| 370/392 [01:21<00:04,  4.58it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  97%|█████████▋| 380/392 [01:21<00:02,  4.66it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5:  99%|█████████▉| 390/392 [01:22<00:00,  4.73it/s, loss=0.787, v_num=9, val_loss=1.280, val_acc=0.593]\n",
      "Epoch 5: 100%|██████████| 392/392 [01:23<00:00,  4.72it/s, loss=0.819, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  82%|████████▏ | 320/392 [01:12<00:16,  4.46it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  84%|████████▍ | 330/392 [01:12<00:13,  4.55it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  87%|████████▋ | 340/392 [01:13<00:11,  4.65it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  89%|████████▉ | 350/392 [01:13<00:08,  4.75it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  92%|█████████▏| 360/392 [01:14<00:06,  4.85it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  94%|█████████▍| 370/392 [01:15<00:04,  4.95it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  97%|█████████▋| 380/392 [01:15<00:02,  5.04it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6:  99%|█████████▉| 390/392 [01:16<00:00,  5.14it/s, loss=0.673, v_num=9, val_loss=0.876, val_acc=0.704]\n",
      "Epoch 6: 100%|██████████| 392/392 [01:16<00:00,  5.13it/s, loss=0.682, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  82%|████████▏ | 320/392 [01:20<00:18,  3.98it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  84%|████████▍ | 330/392 [01:21<00:15,  4.07it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  87%|████████▋ | 340/392 [01:22<00:12,  4.16it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  89%|████████▉ | 350/392 [01:22<00:09,  4.25it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  92%|█████████▏| 360/392 [01:23<00:07,  4.34it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  94%|█████████▍| 370/392 [01:23<00:04,  4.43it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  97%|█████████▋| 380/392 [01:24<00:02,  4.52it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7:  99%|█████████▉| 390/392 [01:24<00:00,  4.61it/s, loss=0.761, v_num=9, val_loss=0.931, val_acc=0.699]\n",
      "Epoch 7: 100%|██████████| 392/392 [01:25<00:00,  4.61it/s, loss=0.753, v_num=9, val_loss=0.942, val_acc=0.707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  82%|████████▏ | 320/392 [01:21<00:18,  3.95it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  84%|████████▍ | 330/392 [01:21<00:15,  4.05it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  87%|████████▋ | 340/392 [01:22<00:12,  4.14it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  89%|████████▉ | 350/392 [01:22<00:09,  4.23it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  92%|█████████▏| 360/392 [01:23<00:07,  4.32it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  94%|█████████▍| 370/392 [01:24<00:04,  4.41it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  97%|█████████▋| 380/392 [01:24<00:02,  4.49it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8:  99%|█████████▉| 390/392 [01:25<00:00,  4.57it/s, loss=0.739, v_num=9, val_loss=0.942, val_acc=0.707]\n",
      "Epoch 8: 100%|██████████| 392/392 [01:26<00:00,  4.56it/s, loss=0.735, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  82%|████████▏ | 320/392 [01:19<00:17,  4.06it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  84%|████████▍ | 330/392 [01:19<00:14,  4.14it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  87%|████████▋ | 340/392 [01:20<00:12,  4.23it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  89%|████████▉ | 350/392 [01:21<00:09,  4.31it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  92%|█████████▏| 360/392 [01:22<00:07,  4.39it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  94%|█████████▍| 370/392 [01:23<00:04,  4.46it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  97%|█████████▋| 380/392 [01:23<00:02,  4.54it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9:  99%|█████████▉| 390/392 [01:24<00:00,  4.62it/s, loss=0.594, v_num=9, val_loss=0.689, val_acc=0.765]\n",
      "Epoch 9: 100%|██████████| 392/392 [01:25<00:00,  4.62it/s, loss=0.595, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  82%|████████▏ | 320/392 [01:09<00:15,  4.59it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  84%|████████▍ | 330/392 [01:10<00:13,  4.68it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  87%|████████▋ | 340/392 [01:11<00:10,  4.79it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  89%|████████▉ | 350/392 [01:11<00:08,  4.88it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  92%|█████████▏| 360/392 [01:12<00:06,  4.98it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  94%|█████████▍| 370/392 [01:13<00:04,  5.07it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  97%|█████████▋| 380/392 [01:13<00:02,  5.16it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10:  99%|█████████▉| 390/392 [01:14<00:00,  5.26it/s, loss=0.589, v_num=9, val_loss=0.697, val_acc=0.768]\n",
      "Epoch 10: 100%|██████████| 392/392 [01:15<00:00,  5.24it/s, loss=0.624, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  82%|████████▏ | 320/392 [01:18<00:17,  4.07it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  84%|████████▍ | 330/392 [01:19<00:14,  4.16it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  87%|████████▋ | 340/392 [01:20<00:12,  4.25it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  89%|████████▉ | 350/392 [01:20<00:09,  4.34it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  92%|█████████▏| 360/392 [01:21<00:07,  4.44it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  94%|█████████▍| 370/392 [01:21<00:04,  4.53it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  97%|█████████▋| 380/392 [01:22<00:02,  4.62it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11:  99%|█████████▉| 390/392 [01:23<00:00,  4.71it/s, loss=0.594, v_num=9, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 11: 100%|██████████| 392/392 [01:23<00:00,  4.71it/s, loss=0.609, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  82%|████████▏ | 320/392 [01:13<00:16,  4.35it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  84%|████████▍ | 330/392 [01:14<00:13,  4.45it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  87%|████████▋ | 340/392 [01:15<00:11,  4.54it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  89%|████████▉ | 350/392 [01:15<00:09,  4.64it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  92%|█████████▏| 360/392 [01:16<00:06,  4.73it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  94%|█████████▍| 370/392 [01:16<00:04,  4.82it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  97%|█████████▋| 380/392 [01:17<00:02,  4.91it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12:  99%|█████████▉| 390/392 [01:18<00:00,  4.98it/s, loss=0.548, v_num=9, val_loss=0.789, val_acc=0.731]\n",
      "Epoch 12: 100%|██████████| 392/392 [01:19<00:00,  4.97it/s, loss=0.556, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  82%|████████▏ | 320/392 [01:14<00:16,  4.34it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  84%|████████▍ | 330/392 [01:14<00:14,  4.42it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  87%|████████▋ | 340/392 [01:15<00:11,  4.50it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  89%|████████▉ | 350/392 [01:16<00:09,  4.60it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  92%|█████████▏| 360/392 [01:16<00:06,  4.70it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  94%|█████████▍| 370/392 [01:17<00:04,  4.80it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  97%|█████████▋| 380/392 [01:17<00:02,  4.89it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13:  99%|█████████▉| 390/392 [01:18<00:00,  4.98it/s, loss=0.613, v_num=9, val_loss=0.969, val_acc=0.696]\n",
      "Epoch 13: 100%|██████████| 392/392 [01:18<00:00,  4.98it/s, loss=0.602, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  82%|████████▏ | 320/392 [01:11<00:16,  4.48it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 14:  84%|████████▍ | 330/392 [01:12<00:13,  4.58it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  87%|████████▋ | 340/392 [01:12<00:11,  4.68it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  89%|████████▉ | 350/392 [01:13<00:08,  4.78it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  92%|█████████▏| 360/392 [01:14<00:06,  4.87it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  94%|█████████▍| 370/392 [01:15<00:04,  4.94it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  97%|█████████▋| 380/392 [01:15<00:02,  5.02it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14:  99%|█████████▉| 390/392 [01:16<00:00,  5.10it/s, loss=0.549, v_num=9, val_loss=0.557, val_acc=0.811]\n",
      "Epoch 14: 100%|██████████| 392/392 [01:17<00:00,  5.10it/s, loss=0.545, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  82%|████████▏ | 320/392 [01:10<00:15,  4.57it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  84%|████████▍ | 330/392 [01:11<00:13,  4.65it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  87%|████████▋ | 340/392 [01:11<00:10,  4.74it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  89%|████████▉ | 350/392 [01:12<00:08,  4.82it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  92%|█████████▏| 360/392 [01:13<00:06,  4.91it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  94%|█████████▍| 370/392 [01:14<00:04,  5.01it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  97%|█████████▋| 380/392 [01:14<00:02,  5.10it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15:  99%|█████████▉| 390/392 [01:15<00:00,  5.20it/s, loss=0.595, v_num=9, val_loss=0.584, val_acc=0.801]\n",
      "Epoch 15: 100%|██████████| 392/392 [01:15<00:00,  5.19it/s, loss=0.564, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  82%|████████▏ | 320/392 [01:08<00:15,  4.71it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 16:  84%|████████▍ | 330/392 [01:08<00:12,  4.82it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  87%|████████▋ | 340/392 [01:09<00:10,  4.90it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  89%|████████▉ | 350/392 [01:10<00:08,  4.99it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  92%|█████████▏| 360/392 [01:11<00:06,  5.07it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  94%|█████████▍| 370/392 [01:11<00:04,  5.17it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  97%|█████████▋| 380/392 [01:12<00:02,  5.25it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16:  99%|█████████▉| 390/392 [01:13<00:00,  5.34it/s, loss=0.494, v_num=9, val_loss=0.651, val_acc=0.790]\n",
      "Epoch 16: 100%|██████████| 392/392 [01:13<00:00,  5.33it/s, loss=0.51, v_num=9, val_loss=0.657, val_acc=0.777] \n",
      "Epoch 17:  82%|████████▏ | 320/392 [01:13<00:16,  4.35it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 17:  84%|████████▍ | 330/392 [01:14<00:13,  4.45it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  87%|████████▋ | 340/392 [01:14<00:11,  4.56it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  89%|████████▉ | 350/392 [01:15<00:09,  4.66it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  92%|█████████▏| 360/392 [01:15<00:06,  4.75it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  94%|█████████▍| 370/392 [01:16<00:04,  4.85it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  97%|█████████▋| 380/392 [01:17<00:02,  4.95it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17:  99%|█████████▉| 390/392 [01:17<00:00,  5.04it/s, loss=0.498, v_num=9, val_loss=0.657, val_acc=0.777]\n",
      "Epoch 17: 100%|██████████| 392/392 [01:18<00:00,  5.04it/s, loss=0.498, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  82%|████████▏ | 320/392 [01:13<00:16,  4.35it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  84%|████████▍ | 330/392 [01:14<00:13,  4.43it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  87%|████████▋ | 340/392 [01:15<00:11,  4.52it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  89%|████████▉ | 350/392 [01:16<00:09,  4.62it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  92%|█████████▏| 360/392 [01:16<00:06,  4.72it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  94%|█████████▍| 370/392 [01:17<00:04,  4.81it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  97%|█████████▋| 380/392 [01:17<00:02,  4.91it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18:  99%|█████████▉| 390/392 [01:18<00:00,  5.00it/s, loss=0.499, v_num=9, val_loss=0.676, val_acc=0.783]\n",
      "Epoch 18: 100%|██████████| 392/392 [01:18<00:00,  5.00it/s, loss=0.519, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  82%|████████▏ | 320/392 [01:14<00:16,  4.34it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]  \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  84%|████████▍ | 330/392 [01:14<00:13,  4.43it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  87%|████████▋ | 340/392 [01:15<00:11,  4.53it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  89%|████████▉ | 350/392 [01:15<00:09,  4.63it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  92%|█████████▏| 360/392 [01:16<00:06,  4.73it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  94%|█████████▍| 370/392 [01:17<00:04,  4.82it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  97%|█████████▋| 380/392 [01:17<00:02,  4.90it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19:  99%|█████████▉| 390/392 [01:18<00:00,  4.99it/s, loss=0.54, v_num=9, val_loss=0.658, val_acc=0.785]\n",
      "Epoch 19: 100%|██████████| 392/392 [01:19<00:00,  4.97it/s, loss=0.527, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  82%|████████▏ | 320/392 [01:18<00:17,  4.06it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  84%|████████▍ | 330/392 [01:19<00:14,  4.16it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  87%|████████▋ | 340/392 [01:20<00:12,  4.25it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  89%|████████▉ | 350/392 [01:20<00:09,  4.35it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  92%|█████████▏| 360/392 [01:21<00:07,  4.44it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  94%|█████████▍| 370/392 [01:21<00:04,  4.53it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  97%|█████████▋| 380/392 [01:22<00:02,  4.62it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20:  99%|█████████▉| 390/392 [01:23<00:00,  4.70it/s, loss=0.528, v_num=9, val_loss=0.515, val_acc=0.824]\n",
      "Epoch 20: 100%|██████████| 392/392 [01:23<00:00,  4.69it/s, loss=0.532, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  82%|████████▏ | 320/392 [01:16<00:17,  4.18it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 21:  84%|████████▍ | 330/392 [01:17<00:14,  4.27it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  87%|████████▋ | 340/392 [01:18<00:11,  4.36it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  89%|████████▉ | 350/392 [01:18<00:09,  4.45it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  92%|█████████▏| 360/392 [01:19<00:07,  4.53it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  94%|█████████▍| 370/392 [01:20<00:04,  4.62it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  97%|█████████▋| 380/392 [01:20<00:02,  4.71it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21:  99%|█████████▉| 390/392 [01:21<00:00,  4.80it/s, loss=0.427, v_num=9, val_loss=0.560, val_acc=0.812]\n",
      "Epoch 21: 100%|██████████| 392/392 [01:22<00:00,  4.79it/s, loss=0.427, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  82%|████████▏ | 320/392 [01:09<00:15,  4.64it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]  \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 22:  84%|████████▍ | 330/392 [01:09<00:13,  4.74it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  87%|████████▋ | 340/392 [01:10<00:10,  4.85it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  89%|████████▉ | 350/392 [01:10<00:08,  4.95it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  92%|█████████▏| 360/392 [01:11<00:06,  5.06it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  94%|█████████▍| 370/392 [01:11<00:04,  5.16it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22:  97%|█████████▋| 380/392 [01:12<00:02,  5.26it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22:  99%|█████████▉| 390/392 [01:13<00:00,  5.35it/s, loss=0.44, v_num=9, val_loss=0.538, val_acc=0.820]\n",
      "Epoch 22: 100%|██████████| 392/392 [01:13<00:00,  5.35it/s, loss=0.436, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  82%|████████▏ | 320/392 [01:14<00:16,  4.30it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 23:  84%|████████▍ | 330/392 [01:15<00:14,  4.40it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  87%|████████▋ | 340/392 [01:15<00:11,  4.50it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  89%|████████▉ | 350/392 [01:16<00:09,  4.60it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  92%|█████████▏| 360/392 [01:16<00:06,  4.70it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  94%|█████████▍| 370/392 [01:17<00:04,  4.79it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  97%|█████████▋| 380/392 [01:17<00:02,  4.89it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23:  99%|█████████▉| 390/392 [01:18<00:00,  4.98it/s, loss=0.484, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 23: 100%|██████████| 392/392 [01:18<00:00,  4.98it/s, loss=0.479, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  82%|████████▏ | 320/392 [01:02<00:14,  5.14it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 24:  84%|████████▍ | 330/392 [01:03<00:11,  5.25it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  87%|████████▋ | 340/392 [01:03<00:09,  5.36it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  89%|████████▉ | 350/392 [01:04<00:07,  5.45it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  92%|█████████▏| 360/392 [01:05<00:05,  5.53it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  94%|█████████▍| 370/392 [01:06<00:03,  5.61it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  97%|█████████▋| 380/392 [01:06<00:02,  5.70it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24:  99%|█████████▉| 390/392 [01:07<00:00,  5.80it/s, loss=0.497, v_num=9, val_loss=0.656, val_acc=0.782]\n",
      "Epoch 24: 100%|██████████| 392/392 [01:07<00:00,  5.79it/s, loss=0.514, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  82%|████████▏ | 320/392 [01:09<00:15,  4.60it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  84%|████████▍ | 330/392 [01:10<00:13,  4.69it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  87%|████████▋ | 340/392 [01:11<00:10,  4.80it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  89%|████████▉ | 350/392 [01:11<00:08,  4.90it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  92%|█████████▏| 360/392 [01:12<00:06,  5.00it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  94%|█████████▍| 370/392 [01:12<00:04,  5.10it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  97%|█████████▋| 380/392 [01:13<00:02,  5.20it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25:  99%|█████████▉| 390/392 [01:13<00:00,  5.30it/s, loss=0.471, v_num=9, val_loss=0.732, val_acc=0.765]\n",
      "Epoch 25: 100%|██████████| 392/392 [01:14<00:00,  5.29it/s, loss=0.468, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  82%|████████▏ | 320/392 [01:12<00:16,  4.43it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  84%|████████▍ | 330/392 [01:13<00:13,  4.53it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  87%|████████▋ | 340/392 [01:13<00:11,  4.63it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  89%|████████▉ | 350/392 [01:14<00:08,  4.73it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  92%|█████████▏| 360/392 [01:14<00:06,  4.83it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  94%|█████████▍| 370/392 [01:15<00:04,  4.93it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  97%|█████████▋| 380/392 [01:15<00:02,  5.03it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26:  99%|█████████▉| 390/392 [01:16<00:00,  5.12it/s, loss=0.515, v_num=9, val_loss=0.590, val_acc=0.799]\n",
      "Epoch 26: 100%|██████████| 392/392 [01:16<00:00,  5.12it/s, loss=0.493, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  82%|████████▏ | 320/392 [01:13<00:16,  4.38it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  84%|████████▍ | 330/392 [01:13<00:13,  4.48it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  87%|████████▋ | 340/392 [01:14<00:11,  4.59it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  89%|████████▉ | 350/392 [01:14<00:08,  4.69it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  92%|█████████▏| 360/392 [01:15<00:06,  4.79it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  94%|█████████▍| 370/392 [01:15<00:04,  4.88it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  97%|█████████▋| 380/392 [01:16<00:02,  4.98it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27:  99%|█████████▉| 390/392 [01:17<00:00,  5.08it/s, loss=0.517, v_num=9, val_loss=0.630, val_acc=0.791]\n",
      "Epoch 27: 100%|██████████| 392/392 [01:17<00:00,  5.07it/s, loss=0.506, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  82%|████████▏ | 320/392 [01:21<00:18,  3.96it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  84%|████████▍ | 330/392 [01:21<00:15,  4.05it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  87%|████████▋ | 340/392 [01:22<00:12,  4.14it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  89%|████████▉ | 350/392 [01:23<00:09,  4.23it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  92%|█████████▏| 360/392 [01:23<00:07,  4.32it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  94%|█████████▍| 370/392 [01:24<00:04,  4.41it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  97%|█████████▋| 380/392 [01:24<00:02,  4.50it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28:  99%|█████████▉| 390/392 [01:25<00:00,  4.59it/s, loss=0.488, v_num=9, val_loss=0.585, val_acc=0.800]\n",
      "Epoch 28: 100%|██████████| 392/392 [01:26<00:00,  4.54it/s, loss=0.493, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  82%|████████▏ | 320/392 [01:17<00:17,  4.13it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/79 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 29:  84%|████████▍ | 330/392 [01:18<00:14,  4.23it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  87%|████████▋ | 340/392 [01:18<00:12,  4.32it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  89%|████████▉ | 350/392 [01:19<00:09,  4.42it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  92%|█████████▏| 360/392 [01:19<00:07,  4.52it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  94%|█████████▍| 370/392 [01:20<00:04,  4.61it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  97%|█████████▋| 380/392 [01:21<00:02,  4.70it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29:  99%|█████████▉| 390/392 [01:21<00:00,  4.79it/s, loss=0.496, v_num=9, val_loss=0.723, val_acc=0.766]\n",
      "Epoch 29: 100%|██████████| 392/392 [01:22<00:00,  4.79it/s, loss=0.486, v_num=9, val_loss=0.526, val_acc=0.821]\n",
      "Epoch 29: 100%|██████████| 392/392 [01:22<00:00,  4.78it/s, loss=0.486, v_num=9, val_loss=0.526, val_acc=0.821]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "Testing: 100%|██████████| 79/79 [00:05<00:00, 13.09it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8253999948501587, 'test_loss': 0.5097630620002747}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 79/79 [00:06<00:00, 13.16it/s][WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8253999948501587, 'test_loss': 0.5097630620002747}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.1)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_multi_ipex\")\n",
    "multi_ipex_trainer = Trainer(num_processes=2,\n",
    "                       use_ipex=True,\n",
    "                       distributed_backend=\"subprocess\",\n",
    "                       progress_bar_refresh_rate=10,\n",
    "                       max_epochs=30,\n",
    "                       logger=TensorBoardLogger(\"lightning_logs/\", name=\"multi_ipx\"),\n",
    "                       callbacks=[LearningRateMonitor(logging_interval=\"step\")])\n",
    "start = time()\n",
    "multi_ipex_trainer.fit(model, datamodule=data_module)\n",
    "multi_ipex_fit_time = time() - start\n",
    "outputs = multi_ipex_trainer.test(model, datamodule=data_module)\n",
    "multi_ipex_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|      Precision    | Fit Time(s)         | Accuracy(%) |\n",
      "|        Basic      |       4650.63       |    91.62    |\n",
      "|  Single With Ipex |       4165.50       |    91.54    |\n",
      "| Multiple With Ipex|       2686.55       |    89.80    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|      Precision    | Fit Time(s)         | Accuracy(%) |\n",
    "|        Basic      |       {:5.2f}       |    {:5.2f}    |\n",
    "|  Single With Ipex |       {:5.2f}       |    {:5.2f}    |\n",
    "| Multiple With Ipex|       {:5.2f}       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    basic_fit_time, basic_acc,\n",
    "    single_ipex_fit_time, single_ipex_acc,\n",
    "    multi_ipex_fit_time, multi_ipex_acc\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}