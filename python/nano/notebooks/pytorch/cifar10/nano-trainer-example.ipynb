{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## BigDL-Nano Resnet example on CIFAR10 dataset\n",
    "---\n",
    "This example illustrates how to apply bigdl-nano optimizations on a image recognition case based on pytorch-lightning framework. The basic image recognition module is implemented with Lightning and trained on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) image recognition Benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from bigdl.nano.pytorch.trainer import Trainer\n",
    "from bigdl.nano.pytorch.vision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CIFAR10 Data Module\n",
    "---\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(data_path, batch_size, num_workers):\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "    cifar10_dm = CIFAR10DataModule(\n",
    "        data_dir=data_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        train_transforms=train_transforms,\n",
    "        test_transforms=test_transforms,\n",
    "        val_transforms=test_transforms\n",
    "    )\n",
    "    return cifar10_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Resnet\n",
    "---\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lightning Module\n",
    "---\n",
    "Check out the [configure_optimizers](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#configure-optimizers) method to use custom Learning Rate schedulers. The OneCycleLR with SGD will get you to around 92-93% accuracy in 20-30 epochs and 93-94% accuracy in 40-50 epochs. Feel free to experiment with different LR schedules from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, learning_rate=0.05, steps_per_epoch=45000 , batch_size=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = self.hparams.steps_per_epoch // self.hparams.batch_size\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "seed_everything(7)\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "data_module = prepare_data(PATH_DATASETS, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train\n",
    "Use Trainer from bigdl.nano.pytorch.trainer for BigDL-Nano pytorch.\n",
    "\n",
    "This Trainer extends PyTorch Lightning Trainer by adding various options to accelerate pytorch training.\n",
    "\n",
    "```\n",
    "    :param num_processes: number of processes in distributed training. default: 4.\n",
    "    :param use_ipex: whether we use ipex as accelerator for trainer. default: True.\n",
    "    :param cpu_for_each_process: A list of length `num_processes`, each containing a list of\n",
    "            indices of cpus each process will be using. default: None, and the cpu will be\n",
    "            automatically and evenly distributed among processes.\n",
    "```\n",
    "The next few cells show examples of different parameters.\n",
    "#### Single Process\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd72f5adc4f74015b0c2802254f82748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b558fb059c474e837f73a589a0143d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec454b0cfbe3485f8c969e3799c90b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.902899980545044, 'test_loss': 0.2910485565662384}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.05)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_single_none\")\n",
    "basic_trainer = Trainer(num_processes = 1,\n",
    "                  use_ipex = False,\n",
    "                  progress_bar_refresh_rate=10,\n",
    "                  max_epochs=30,\n",
    "                  logger=TensorBoardLogger(\"lightning_logs/\", name=\"basic\"),\n",
    "                  callbacks=[LearningRateMonitor(logging_interval=\"step\"), checkpoint_callback])\n",
    "start = time()\n",
    "basic_trainer.fit(model, datamodule=data_module)\n",
    "basic_fit_time = time() - start\n",
    "outputs = basic_trainer.test(model, datamodule=data_module)\n",
    "basic_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Single Process with IPEX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a32ab8d5dc44269b760f96e6330a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c975f1141cfd448090b8b07b2eb342ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07a2b48507e4a91a3aa014856867650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.899399995803833, 'test_loss': 0.2945179343223572}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.05)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_single_ipex\")\n",
    "single_ipex_trainer = Trainer(num_processes=1,\n",
    "                        use_ipex = True,\n",
    "                        distributed_backend=\"subprocess\",\n",
    "                        progress_bar_refresh_rate=10,\n",
    "                        max_epochs=30,\n",
    "                        logger=TensorBoardLogger(\"lightning_logs/\", name=\"single_ipex\"),\n",
    "                        callbacks=[LearningRateMonitor(logging_interval=\"step\"), checkpoint_callback])\n",
    "start = time()\n",
    "single_ipex_trainer.fit(model, datamodule=data_module)\n",
    "single_ipex_fit_time = time() - start\n",
    "outputs = single_ipex_trainer.test(model, datamodule=data_module)\n",
    "single_ipex_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Multiple Processes with IPEX\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:75: LightningDeprecationWarning: Argument `num_nodes` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "  \"Argument `num_nodes` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:81: LightningDeprecationWarning: Argument `sync_batchnorm` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. Notice that it will be overriden by the trainer setting.\n",
      "  \"Argument `sync_batchnorm` in `DDPSpawnPlugin` is deprecated in v1.4, and will be removed in v1.6. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:685: UserWarning: Specified `Precision` and `TrainingType` plugins will be ignored, since an `Accelerator` instance was provided.\n",
      "  \"Specified `Precision` and `TrainingType` plugins will be ignored,\"\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "\r",
      "Validation sanity check: 0it [00:00, ?it/s]\r",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]\r",
      "                                                              \r",
      "\r",
      "Training: -1it [00:00, ?it/s]\r",
      "Training:   0%|          | 0/782 [00:00<00:00, 21076.90it/s]\r",
      "Epoch 0:   0%|          | 0/782 [00:00<00:00, 3711.77it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n",
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  81%|████████  | 630/782 [01:44<00:25,  6.02it/s, loss=1.68, v_num=3]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  82%|████████▏ | 640/782 [01:45<00:23,  6.09it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  83%|████████▎ | 650/782 [01:45<00:21,  6.16it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  84%|████████▍ | 660/782 [01:46<00:19,  6.24it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  86%|████████▌ | 670/782 [01:46<00:17,  6.31it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  87%|████████▋ | 680/782 [01:46<00:15,  6.39it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  88%|████████▊ | 690/782 [01:46<00:14,  6.46it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  90%|████████▉ | 700/782 [01:47<00:12,  6.53it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  91%|█████████ | 710/782 [01:47<00:10,  6.61it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  92%|█████████▏| 720/782 [01:47<00:09,  6.68it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  93%|█████████▎| 730/782 [01:48<00:07,  6.75it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  95%|█████████▍| 740/782 [01:48<00:06,  6.83it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  96%|█████████▌| 750/782 [01:48<00:04,  6.90it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  97%|█████████▋| 760/782 [01:49<00:03,  6.97it/s, loss=1.68, v_num=3]\n",
      "Epoch 0:  98%|█████████▊| 770/782 [01:49<00:01,  7.04it/s, loss=1.68, v_num=3]\n",
      "Epoch 0: 100%|█████████▉| 780/782 [01:49<00:00,  7.11it/s, loss=1.68, v_num=3]\n",
      "Epoch 0: 100%|██████████| 782/782 [01:50<00:00,  7.11it/s, loss=1.67, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  81%|████████  | 630/782 [01:39<00:23,  6.35it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  82%|████████▏ | 640/782 [01:39<00:22,  6.42it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  83%|████████▎ | 650/782 [01:40<00:20,  6.50it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  84%|████████▍ | 660/782 [01:40<00:18,  6.58it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  86%|████████▌ | 670/782 [01:40<00:16,  6.65it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  87%|████████▋ | 680/782 [01:41<00:15,  6.73it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  88%|████████▊ | 690/782 [01:41<00:13,  6.80it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  90%|████████▉ | 700/782 [01:41<00:11,  6.88it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  91%|█████████ | 710/782 [01:42<00:10,  6.96it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  92%|█████████▏| 720/782 [01:42<00:08,  7.03it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  93%|█████████▎| 730/782 [01:42<00:07,  7.10it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  95%|█████████▍| 740/782 [01:43<00:05,  7.17it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  96%|█████████▌| 750/782 [01:43<00:04,  7.25it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  97%|█████████▋| 760/782 [01:44<00:03,  7.31it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1:  98%|█████████▊| 770/782 [01:44<00:01,  7.38it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1: 100%|█████████▉| 780/782 [01:44<00:00,  7.45it/s, loss=1.35, v_num=3, val_loss=1.650, val_acc=0.436]\n",
      "Epoch 1: 100%|██████████| 782/782 [01:45<00:00,  7.42it/s, loss=1.31, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  81%|████████  | 630/782 [01:46<00:25,  5.93it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  82%|████████▏ | 640/782 [01:46<00:23,  6.00it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  83%|████████▎ | 650/782 [01:47<00:21,  6.08it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  84%|████████▍ | 660/782 [01:47<00:19,  6.15it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  86%|████████▌ | 670/782 [01:47<00:17,  6.23it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  87%|████████▋ | 680/782 [01:48<00:16,  6.30it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  88%|████████▊ | 690/782 [01:48<00:14,  6.37it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  90%|████████▉ | 700/782 [01:48<00:12,  6.45it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  91%|█████████ | 710/782 [01:49<00:11,  6.52it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  92%|█████████▏| 720/782 [01:49<00:09,  6.59it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  93%|█████████▎| 730/782 [01:49<00:07,  6.66it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  95%|█████████▍| 740/782 [01:49<00:06,  6.74it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  96%|█████████▌| 750/782 [01:50<00:04,  6.81it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  97%|█████████▋| 760/782 [01:50<00:03,  6.88it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2:  98%|█████████▊| 770/782 [01:50<00:01,  6.95it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2: 100%|█████████▉| 780/782 [01:51<00:00,  7.02it/s, loss=1.09, v_num=3, val_loss=1.420, val_acc=0.536]\n",
      "Epoch 2: 100%|██████████| 782/782 [01:51<00:00,  7.02it/s, loss=1.18, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  81%|████████  | 630/782 [01:42<00:24,  6.17it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  82%|████████▏ | 640/782 [01:42<00:22,  6.24it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  83%|████████▎ | 650/782 [01:42<00:20,  6.32it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  84%|████████▍ | 660/782 [01:43<00:19,  6.40it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  86%|████████▌ | 670/782 [01:43<00:17,  6.48it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  87%|████████▋ | 680/782 [01:43<00:15,  6.55it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  88%|████████▊ | 690/782 [01:44<00:13,  6.63it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  90%|████████▉ | 700/782 [01:44<00:12,  6.70it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  91%|█████████ | 710/782 [01:45<00:10,  6.77it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  92%|█████████▏| 720/782 [01:45<00:09,  6.83it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  93%|█████████▎| 730/782 [01:45<00:07,  6.90it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  95%|█████████▍| 740/782 [01:46<00:06,  6.96it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  96%|█████████▌| 750/782 [01:46<00:04,  7.03it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  97%|█████████▋| 760/782 [01:47<00:03,  7.10it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3:  98%|█████████▊| 770/782 [01:47<00:01,  7.17it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3: 100%|█████████▉| 780/782 [01:47<00:00,  7.24it/s, loss=0.984, v_num=3, val_loss=1.360, val_acc=0.560]\n",
      "Epoch 3: 100%|██████████| 782/782 [01:48<00:00,  7.24it/s, loss=0.986, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  81%|████████  | 630/782 [01:30<00:21,  6.96it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  82%|████████▏ | 640/782 [01:31<00:20,  7.03it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  83%|████████▎ | 650/782 [01:31<00:18,  7.11it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  84%|████████▍ | 660/782 [01:32<00:16,  7.18it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  86%|████████▌ | 670/782 [01:32<00:15,  7.25it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  87%|████████▋ | 680/782 [01:32<00:13,  7.32it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  88%|████████▊ | 690/782 [01:33<00:12,  7.40it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  90%|████████▉ | 700/782 [01:33<00:10,  7.47it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  91%|█████████ | 710/782 [01:34<00:09,  7.54it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  92%|█████████▏| 720/782 [01:34<00:08,  7.61it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  93%|█████████▎| 730/782 [01:35<00:06,  7.68it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  95%|█████████▍| 740/782 [01:35<00:05,  7.76it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  96%|█████████▌| 750/782 [01:35<00:04,  7.83it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  97%|█████████▋| 760/782 [01:36<00:02,  7.91it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4:  98%|█████████▊| 770/782 [01:36<00:01,  7.99it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4: 100%|█████████▉| 780/782 [01:36<00:00,  8.07it/s, loss=0.848, v_num=3, val_loss=1.090, val_acc=0.617]\n",
      "Epoch 4: 100%|██████████| 782/782 [01:37<00:00,  8.07it/s, loss=0.886, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  81%|████████  | 630/782 [01:30<00:21,  6.99it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  82%|████████▏ | 640/782 [01:30<00:20,  7.07it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  83%|████████▎ | 650/782 [01:31<00:18,  7.15it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  84%|████████▍ | 660/782 [01:31<00:16,  7.23it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  86%|████████▌ | 670/782 [01:31<00:15,  7.32it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  87%|████████▋ | 680/782 [01:32<00:13,  7.40it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  88%|████████▊ | 690/782 [01:32<00:12,  7.48it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  90%|████████▉ | 700/782 [01:32<00:10,  7.57it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  91%|█████████ | 710/782 [01:32<00:09,  7.65it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  92%|█████████▏| 720/782 [01:33<00:08,  7.72it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  93%|█████████▎| 730/782 [01:33<00:06,  7.80it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  95%|█████████▍| 740/782 [01:34<00:05,  7.87it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  96%|█████████▌| 750/782 [01:34<00:04,  7.94it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  97%|█████████▋| 760/782 [01:34<00:02,  8.02it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5:  98%|█████████▊| 770/782 [01:35<00:01,  8.09it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5: 100%|█████████▉| 780/782 [01:35<00:00,  8.16it/s, loss=0.804, v_num=3, val_loss=1.100, val_acc=0.641]\n",
      "Epoch 5: 100%|██████████| 782/782 [01:35<00:00,  8.16it/s, loss=0.805, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  81%|████████  | 630/782 [01:37<00:23,  6.45it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  82%|████████▏ | 640/782 [01:38<00:21,  6.53it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  83%|████████▎ | 650/782 [01:38<00:19,  6.60it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  84%|████████▍ | 660/782 [01:38<00:18,  6.68it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  86%|████████▌ | 670/782 [01:39<00:16,  6.76it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  87%|████████▋ | 680/782 [01:39<00:14,  6.84it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  88%|████████▊ | 690/782 [01:39<00:13,  6.92it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  90%|████████▉ | 700/782 [01:40<00:11,  7.00it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  91%|█████████ | 710/782 [01:40<00:10,  7.07it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  92%|█████████▏| 720/782 [01:40<00:08,  7.15it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  93%|█████████▎| 730/782 [01:41<00:07,  7.23it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  95%|█████████▍| 740/782 [01:41<00:05,  7.30it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  96%|█████████▌| 750/782 [01:41<00:04,  7.38it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  97%|█████████▋| 760/782 [01:42<00:02,  7.45it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6:  98%|█████████▊| 770/782 [01:42<00:01,  7.53it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6: 100%|█████████▉| 780/782 [01:42<00:00,  7.60it/s, loss=0.757, v_num=3, val_loss=0.891, val_acc=0.701]\n",
      "Epoch 6: 100%|██████████| 782/782 [01:42<00:00,  7.60it/s, loss=0.763, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  81%|████████  | 630/782 [01:34<00:22,  6.68it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 7:  82%|████████▏ | 640/782 [01:34<00:21,  6.76it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  83%|████████▎ | 650/782 [01:35<00:19,  6.84it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  84%|████████▍ | 660/782 [01:35<00:17,  6.92it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  86%|████████▌ | 670/782 [01:35<00:15,  7.01it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  87%|████████▋ | 680/782 [01:36<00:14,  7.09it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  88%|████████▊ | 690/782 [01:36<00:12,  7.17it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  90%|████████▉ | 700/782 [01:36<00:11,  7.25it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  91%|█████████ | 710/782 [01:37<00:09,  7.33it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  92%|█████████▏| 720/782 [01:37<00:08,  7.40it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  93%|█████████▎| 730/782 [01:37<00:06,  7.48it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  95%|█████████▍| 740/782 [01:38<00:05,  7.56it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  96%|█████████▌| 750/782 [01:38<00:04,  7.64it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  97%|█████████▋| 760/782 [01:38<00:02,  7.71it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7:  98%|█████████▊| 770/782 [01:38<00:01,  7.79it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7: 100%|█████████▉| 780/782 [01:39<00:00,  7.87it/s, loss=0.75, v_num=3, val_loss=0.899, val_acc=0.698]\n",
      "Epoch 7: 100%|██████████| 782/782 [01:39<00:00,  7.86it/s, loss=0.763, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  81%|████████  | 630/782 [01:38<00:23,  6.38it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 8:  82%|████████▏ | 640/782 [01:39<00:21,  6.46it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  83%|████████▎ | 650/782 [01:39<00:20,  6.53it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  84%|████████▍ | 660/782 [01:39<00:18,  6.61it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  86%|████████▌ | 670/782 [01:40<00:16,  6.69it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  87%|████████▋ | 680/782 [01:40<00:15,  6.77it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  88%|████████▊ | 690/782 [01:40<00:13,  6.85it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  90%|████████▉ | 700/782 [01:41<00:11,  6.92it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  91%|█████████ | 710/782 [01:41<00:10,  7.00it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  92%|█████████▏| 720/782 [01:41<00:08,  7.08it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:  93%|█████████▎| 730/782 [01:42<00:07,  7.15it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  95%|█████████▍| 740/782 [01:42<00:05,  7.23it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  96%|█████████▌| 750/782 [01:42<00:04,  7.30it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  97%|█████████▋| 760/782 [01:43<00:02,  7.38it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8:  98%|█████████▊| 770/782 [01:43<00:01,  7.45it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8: 100%|█████████▉| 780/782 [01:43<00:00,  7.52it/s, loss=0.816, v_num=3, val_loss=0.814, val_acc=0.727]\n",
      "Epoch 8: 100%|██████████| 782/782 [01:44<00:00,  7.53it/s, loss=0.843, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  81%|████████  | 630/782 [01:39<00:23,  6.35it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 9:  82%|████████▏ | 640/782 [01:39<00:22,  6.43it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  83%|████████▎ | 650/782 [01:40<00:20,  6.51it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  84%|████████▍ | 660/782 [01:40<00:18,  6.58it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  86%|████████▌ | 670/782 [01:40<00:16,  6.66it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  87%|████████▋ | 680/782 [01:41<00:15,  6.73it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  88%|████████▊ | 690/782 [01:41<00:13,  6.81it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  90%|████████▉ | 700/782 [01:41<00:11,  6.88it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  91%|█████████ | 710/782 [01:42<00:10,  6.95it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  92%|█████████▏| 720/782 [01:42<00:08,  7.02it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  93%|█████████▎| 730/782 [01:43<00:07,  7.09it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  95%|█████████▍| 740/782 [01:43<00:05,  7.17it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  96%|█████████▌| 750/782 [01:43<00:04,  7.24it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  97%|█████████▋| 760/782 [01:44<00:03,  7.31it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9:  98%|█████████▊| 770/782 [01:44<00:01,  7.38it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9: 100%|█████████▉| 780/782 [01:44<00:00,  7.45it/s, loss=0.659, v_num=3, val_loss=0.852, val_acc=0.713]\n",
      "Epoch 9: 100%|██████████| 782/782 [01:45<00:00,  7.46it/s, loss=0.7, v_num=3, val_loss=0.861, val_acc=0.720]  \n",
      "Epoch 10:  81%|████████  | 630/782 [01:39<00:23,  6.35it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 10:  82%|████████▏ | 640/782 [01:39<00:22,  6.43it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  83%|████████▎ | 650/782 [01:40<00:20,  6.50it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  84%|████████▍ | 660/782 [01:40<00:18,  6.57it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  86%|████████▌ | 670/782 [01:41<00:16,  6.64it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  87%|████████▋ | 680/782 [01:41<00:15,  6.71it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  88%|████████▊ | 690/782 [01:41<00:13,  6.77it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  90%|████████▉ | 700/782 [01:42<00:11,  6.85it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  91%|█████████ | 710/782 [01:42<00:10,  6.92it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  92%|█████████▏| 720/782 [01:43<00:08,  7.00it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  93%|█████████▎| 730/782 [01:43<00:07,  7.07it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  95%|█████████▍| 740/782 [01:43<00:05,  7.13it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  96%|█████████▌| 750/782 [01:44<00:04,  7.20it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  97%|█████████▋| 760/782 [01:44<00:03,  7.26it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10:  98%|█████████▊| 770/782 [01:45<00:01,  7.32it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10: 100%|█████████▉| 780/782 [01:45<00:00,  7.39it/s, loss=0.682, v_num=3, val_loss=0.861, val_acc=0.720]\n",
      "Epoch 10: 100%|██████████| 782/782 [01:45<00:00,  7.39it/s, loss=0.697, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  81%|████████  | 630/782 [01:34<00:22,  6.65it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 11:  82%|████████▏ | 640/782 [01:35<00:21,  6.72it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  83%|████████▎ | 650/782 [01:35<00:19,  6.80it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  84%|████████▍ | 660/782 [01:36<00:17,  6.88it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  86%|████████▌ | 670/782 [01:36<00:16,  6.96it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  87%|████████▋ | 680/782 [01:36<00:14,  7.04it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  88%|████████▊ | 690/782 [01:37<00:12,  7.12it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  90%|████████▉ | 700/782 [01:37<00:11,  7.20it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  91%|█████████ | 710/782 [01:37<00:09,  7.27it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  92%|█████████▏| 720/782 [01:38<00:08,  7.34it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  93%|█████████▎| 730/782 [01:38<00:07,  7.42it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  95%|█████████▍| 740/782 [01:38<00:05,  7.49it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  96%|█████████▌| 750/782 [01:39<00:04,  7.55it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  97%|█████████▋| 760/782 [01:39<00:02,  7.63it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11:  98%|█████████▊| 770/782 [01:40<00:01,  7.70it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11: 100%|█████████▉| 780/782 [01:40<00:00,  7.77it/s, loss=0.753, v_num=3, val_loss=0.737, val_acc=0.755]\n",
      "Epoch 11: 100%|██████████| 782/782 [01:40<00:00,  7.77it/s, loss=0.816, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  81%|████████  | 630/782 [01:36<00:23,  6.54it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 12:  82%|████████▏ | 640/782 [01:36<00:21,  6.63it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  83%|████████▎ | 650/782 [01:37<00:19,  6.71it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  84%|████████▍ | 660/782 [01:37<00:17,  6.79it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  86%|████████▌ | 670/782 [01:37<00:16,  6.87it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  87%|████████▋ | 680/782 [01:38<00:14,  6.95it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  88%|████████▊ | 690/782 [01:38<00:13,  7.03it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  90%|████████▉ | 700/782 [01:38<00:11,  7.11it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  91%|█████████ | 710/782 [01:38<00:10,  7.18it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  92%|█████████▏| 720/782 [01:39<00:08,  7.26it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  93%|█████████▎| 730/782 [01:39<00:07,  7.34it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  95%|█████████▍| 740/782 [01:39<00:05,  7.41it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  96%|█████████▌| 750/782 [01:40<00:04,  7.49it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  97%|█████████▋| 760/782 [01:40<00:02,  7.57it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12:  98%|█████████▊| 770/782 [01:40<00:01,  7.64it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12: 100%|█████████▉| 780/782 [01:41<00:00,  7.72it/s, loss=0.582, v_num=3, val_loss=0.787, val_acc=0.727]\n",
      "Epoch 12: 100%|██████████| 782/782 [01:41<00:00,  7.72it/s, loss=0.587, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  81%|████████  | 630/782 [01:38<00:23,  6.38it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 13:  82%|████████▏ | 640/782 [01:39<00:22,  6.45it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  83%|████████▎ | 650/782 [01:39<00:20,  6.53it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  84%|████████▍ | 660/782 [01:40<00:18,  6.60it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  86%|████████▌ | 670/782 [01:40<00:16,  6.67it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  87%|████████▋ | 680/782 [01:40<00:15,  6.75it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  88%|████████▊ | 690/782 [01:41<00:13,  6.82it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  90%|████████▉ | 700/782 [01:41<00:11,  6.89it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  91%|█████████ | 710/782 [01:42<00:10,  6.96it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  92%|█████████▏| 720/782 [01:42<00:08,  7.03it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  93%|█████████▎| 730/782 [01:42<00:07,  7.10it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  95%|█████████▍| 740/782 [01:43<00:05,  7.18it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  96%|█████████▌| 750/782 [01:43<00:04,  7.24it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  97%|█████████▋| 760/782 [01:44<00:03,  7.31it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13:  98%|█████████▊| 770/782 [01:44<00:01,  7.37it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13: 100%|█████████▉| 780/782 [01:44<00:00,  7.44it/s, loss=0.768, v_num=3, val_loss=0.905, val_acc=0.702]\n",
      "Epoch 13: 100%|██████████| 782/782 [01:45<00:00,  7.44it/s, loss=0.75, v_num=3, val_loss=0.817, val_acc=0.729] \n",
      "Epoch 14:  81%|████████  | 630/782 [01:40<00:24,  6.30it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 14:  82%|████████▏ | 640/782 [01:40<00:22,  6.38it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  83%|████████▎ | 650/782 [01:40<00:20,  6.46it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  84%|████████▍ | 660/782 [01:41<00:18,  6.54it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  86%|████████▌ | 670/782 [01:41<00:16,  6.61it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  87%|████████▋ | 680/782 [01:41<00:15,  6.68it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  88%|████████▊ | 690/782 [01:42<00:13,  6.75it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  90%|████████▉ | 700/782 [01:42<00:12,  6.82it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  91%|█████████ | 710/782 [01:43<00:10,  6.88it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  92%|█████████▏| 720/782 [01:43<00:08,  6.95it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  93%|█████████▎| 730/782 [01:43<00:07,  7.03it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  95%|█████████▍| 740/782 [01:44<00:05,  7.10it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  96%|█████████▌| 750/782 [01:44<00:04,  7.18it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  97%|█████████▋| 760/782 [01:44<00:03,  7.25it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14:  98%|█████████▊| 770/782 [01:45<00:01,  7.32it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14: 100%|█████████▉| 780/782 [01:45<00:00,  7.40it/s, loss=0.634, v_num=3, val_loss=0.817, val_acc=0.729]\n",
      "Epoch 14: 100%|██████████| 782/782 [01:45<00:00,  7.40it/s, loss=0.611, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  81%|████████  | 630/782 [01:42<00:24,  6.18it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 15:  82%|████████▏ | 640/782 [01:42<00:22,  6.25it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  83%|████████▎ | 650/782 [01:42<00:20,  6.32it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  84%|████████▍ | 660/782 [01:43<00:19,  6.40it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  86%|████████▌ | 670/782 [01:43<00:17,  6.48it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  87%|████████▋ | 680/782 [01:43<00:15,  6.55it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  88%|████████▊ | 690/782 [01:44<00:13,  6.62it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  90%|████████▉ | 700/782 [01:44<00:12,  6.69it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  91%|█████████ | 710/782 [01:45<00:10,  6.77it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  92%|█████████▏| 720/782 [01:45<00:09,  6.84it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  93%|█████████▎| 730/782 [01:45<00:07,  6.91it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  95%|█████████▍| 740/782 [01:46<00:06,  6.98it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  96%|█████████▌| 750/782 [01:46<00:04,  7.06it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  97%|█████████▋| 760/782 [01:46<00:03,  7.13it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15:  98%|█████████▊| 770/782 [01:47<00:01,  7.20it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15: 100%|█████████▉| 780/782 [01:47<00:00,  7.27it/s, loss=0.646, v_num=3, val_loss=0.765, val_acc=0.737]\n",
      "Epoch 15: 100%|██████████| 782/782 [01:47<00:00,  7.28it/s, loss=0.623, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  81%|████████  | 630/782 [01:41<00:24,  6.23it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 16:  82%|████████▏ | 640/782 [01:41<00:22,  6.31it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  83%|████████▎ | 650/782 [01:41<00:20,  6.39it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  84%|████████▍ | 660/782 [01:42<00:18,  6.46it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  86%|████████▌ | 670/782 [01:42<00:17,  6.54it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  87%|████████▋ | 680/782 [01:42<00:15,  6.62it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  88%|████████▊ | 690/782 [01:43<00:13,  6.69it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  90%|████████▉ | 700/782 [01:43<00:12,  6.77it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  91%|█████████ | 710/782 [01:43<00:10,  6.84it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  92%|█████████▏| 720/782 [01:44<00:08,  6.91it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  93%|█████████▎| 730/782 [01:44<00:07,  6.99it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  95%|█████████▍| 740/782 [01:44<00:05,  7.06it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  96%|█████████▌| 750/782 [01:45<00:04,  7.13it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:  97%|█████████▋| 760/782 [01:45<00:03,  7.19it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16:  98%|█████████▊| 770/782 [01:46<00:01,  7.26it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16: 100%|█████████▉| 780/782 [01:46<00:00,  7.34it/s, loss=0.578, v_num=3, val_loss=0.674, val_acc=0.772]\n",
      "Epoch 16: 100%|██████████| 782/782 [01:46<00:00,  7.34it/s, loss=0.582, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  81%|████████  | 630/782 [01:32<00:22,  6.82it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 17:  82%|████████▏ | 640/782 [01:32<00:20,  6.90it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  83%|████████▎ | 650/782 [01:33<00:18,  6.98it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  84%|████████▍ | 660/782 [01:33<00:17,  7.06it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  86%|████████▌ | 670/782 [01:34<00:15,  7.13it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  87%|████████▋ | 680/782 [01:34<00:14,  7.22it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  88%|████████▊ | 690/782 [01:34<00:12,  7.30it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  90%|████████▉ | 700/782 [01:34<00:11,  7.38it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  91%|█████████ | 710/782 [01:35<00:09,  7.46it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  92%|█████████▏| 720/782 [01:35<00:08,  7.54it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  93%|█████████▎| 730/782 [01:35<00:06,  7.62it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  95%|█████████▍| 740/782 [01:36<00:05,  7.70it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  96%|█████████▌| 750/782 [01:36<00:04,  7.78it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  97%|█████████▋| 760/782 [01:36<00:02,  7.85it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17:  98%|█████████▊| 770/782 [01:37<00:01,  7.93it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17: 100%|█████████▉| 780/782 [01:37<00:00,  8.01it/s, loss=0.587, v_num=3, val_loss=0.750, val_acc=0.756]\n",
      "Epoch 17: 100%|██████████| 782/782 [01:37<00:00,  8.01it/s, loss=0.593, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  81%|████████  | 630/782 [01:34<00:22,  6.71it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 18:  82%|████████▏ | 640/782 [01:34<00:20,  6.79it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  83%|████████▎ | 650/782 [01:34<00:19,  6.87it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  84%|████████▍ | 660/782 [01:35<00:17,  6.95it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  86%|████████▌ | 670/782 [01:35<00:15,  7.03it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  87%|████████▋ | 680/782 [01:35<00:14,  7.11it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  88%|████████▊ | 690/782 [01:36<00:12,  7.19it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  90%|████████▉ | 700/782 [01:36<00:11,  7.27it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  91%|█████████ | 710/782 [01:36<00:09,  7.35it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  92%|█████████▏| 720/782 [01:36<00:08,  7.43it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  93%|█████████▎| 730/782 [01:37<00:06,  7.51it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  95%|█████████▍| 740/782 [01:37<00:05,  7.59it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  96%|█████████▌| 750/782 [01:37<00:04,  7.67it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  97%|█████████▋| 760/782 [01:38<00:02,  7.74it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18:  98%|█████████▊| 770/782 [01:38<00:01,  7.82it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18: 100%|█████████▉| 780/782 [01:38<00:00,  7.90it/s, loss=0.682, v_num=3, val_loss=0.729, val_acc=0.764]\n",
      "Epoch 18: 100%|██████████| 782/782 [01:39<00:00,  7.90it/s, loss=0.687, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  81%|████████  | 630/782 [01:39<00:23,  6.37it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 19:  82%|████████▏ | 640/782 [01:39<00:22,  6.44it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  83%|████████▎ | 650/782 [01:39<00:20,  6.51it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  84%|████████▍ | 660/782 [01:40<00:18,  6.59it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  86%|████████▌ | 670/782 [01:40<00:16,  6.67it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  87%|████████▋ | 680/782 [01:40<00:15,  6.75it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  88%|████████▊ | 690/782 [01:41<00:13,  6.83it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  90%|████████▉ | 700/782 [01:41<00:11,  6.90it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  91%|█████████ | 710/782 [01:41<00:10,  6.98it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  92%|█████████▏| 720/782 [01:42<00:08,  7.06it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  93%|█████████▎| 730/782 [01:42<00:07,  7.13it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  95%|█████████▍| 740/782 [01:42<00:05,  7.21it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  96%|█████████▌| 750/782 [01:43<00:04,  7.28it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  97%|█████████▋| 760/782 [01:43<00:02,  7.36it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19:  98%|█████████▊| 770/782 [01:43<00:01,  7.43it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19: 100%|█████████▉| 780/782 [01:44<00:00,  7.50it/s, loss=0.626, v_num=3, val_loss=0.676, val_acc=0.774]\n",
      "Epoch 19: 100%|██████████| 782/782 [01:44<00:00,  7.51it/s, loss=0.607, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  81%|████████  | 630/782 [01:36<00:23,  6.56it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 20:  82%|████████▏ | 640/782 [01:36<00:21,  6.64it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  83%|████████▎ | 650/782 [01:36<00:19,  6.72it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  84%|████████▍ | 660/782 [01:37<00:17,  6.80it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  86%|████████▌ | 670/782 [01:37<00:16,  6.87it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  87%|████████▋ | 680/782 [01:37<00:14,  6.95it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  88%|████████▊ | 690/782 [01:38<00:13,  7.03it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  90%|████████▉ | 700/782 [01:38<00:11,  7.10it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  91%|█████████ | 710/782 [01:39<00:10,  7.16it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  92%|█████████▏| 720/782 [01:39<00:08,  7.24it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  93%|█████████▎| 730/782 [01:39<00:07,  7.32it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  95%|█████████▍| 740/782 [01:40<00:05,  7.39it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  96%|█████████▌| 750/782 [01:40<00:04,  7.47it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  97%|█████████▋| 760/782 [01:40<00:02,  7.54it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20:  98%|█████████▊| 770/782 [01:41<00:01,  7.62it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20: 100%|█████████▉| 780/782 [01:41<00:00,  7.68it/s, loss=0.587, v_num=3, val_loss=0.580, val_acc=0.799]\n",
      "Epoch 20: 100%|██████████| 782/782 [01:41<00:00,  7.69it/s, loss=0.567, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  81%|████████  | 630/782 [01:35<00:22,  6.63it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 21:  82%|████████▏ | 640/782 [01:35<00:21,  6.70it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  83%|████████▎ | 650/782 [01:36<00:19,  6.77it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  84%|████████▍ | 660/782 [01:36<00:17,  6.84it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  86%|████████▌ | 670/782 [01:36<00:16,  6.92it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  87%|████████▋ | 680/782 [01:37<00:14,  7.00it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  88%|████████▊ | 690/782 [01:37<00:13,  7.07it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  90%|████████▉ | 700/782 [01:38<00:11,  7.14it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  91%|█████████ | 710/782 [01:38<00:09,  7.21it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  92%|█████████▏| 720/782 [01:39<00:08,  7.27it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  93%|█████████▎| 730/782 [01:39<00:07,  7.34it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  95%|█████████▍| 740/782 [01:39<00:05,  7.42it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  96%|█████████▌| 750/782 [01:40<00:04,  7.49it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  97%|█████████▋| 760/782 [01:40<00:02,  7.57it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21:  98%|█████████▊| 770/782 [01:40<00:01,  7.65it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21: 100%|█████████▉| 780/782 [01:41<00:00,  7.72it/s, loss=0.523, v_num=3, val_loss=0.654, val_acc=0.783]\n",
      "Epoch 21: 100%|██████████| 782/782 [01:41<00:00,  7.72it/s, loss=0.519, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  81%|████████  | 630/782 [01:26<00:20,  7.26it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 22:  82%|████████▏ | 640/782 [01:27<00:19,  7.35it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  83%|████████▎ | 650/782 [01:27<00:17,  7.43it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  84%|████████▍ | 660/782 [01:27<00:16,  7.51it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  86%|████████▌ | 670/782 [01:28<00:14,  7.59it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  87%|████████▋ | 680/782 [01:28<00:13,  7.67it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  88%|████████▊ | 690/782 [01:29<00:11,  7.75it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  90%|████████▉ | 700/782 [01:29<00:10,  7.84it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  91%|█████████ | 710/782 [01:29<00:09,  7.92it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  92%|█████████▏| 720/782 [01:30<00:07,  8.00it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  93%|█████████▎| 730/782 [01:30<00:06,  8.09it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  95%|█████████▍| 740/782 [01:30<00:05,  8.17it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  96%|█████████▌| 750/782 [01:31<00:03,  8.25it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  97%|█████████▋| 760/782 [01:31<00:02,  8.33it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22:  98%|█████████▊| 770/782 [01:31<00:01,  8.40it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22: 100%|█████████▉| 780/782 [01:32<00:00,  8.47it/s, loss=0.527, v_num=3, val_loss=0.616, val_acc=0.793]\n",
      "Epoch 22: 100%|██████████| 782/782 [01:32<00:00,  8.47it/s, loss=0.493, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  81%|████████  | 630/782 [01:22<00:19,  7.63it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]  \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 23:  82%|████████▏ | 640/782 [01:23<00:18,  7.72it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  83%|████████▎ | 650/782 [01:23<00:16,  7.81it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  84%|████████▍ | 660/782 [01:23<00:15,  7.90it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  86%|████████▌ | 670/782 [01:23<00:14,  7.99it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  87%|████████▋ | 680/782 [01:24<00:12,  8.08it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  88%|████████▊ | 690/782 [01:24<00:11,  8.17it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  90%|████████▉ | 700/782 [01:24<00:09,  8.25it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  91%|█████████ | 710/782 [01:25<00:08,  8.34it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  92%|█████████▏| 720/782 [01:25<00:07,  8.43it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  93%|█████████▎| 730/782 [01:25<00:06,  8.51it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  95%|█████████▍| 740/782 [01:26<00:04,  8.60it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  96%|█████████▌| 750/782 [01:26<00:03,  8.68it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  97%|█████████▋| 760/782 [01:26<00:02,  8.76it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23:  98%|█████████▊| 770/782 [01:27<00:01,  8.85it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23: 100%|█████████▉| 780/782 [01:27<00:00,  8.93it/s, loss=0.48, v_num=3, val_loss=0.552, val_acc=0.814]\n",
      "Epoch 23: 100%|██████████| 782/782 [01:27<00:00,  8.92it/s, loss=0.488, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  81%|████████  | 630/782 [01:33<00:22,  6.72it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 24:  82%|████████▏ | 640/782 [01:34<00:20,  6.80it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  83%|████████▎ | 650/782 [01:34<00:19,  6.88it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  84%|████████▍ | 660/782 [01:35<00:17,  6.95it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  86%|████████▌ | 670/782 [01:35<00:15,  7.02it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  87%|████████▋ | 680/782 [01:35<00:14,  7.10it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  88%|████████▊ | 690/782 [01:36<00:12,  7.18it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  90%|████████▉ | 700/782 [01:36<00:11,  7.26it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  91%|█████████ | 710/782 [01:36<00:09,  7.34it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  92%|█████████▏| 720/782 [01:37<00:08,  7.42it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  93%|█████████▎| 730/782 [01:37<00:06,  7.50it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  95%|█████████▍| 740/782 [01:37<00:05,  7.57it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  96%|█████████▌| 750/782 [01:38<00:04,  7.65it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  97%|█████████▋| 760/782 [01:38<00:02,  7.73it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24:  98%|█████████▊| 770/782 [01:38<00:01,  7.80it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24: 100%|█████████▉| 780/782 [01:39<00:00,  7.88it/s, loss=0.524, v_num=3, val_loss=0.647, val_acc=0.779]\n",
      "Epoch 24: 100%|██████████| 782/782 [01:39<00:00,  7.88it/s, loss=0.549, v_num=3, val_loss=0.636, val_acc=0.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25:  81%|████████  | 630/782 [01:38<00:23,  6.44it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 25:  82%|████████▏ | 640/782 [01:38<00:21,  6.51it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  83%|████████▎ | 650/782 [01:38<00:20,  6.59it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  84%|████████▍ | 660/782 [01:39<00:18,  6.66it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  86%|████████▌ | 670/782 [01:39<00:16,  6.73it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  87%|████████▋ | 680/782 [01:39<00:14,  6.81it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  88%|████████▊ | 690/782 [01:40<00:13,  6.89it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  90%|████████▉ | 700/782 [01:40<00:11,  6.96it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  91%|█████████ | 710/782 [01:41<00:10,  7.04it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  92%|█████████▏| 720/782 [01:41<00:08,  7.11it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  93%|█████████▎| 730/782 [01:41<00:07,  7.18it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  95%|█████████▍| 740/782 [01:42<00:05,  7.26it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  96%|█████████▌| 750/782 [01:42<00:04,  7.33it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  97%|█████████▋| 760/782 [01:42<00:02,  7.41it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25:  98%|█████████▊| 770/782 [01:43<00:01,  7.48it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25: 100%|█████████▉| 780/782 [01:43<00:00,  7.55it/s, loss=0.482, v_num=3, val_loss=0.636, val_acc=0.787]\n",
      "Epoch 25: 100%|██████████| 782/782 [01:43<00:00,  7.56it/s, loss=0.469, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  81%|████████  | 630/782 [01:40<00:24,  6.28it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 26:  82%|████████▏ | 640/782 [01:40<00:22,  6.36it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  83%|████████▎ | 650/782 [01:41<00:20,  6.44it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  84%|████████▍ | 660/782 [01:41<00:18,  6.51it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  86%|████████▌ | 670/782 [01:41<00:17,  6.59it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  87%|████████▋ | 680/782 [01:42<00:15,  6.65it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  88%|████████▊ | 690/782 [01:42<00:13,  6.73it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  90%|████████▉ | 700/782 [01:43<00:12,  6.80it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  91%|█████████ | 710/782 [01:43<00:10,  6.87it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  92%|█████████▏| 720/782 [01:43<00:08,  6.94it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  93%|█████████▎| 730/782 [01:44<00:07,  7.01it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  95%|█████████▍| 740/782 [01:44<00:05,  7.08it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  96%|█████████▌| 750/782 [01:44<00:04,  7.15it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  97%|█████████▋| 760/782 [01:45<00:03,  7.23it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26:  98%|█████████▊| 770/782 [01:45<00:01,  7.30it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26: 100%|█████████▉| 780/782 [01:45<00:00,  7.37it/s, loss=0.421, v_num=3, val_loss=0.456, val_acc=0.845]\n",
      "Epoch 26: 100%|██████████| 782/782 [01:46<00:00,  7.35it/s, loss=0.396, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  81%|████████  | 630/782 [01:39<00:23,  6.37it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 27:  82%|████████▏ | 640/782 [01:39<00:22,  6.45it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  83%|████████▎ | 650/782 [01:39<00:20,  6.52it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  84%|████████▍ | 660/782 [01:40<00:18,  6.60it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  86%|████████▌ | 670/782 [01:40<00:16,  6.68it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  87%|████████▋ | 680/782 [01:40<00:15,  6.75it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  88%|████████▊ | 690/782 [01:41<00:13,  6.83it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  90%|████████▉ | 700/782 [01:41<00:11,  6.90it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  91%|█████████ | 710/782 [01:41<00:10,  6.98it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  92%|█████████▏| 720/782 [01:42<00:08,  7.05it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  93%|█████████▎| 730/782 [01:42<00:07,  7.12it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  95%|█████████▍| 740/782 [01:43<00:05,  7.19it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  96%|█████████▌| 750/782 [01:43<00:04,  7.27it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  97%|█████████▋| 760/782 [01:43<00:02,  7.34it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27:  98%|█████████▊| 770/782 [01:43<00:01,  7.42it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27: 100%|█████████▉| 780/782 [01:44<00:00,  7.49it/s, loss=0.431, v_num=3, val_loss=0.500, val_acc=0.835]\n",
      "Epoch 27: 100%|██████████| 782/782 [01:44<00:00,  7.48it/s, loss=0.381, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  81%|████████  | 630/782 [01:42<00:24,  6.17it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 28:  82%|████████▏ | 640/782 [01:42<00:22,  6.25it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  83%|████████▎ | 650/782 [01:42<00:20,  6.32it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  84%|████████▍ | 660/782 [01:43<00:19,  6.40it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  86%|████████▌ | 670/782 [01:43<00:17,  6.47it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  87%|████████▋ | 680/782 [01:44<00:15,  6.54it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  88%|████████▊ | 690/782 [01:44<00:13,  6.61it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  90%|████████▉ | 700/782 [01:45<00:12,  6.67it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  91%|█████████ | 710/782 [01:45<00:10,  6.74it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  92%|█████████▏| 720/782 [01:45<00:09,  6.80it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  93%|█████████▎| 730/782 [01:46<00:07,  6.88it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  95%|█████████▍| 740/782 [01:46<00:06,  6.95it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  96%|█████████▌| 750/782 [01:47<00:04,  7.01it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  97%|█████████▋| 760/782 [01:47<00:03,  7.08it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28:  98%|█████████▊| 770/782 [01:47<00:01,  7.15it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28: 100%|█████████▉| 780/782 [01:48<00:00,  7.22it/s, loss=0.38, v_num=3, val_loss=0.389, val_acc=0.869]\n",
      "Epoch 28: 100%|██████████| 782/782 [01:48<00:00,  7.23it/s, loss=0.35, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  81%|████████  | 630/782 [01:42<00:24,  6.16it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872] \n",
      "Validating: 0it [00:00, ?it/s]\u001B[A\n",
      "Validating:   0%|          | 0/157 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 29:  82%|████████▏ | 640/782 [01:42<00:22,  6.24it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  83%|████████▎ | 650/782 [01:43<00:20,  6.31it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  84%|████████▍ | 660/782 [01:43<00:19,  6.39it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  86%|████████▌ | 670/782 [01:43<00:17,  6.47it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  87%|████████▋ | 680/782 [01:44<00:15,  6.54it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  88%|████████▊ | 690/782 [01:44<00:13,  6.62it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  90%|████████▉ | 700/782 [01:44<00:12,  6.70it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  91%|█████████ | 710/782 [01:45<00:10,  6.77it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  92%|█████████▏| 720/782 [01:45<00:09,  6.84it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  93%|█████████▎| 730/782 [01:45<00:07,  6.92it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  95%|█████████▍| 740/782 [01:45<00:06,  6.99it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  96%|█████████▌| 750/782 [01:46<00:04,  7.06it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  97%|█████████▋| 760/782 [01:46<00:03,  7.14it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29:  98%|█████████▊| 770/782 [01:46<00:01,  7.21it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29: 100%|█████████▉| 780/782 [01:47<00:00,  7.28it/s, loss=0.34, v_num=3, val_loss=0.370, val_acc=0.872]\n",
      "Epoch 29: 100%|██████████| 782/782 [01:47<00:00,  7.28it/s, loss=0.325, v_num=3, val_loss=0.309, val_acc=0.895]\n",
      "                                                             \u001B[A[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "Epoch 29: 100%|██████████| 782/782 [01:47<00:00,  7.25it/s, loss=0.325, v_num=3, val_loss=0.309, val_acc=0.895]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "Testing: 100%|██████████| 157/157 [00:07<00:00, 26.50it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8849999904632568, 'test_loss': 0.342165470123291}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 157/157 [00:07<00:00, 22.33it/s]\n",
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8849999904632568, 'test_loss': 0.342165470123291}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/home/mingzhi/anaconda3/envs/bigdlBasicEnv/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.1, batch_size=64)\n",
    "model.datamodule = data_module\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", filename=\"renet18_multi_ipex\")\n",
    "multi_ipex_trainer = Trainer(num_processes=2,\n",
    "                       use_ipex=True,\n",
    "                       distributed_backend=\"subprocess\",\n",
    "                       progress_bar_refresh_rate=10,\n",
    "                       max_epochs=30,\n",
    "                       logger=TensorBoardLogger(\"lightning_logs/\", name=\"multi_ipx\"),\n",
    "                       callbacks=[LearningRateMonitor(logging_interval=\"step\")])\n",
    "start = time()\n",
    "multi_ipex_trainer.fit(model, datamodule=data_module)\n",
    "multi_ipex_fit_time = time() - start\n",
    "outputs = multi_ipex_trainer.test(model, datamodule=data_module)\n",
    "multi_ipex_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|      Precision    | Fit Time(s)       | Accuracy(%) |\n",
      "|        Basic      |       7212.29       |    90.29    |\n",
      "|  Single With Ipex |       5771.51       |    89.94    |\n",
      "| Multiple With Ipex|       3103.78       |    88.50    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|      Precision    | Fit Time(s)       | Accuracy(%) |\n",
    "|        Basic      |       {:5.2f}       |    {:5.2f}    |\n",
    "|  Single With Ipex |       {:5.2f}       |    {:5.2f}    |\n",
    "| Multiple With Ipex|       {:5.2f}       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    basic_fit_time, basic_acc,\n",
    "    single_ipex_fit_time, single_ipex_acc,\n",
    "    multi_ipex_fit_time, multi_ipex_acc\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}