{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we'll demonstrate how to use BigDL-Nano to accelerate the LightningLite tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment\n",
    "\n",
    "Before you start with APIs delivered by BigDL-Nano, you have to make sure BigDL-Nano is correctly installed for PyTorch. If not, please follow [this](../../../../../docs/readthedocs/source/doc/Nano/Overview/nano.md) to setup your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cifar10 Dataset\n",
    "\n",
    "Import Cifar10 dataset from torch_vision and modify the train transform. You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset.\n",
    "\n",
    "Leveraging OpenCV and libjpeg-turbo, BigDL-Nano can accelerate computer vision data pipelines by providing a drop-in replacement of torch_vision's `datasets` and `transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from bigdl.nano.pytorch.vision import transforms\n",
    "from bigdl.nano.pytorch.vision.datasets import CIFAR10\n",
    "\n",
    "def create_dataloader(data_path, batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.Resize(128),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = CIFAR10(root=data_path, train=True,\n",
    "                            download=True, transform=train_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=0)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model\n",
    "\n",
    "We use the Resnet18 module but add a Linear layer to change its output size to 10, because the CIFAR10 dataset has 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from bigdl.nano.pytorch.vision.models import vision\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, include_top=False, freeze=True):\n",
    "        super().__init__()\n",
    "        backbone = vision.resnet18(pretrained=pretrained, include_top=include_top, freeze=freeze)\n",
    "        output_size = backbone.get_output_size()\n",
    "        head = nn.Linear(output_size, num_classes)\n",
    "        self.model = nn.Sequential(backbone, head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Train Loop\n",
    "\n",
    "The `LightningLite` (`bigdl.nano.pytorch.lite.LightningLite`) class is the place where we integrate most optimizations. It extends PyTorch Lightning's `LightningLite` class and has a few more parameters and methods specific to BigDL-Nano.\n",
    "\n",
    "Our `LightningLite` can be directly used to replace PyTorch Lightning's, all optimizations will be applied automatically, you don't need to change any training codes.\n",
    "\n",
    "We define the train loop in the overrided `run` method of `LightningLite`, which is required by PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from bigdl.nano.pytorch.lite import LightningLite\n",
    "\n",
    "\n",
    "data_path = os.environ.get(\"DATA_PATH\", \".\")\n",
    "batch_size = 256\n",
    "max_epochs = 3\n",
    "\n",
    "\n",
    "class Lite(LightningLite):\n",
    "    def run(self):\n",
    "        model = ResNet18(10, pretrained=False, include_top=False, freeze=True)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        train_loader = create_dataloader(data_path, batch_size)\n",
    "\n",
    "        model, optimizer = self.setup(model, optimizer)\n",
    "        train_loader = self.setup_dataloaders(train_loader)\n",
    "        model.train()\n",
    "\n",
    "        for _i in range(max_epochs):\n",
    "            total_loss, num = 0, 0\n",
    "            for X, y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                l = loss(model(X), y)\n",
    "                self.backward(l)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += l.sum()\n",
    "                num += 1\n",
    "            print(f'avg_loss: {total_loss / num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train in Non-distributed Mode\n",
    "\n",
    "To run the train loop, we only need to create an instance of `LightningLite` and call its `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "avg_loss: 2.19347882270813\n",
      "avg_loss: 1.9237079620361328\n",
      "avg_loss: 1.8780473470687866\n"
     ]
    }
   ],
   "source": [
    "Lite().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Extension for Pytorch (a.k.a [IPEX](https://github.com/intel/intel-extension-for-pytorch)) extends Pytorch with optimizations on intel hardware. BigDL-Nano also integrates IPEX into the `LightningLite`, you can turn on IPEX optimization by setting `use_ipex=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "avg_loss: 2.1605348587036133\n",
      "avg_loss: 1.9018423557281494\n",
      "avg_loss: 1.8672975301742554\n"
     ]
    }
   ],
   "source": [
    "Lite(use_ipex=True).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train in Distributed Mode\n",
    "\n",
    "You can set the number of processes to enable distributed training to acclerate training. You can also set different distributed strategies, now BigDL-Nano supports 'spawn', 'subprocess' and 'ray', the default strategy is 'subprocess'.\n",
    "\n",
    "- Note: only the 'subprocess' strategy can be used in interactive environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/nano-dev/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/root/miniconda3/envs/nano-dev/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 2.3727188110351562\n",
      "avg_loss: 2.3467695713043213\n",
      "avg_loss: 1.9336925745010376\n",
      "avg_loss: 1.9219833612442017\n",
      "avg_loss: 1.8722683191299438avg_loss: 1.875815987586975\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lite(num_processes=2, strategy=\"subprocess\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can enable both distributed training and IPEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n",
      "[WARNING] \"import intel_pytorch_extension\" will be deprecated in future releases. Please use \"import torch_ipex\" instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n",
      "[W LegacyTypeDispatch.h:79] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 2.372002363204956\n",
      "avg_loss: 2.344726085662842\n",
      "avg_loss: 1.9626035690307617\n",
      "avg_loss: 1.9547721147537231\n",
      "avg_loss: 1.906415343284607\n",
      "avg_loss: 1.9092830419540405\n"
     ]
    }
   ],
   "source": [
    "Lite(use_ipex=True, num_processes=2, strategy=\"subprocess\").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nano-dev': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dda1d5f709f7f060022bc27c348a281835c405e1e2acbb42e3d907d6ed3046bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
