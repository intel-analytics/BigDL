{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantiza Model with Post-training Optimization Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment\n",
    "Before you start with Apis delivered by bigdl-nano, you have to make sure BigDL-Nano is correctly installed for PyTorch. If not, please follow [this](../../../../../docs/readthedocs/source/doc/Nano/Overview/nano.md) to set up your environment.<br><br>\n",
    "The POT(Post-training Optimization Tools) is provided by OpenVINO toolkit. To use POT, you need to install OpenVINO.\n",
    "```bash\n",
    "pip install openvino-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load Data\n",
    "We used the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) for demo, which contains 37 categories with roughly 200 images for each classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "val_transform = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "train_dataset = OxfordIIITPet(root = \".\", transform=train_transform)\n",
    "val_dataset = OxfordIIITPet(root=\".\", transform=val_transform)\n",
    "# obtain training indices that will be used for validation\n",
    "indices = torch.randperm(len(train_dataset))\n",
    "val_size = len(train_dataset) // 4\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, indices[:-val_size])\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, indices[-val_size:])\n",
    "# prepare data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Custom Model\n",
    "Regarding the model, we used pretrained torchvision.models.resnet18. More details, please refer to [here](https://pytorch.org/vision/0.12/generated/torchvision.models.resnet18.html?highlight=resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:532: LightningDeprecationWarning: `trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6. Use `trainer.fit(train_dataloaders)` instead. HINT: added 's'\n",
      "  \"`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\"\n",
      "/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  rank_zero_warn(f\"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop\")\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | ResNet           | 11.2 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.782    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 87/87 [00:42<00:00,  2.07it/s, loss=0.316, v_num=2]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 16])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from bigdl.nano.pytorch import Trainer\n",
    "model_ft = resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Here the size of each output sample is set to 37.\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, 37)\n",
    "loss_ft = torch.nn.CrossEntropyLoss()\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Compile our model with loss function, optimizer.\n",
    "model = Trainer.compile(model_ft, loss_ft, optimizer_ft)\n",
    "trainer = Trainer(max_epochs=5)\n",
    "trainer.fit(model, train_dataloader=train_dataloader)\n",
    "\n",
    "# Inference/Prediction\n",
    "x = torch.stack([val_dataset[0][0], val_dataset[1][0]])\n",
    "model_ft.eval()\n",
    "y_hat = model_ft(x)\n",
    "y_hat.argmax(dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Quantization with Post-training Optimization Tools\n",
    "Add quantization as follow, accelerator='openvino' means using OpenVINO POT to do quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/tmp/tmp_h5tiarj/tmp.onnx\n",
      "\t- Path for generated IR: \t/tmp/tmpui3v8zys\n",
      "\t- IR output name: \ttmp\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \t/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpui3v8zys/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpui3v8zys/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.48 seconds. \n",
      "[ SUCCESS ] Memory consumed: 181 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7329e+01, -3.2855e+00, -1.1449e+00, -9.7942e-01, -1.5179e-01,\n",
       "          9.2083e+00, -3.6007e+00, -5.1113e-01, -1.1325e+00,  3.0623e+00,\n",
       "          3.0486e+00,  5.7270e+00, -1.7772e+00, -4.8688e+00,  4.2304e-01,\n",
       "         -6.5337e+00, -3.7628e+00, -2.1463e+00, -5.7218e+00, -3.9071e+00,\n",
       "          4.8865e+00,  1.8713e+00, -6.4810e+00, -1.7373e+00,  9.8768e-01,\n",
       "         -4.9941e+00,  2.6608e+00,  4.8284e+00, -2.7764e+00, -4.6965e+00,\n",
       "         -3.5127e+00,  5.5518e+00,  3.3330e+00,  2.4686e+00, -5.0390e+00,\n",
       "         -1.3475e+00, -2.0684e-01],\n",
       "        [ 1.1245e+01, -3.0037e+00, -1.7740e-04, -1.8865e+00,  1.3851e+00,\n",
       "          9.7956e+00, -2.2753e+00,  5.0934e-01, -4.8474e-01,  1.0725e+00,\n",
       "          4.4877e+00,  4.0109e+00, -1.5552e+00, -4.1258e+00, -1.4994e+00,\n",
       "         -5.8474e+00, -2.3569e+00, -7.3282e-01, -5.3210e+00, -3.4000e+00,\n",
       "          2.7175e+00,  2.7583e+00, -4.3755e+00, -1.7895e+00,  8.6411e-01,\n",
       "         -2.5912e+00,  1.1335e+00,  2.4955e+00, -1.2317e+00, -5.0416e+00,\n",
       "         -2.3891e+00,  1.6632e+00,  2.0518e+00,  3.1253e+00, -4.5545e+00,\n",
       "         -1.3066e+00,  2.2226e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "ov_q_model = trainer.quantize(model, accelerator=\"openvino\", calib_dataloader=data_loader)\n",
    "batch = torch.stack([data_set[0][0], data_set[1][0]])\n",
    "ov_q_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('testOpenvino')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf698c9356c4f0740ac364d5e27207f5ab66fab651ea6afd333d7f1671f1a40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
