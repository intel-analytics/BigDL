{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantiza Model with Post-training Optimization Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment\n",
    "Before you start with Apis delivered by bigdl-nano, you have to make sure BigDL-Nano is correctly installed for PyTorch. If not, please follow [this](../../../../../docs/readthedocs/source/doc/Nano/Overview/nano.md) to set up your environment.<br><br>\n",
    "The POT(Post-training Optimization Tools) is provided by OpenVINO toolkit. To use POT, you need to install OpenVINO.\n",
    "```bash\n",
    "pip install openvino-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Load Data\n",
    "We used the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) for demo, which contains 37 categories with roughly 200 images for each classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "data_set = OxfordIIITPet(root=\"./data/\", transform=data_transforms,\n",
    "                         target_transform=transforms.Lambda(lambda label: torch.tensor(label, dtype=torch.long)))\n",
    "data_loader = DataLoader(data_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Custom Model\n",
    "Regarding the model, we used pretrained torchvision.models.resnet18. More details, please refer to [here](https://pytorch.org/vision/0.12/generated/torchvision.models.resnet18.html?highlight=resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | ResNet           | 11.2 M\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.782    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 115/115 [00:56<00:00,  2.06it/s, loss=0.393, v_num=42] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# define your own model\n",
    "model_ft = resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(data_set.classes))\n",
    "loss_ft = nn.CrossEntropyLoss()\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "from bigdl.nano.pytorch import Trainer\n",
    "model = Trainer.compile(model_ft, loss_ft, optimizer_ft)\n",
    "# (Optional) Something else, like training ...\n",
    "trainer = Trainer(max_epochs=5)\n",
    "trainer.fit(model, train_dataloader=data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Quantization with Post-training Optimization Tools\n",
    "Add quantization as follow, accelerator='openvino' means using OpenVINO POT to do quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/tmp/tmp_h5tiarj/tmp.onnx\n",
      "\t- Path for generated IR: \t/tmp/tmpui3v8zys\n",
      "\t- IR output name: \ttmp\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \t/opt/conda/envs/testOpenvino/lib/python3.7/site-packages/openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpui3v8zys/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpui3v8zys/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.48 seconds. \n",
      "[ SUCCESS ] Memory consumed: 181 MB. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7329e+01, -3.2855e+00, -1.1449e+00, -9.7942e-01, -1.5179e-01,\n",
       "          9.2083e+00, -3.6007e+00, -5.1113e-01, -1.1325e+00,  3.0623e+00,\n",
       "          3.0486e+00,  5.7270e+00, -1.7772e+00, -4.8688e+00,  4.2304e-01,\n",
       "         -6.5337e+00, -3.7628e+00, -2.1463e+00, -5.7218e+00, -3.9071e+00,\n",
       "          4.8865e+00,  1.8713e+00, -6.4810e+00, -1.7373e+00,  9.8768e-01,\n",
       "         -4.9941e+00,  2.6608e+00,  4.8284e+00, -2.7764e+00, -4.6965e+00,\n",
       "         -3.5127e+00,  5.5518e+00,  3.3330e+00,  2.4686e+00, -5.0390e+00,\n",
       "         -1.3475e+00, -2.0684e-01],\n",
       "        [ 1.1245e+01, -3.0037e+00, -1.7740e-04, -1.8865e+00,  1.3851e+00,\n",
       "          9.7956e+00, -2.2753e+00,  5.0934e-01, -4.8474e-01,  1.0725e+00,\n",
       "          4.4877e+00,  4.0109e+00, -1.5552e+00, -4.1258e+00, -1.4994e+00,\n",
       "         -5.8474e+00, -2.3569e+00, -7.3282e-01, -5.3210e+00, -3.4000e+00,\n",
       "          2.7175e+00,  2.7583e+00, -4.3755e+00, -1.7895e+00,  8.6411e-01,\n",
       "         -2.5912e+00,  1.1335e+00,  2.4955e+00, -1.2317e+00, -5.0416e+00,\n",
       "         -2.3891e+00,  1.6632e+00,  2.0518e+00,  3.1253e+00, -4.5545e+00,\n",
       "         -1.3066e+00,  2.2226e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "ov_q_model = trainer.quantize(model, accelerator=\"openvino\", calib_dataloader=data_loader)\n",
    "batch = torch.stack([data_set[0][0], data_set[1][0]])\n",
    "ov_q_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('testOpenvino')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "edf698c9356c4f0740ac364d5e27207f5ab66fab651ea6afd333d7f1671f1a40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
