{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we will demonstrates how to use BigDL-Nano to accelerate PyTorch or PyTorch-Lightning applications on training workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment\n",
    "Before you start with Apis delivered by bigdl-nano, you have to make sure BigDL-Nano is correctly installed for PyTorch. If not, please follow [this](../../../../../docs/readthedocs/source/doc/Nano/Overview/nano.md) to set up your environment.<br>\n",
    "\n",
    "We used pre-built cifar10 datamodule from lightning-bolts for demo. You are required to install lightnig-bolts as follows:\n",
    "```python\n",
    "pip install lightning-bolts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cifar10 DataModule\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset.\n",
    "Leveraging OpenCV and libjpeg-turbo, BigDL-Nano can accelerate computer vision data pipelines by providing a drop-in replacement of torch_vision's `datasets` and `transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from bigdl.nano.pytorch.vision import transforms\n",
    "DATA_PATH = os.environ.get('DATA_PATH', '.')\n",
    "BATCH_SIZE = 64\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        cifar10_normalization()\n",
    "    ]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        cifar10_normalization()\n",
    "    ]\n",
    ")\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir = DATA_PATH,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    train_transforms = train_transforms,\n",
    "    val_transforms = test_transforms,\n",
    "    test_transforms = test_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Custom Model\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision.models import resnet18\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from torchmetrics.functional import accuracy\n",
    "seed_everything(7)\n",
    "def create_model():\n",
    "    model = resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, learning_rate=0.05, num_processes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE // self.hparams.num_processes\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Nano Apis\n",
    "The PyTorch Trainer (`bigdl.nano.pytorch.Trainer`) is the place where we integrate most optimizations. It extends PyTorch Lightning's Trainer and has a few more parameters and methods specific to BigDL-Nano. The Trainer can be directly used to train a `LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "Global seed set to 7\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 782/782 [01:42<00:00,  7.67it/s, loss=0.145, v_num=11, val_loss=0.252, val_acc=0.917] \n",
      "51min 6s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▊| 155/157 [00:07<00:00, 20.90it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9081000089645386, 'test_loss': 0.27566173672676086}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 157/157 [00:07<00:00, 20.88it/s]\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nano.pytorch import Trainer\n",
    "model = LitResnet()\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30)\n",
    "fit_time_basic = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_basic = trainer.test(model, datamodule=cifar10_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the summary of all layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LightningModule.summarize of LitResnet(\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Extension for Pytorch (a.k.a. IPEX) link extends PyTorch with optimizations for an extra performance boost on Intel hardware. BigDL-Nano integrates IPEX through the Trainer. Users can turn on IPEX by setting use_ipex=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.712    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/782 [00:00<01:16, 10.18it/s, loss=2.36, v_num=12]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 782/782 [01:44<00:00,  7.48it/s, loss=0.163, v_num=12, val_loss=0.235, val_acc=0.922] \n",
      "52min 49s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▉| 156/157 [00:07<00:00, 21.61it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9156000018119812, 'test_loss': 0.2631952464580536}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 157/157 [00:07<00:00, 21.46it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet()\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  use_ipex=True)\n",
    "fit_time_ipex = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_ipex = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the optimization, some layers in the model were replaced, for example, the `Conv2d` is replaced by `_IPEXConv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LightningModule.summarize of LitResnet(\n",
       "  (model): ResNet(\n",
       "    (conv1): _IPEXConv2d()\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): Identity()\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): _IPEXConv2d()\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): _IPEXConv2d()\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): _IPEXConv2d()\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): _IPEXConv2d()\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): _IPEXConv2d()\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of processes on distributed training to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "2022-06-30 00:51:21,491 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2022-06-30 00:51:21,493 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - distributed_backend=ddp_subprocess\n",
      "2022-06-30 00:51:21,494 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - All DDP processes registered. Starting ddp with 4 processes\n",
      "2022-06-30 00:51:21,494 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.696    Total estimated model params size (MB)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/197 [00:00<00:00, 7319.90it/s]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▉  | 157/197 [01:03<00:16,  2.49it/s, loss=1.56, v_num=13]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  81%|████████  | 159/197 [01:03<00:15,  2.51it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  82%|████████▏ | 161/197 [01:03<00:14,  2.54it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  83%|████████▎ | 163/197 [01:04<00:13,  2.56it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  84%|████████▍ | 165/197 [01:04<00:12,  2.59it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  85%|████████▍ | 167/197 [01:04<00:11,  2.61it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  86%|████████▌ | 169/197 [01:04<00:10,  2.63it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  87%|████████▋ | 171/197 [01:04<00:09,  2.66it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  88%|████████▊ | 173/197 [01:04<00:08,  2.68it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  89%|████████▉ | 175/197 [01:05<00:08,  2.70it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  90%|████████▉ | 177/197 [01:05<00:07,  2.72it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  91%|█████████ | 179/197 [01:05<00:06,  2.75it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  92%|█████████▏| 181/197 [01:05<00:05,  2.77it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  93%|█████████▎| 183/197 [01:05<00:05,  2.79it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  94%|█████████▍| 185/197 [01:06<00:04,  2.81it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  95%|█████████▍| 187/197 [01:06<00:03,  2.84it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  96%|█████████▌| 189/197 [01:06<00:02,  2.86it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  97%|█████████▋| 191/197 [01:06<00:02,  2.88it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  98%|█████████▊| 193/197 [01:06<00:01,  2.90it/s, loss=1.56, v_num=13]\n",
      "Epoch 0:  99%|█████████▉| 195/197 [01:07<00:00,  2.92it/s, loss=1.56, v_num=13]\n",
      "Epoch 0: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=1.56, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  80%|████████  | 158/197 [01:02<00:15,  2.55it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  81%|████████  | 160/197 [01:02<00:14,  2.57it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  82%|████████▏ | 162/197 [01:02<00:13,  2.59it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  83%|████████▎ | 164/197 [01:03<00:12,  2.62it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  84%|████████▍ | 166/197 [01:03<00:11,  2.64it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  85%|████████▌ | 168/197 [01:03<00:10,  2.67it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  86%|████████▋ | 170/197 [01:03<00:10,  2.69it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  87%|████████▋ | 172/197 [01:03<00:09,  2.71it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  88%|████████▊ | 174/197 [01:03<00:08,  2.74it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  89%|████████▉ | 176/197 [01:04<00:07,  2.76it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  90%|█████████ | 178/197 [01:04<00:06,  2.78it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  91%|█████████▏| 180/197 [01:04<00:06,  2.81it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  92%|█████████▏| 182/197 [01:04<00:05,  2.83it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  93%|█████████▎| 184/197 [01:04<00:04,  2.85it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  94%|█████████▍| 186/197 [01:05<00:03,  2.87it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  95%|█████████▌| 188/197 [01:05<00:03,  2.90it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  96%|█████████▋| 190/197 [01:05<00:02,  2.92it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  97%|█████████▋| 192/197 [01:05<00:01,  2.94it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  98%|█████████▊| 194/197 [01:05<00:01,  2.96it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  99%|█████████▉| 196/197 [01:06<00:00,  2.98it/s, loss=1.39, v_num=13, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1: 100%|██████████| 197/197 [01:06<00:00,  2.99it/s, loss=1.39, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  80%|████████  | 158/197 [01:02<00:15,  2.54it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  81%|████████  | 160/197 [01:02<00:14,  2.57it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  82%|████████▏ | 162/197 [01:02<00:13,  2.59it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  83%|████████▎ | 164/197 [01:03<00:12,  2.62it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  84%|████████▍ | 166/197 [01:03<00:11,  2.64it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  85%|████████▌ | 168/197 [01:03<00:10,  2.66it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  86%|████████▋ | 170/197 [01:03<00:10,  2.69it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.50it/s]\u001b[A\n",
      "Epoch 2:  87%|████████▋ | 172/197 [01:03<00:09,  2.71it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  88%|████████▊ | 174/197 [01:04<00:08,  2.73it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  89%|████████▉ | 176/197 [01:04<00:07,  2.75it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  90%|█████████ | 178/197 [01:04<00:06,  2.78it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  91%|█████████▏| 180/197 [01:04<00:06,  2.80it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  92%|█████████▏| 182/197 [01:04<00:05,  2.82it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  93%|█████████▎| 184/197 [01:05<00:04,  2.84it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  94%|█████████▍| 186/197 [01:05<00:03,  2.87it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  95%|█████████▌| 188/197 [01:05<00:03,  2.89it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  96%|█████████▋| 190/197 [01:05<00:02,  2.91it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  97%|█████████▋| 192/197 [01:05<00:01,  2.93it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  98%|█████████▊| 194/197 [01:06<00:01,  2.95it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2:  99%|█████████▉| 196/197 [01:06<00:00,  2.98it/s, loss=1.04, v_num=13, val_loss=1.470, val_acc=0.491]\n",
      "Epoch 2: 100%|██████████| 197/197 [01:06<00:00,  2.99it/s, loss=1.04, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  80%|████████  | 158/197 [01:02<00:15,  2.55it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  81%|████████  | 160/197 [01:02<00:14,  2.57it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  82%|████████▏ | 162/197 [01:02<00:13,  2.59it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  83%|████████▎ | 164/197 [01:03<00:12,  2.62it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  84%|████████▍ | 166/197 [01:03<00:11,  2.64it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  85%|████████▌ | 168/197 [01:03<00:10,  2.66it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  86%|████████▋ | 170/197 [01:03<00:10,  2.69it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  87%|████████▋ | 172/197 [01:03<00:09,  2.71it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  88%|████████▊ | 174/197 [01:04<00:08,  2.73it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  89%|████████▉ | 176/197 [01:04<00:07,  2.75it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.80it/s]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 178/197 [01:04<00:06,  2.78it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  91%|█████████▏| 180/197 [01:04<00:06,  2.80it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  92%|█████████▏| 182/197 [01:04<00:05,  2.82it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  93%|█████████▎| 184/197 [01:05<00:04,  2.84it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  94%|█████████▍| 186/197 [01:05<00:03,  2.87it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  95%|█████████▌| 188/197 [01:05<00:03,  2.89it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  96%|█████████▋| 190/197 [01:05<00:02,  2.91it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  97%|█████████▋| 192/197 [01:05<00:01,  2.93it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3:  98%|█████████▊| 194/197 [01:06<00:01,  2.95it/s, loss=0.905, v_num=13, val_loss=1.120, val_acc=0.618]\n",
      "Epoch 3: 100%|██████████| 197/197 [01:06<00:00,  2.99it/s, loss=0.905, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  80%|████████  | 158/197 [01:02<00:15,  2.54it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  81%|████████  | 160/197 [01:02<00:14,  2.57it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  82%|████████▏ | 162/197 [01:02<00:13,  2.59it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  83%|████████▎ | 164/197 [01:03<00:12,  2.61it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.06it/s]\u001b[A\n",
      "Epoch 4:  84%|████████▍ | 166/197 [01:03<00:11,  2.64it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  85%|████████▌ | 168/197 [01:03<00:10,  2.66it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  86%|████████▋ | 170/197 [01:03<00:10,  2.68it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  87%|████████▋ | 172/197 [01:03<00:09,  2.71it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  88%|████████▊ | 174/197 [01:04<00:08,  2.73it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  89%|████████▉ | 176/197 [01:04<00:07,  2.75it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  90%|█████████ | 178/197 [01:04<00:06,  2.78it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  91%|█████████▏| 180/197 [01:04<00:06,  2.80it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  92%|█████████▏| 182/197 [01:04<00:05,  2.82it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  93%|█████████▎| 184/197 [01:05<00:04,  2.84it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  94%|█████████▍| 186/197 [01:05<00:03,  2.87it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  95%|█████████▌| 188/197 [01:05<00:03,  2.89it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  96%|█████████▋| 190/197 [01:05<00:02,  2.91it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  97%|█████████▋| 192/197 [01:05<00:01,  2.93it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  98%|█████████▊| 194/197 [01:06<00:01,  2.95it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4:  99%|█████████▉| 196/197 [01:06<00:00,  2.97it/s, loss=0.762, v_num=13, val_loss=0.869, val_acc=0.704]\n",
      "Epoch 4: 100%|██████████| 197/197 [01:06<00:00,  2.98it/s, loss=0.762, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  80%|████████  | 158/197 [01:02<00:15,  2.53it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  81%|████████  | 160/197 [01:03<00:14,  2.55it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  82%|████████▏ | 162/197 [01:03<00:13,  2.58it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  83%|████████▎ | 164/197 [01:03<00:12,  2.60it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  84%|████████▍ | 166/197 [01:03<00:11,  2.63it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  85%|████████▌ | 168/197 [01:03<00:10,  2.65it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  86%|████████▋ | 170/197 [01:03<00:10,  2.67it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  87%|████████▋ | 172/197 [01:04<00:09,  2.70it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  88%|████████▊ | 174/197 [01:04<00:08,  2.72it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  89%|████████▉ | 176/197 [01:04<00:07,  2.74it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  90%|█████████ | 178/197 [01:04<00:06,  2.77it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  91%|█████████▏| 180/197 [01:04<00:06,  2.79it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  92%|█████████▏| 182/197 [01:05<00:05,  2.81it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  93%|█████████▎| 184/197 [01:05<00:04,  2.83it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  94%|█████████▍| 186/197 [01:05<00:03,  2.86it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  95%|█████████▌| 188/197 [01:05<00:03,  2.88it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  96%|█████████▋| 190/197 [01:05<00:02,  2.90it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  97%|█████████▋| 192/197 [01:06<00:01,  2.92it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  98%|█████████▊| 194/197 [01:06<00:01,  2.94it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5:  99%|█████████▉| 196/197 [01:06<00:00,  2.97it/s, loss=0.718, v_num=13, val_loss=1.050, val_acc=0.673]\n",
      "Epoch 5: 100%|██████████| 197/197 [01:06<00:00,  2.97it/s, loss=0.718, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  80%|████████  | 158/197 [01:02<00:15,  2.54it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  81%|████████  | 160/197 [01:02<00:14,  2.57it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  82%|████████▏ | 162/197 [01:02<00:13,  2.59it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  83%|████████▎ | 164/197 [01:03<00:12,  2.61it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  84%|████████▍ | 166/197 [01:03<00:11,  2.64it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  85%|████████▌ | 168/197 [01:03<00:10,  2.66it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  86%|████████▋ | 170/197 [01:03<00:10,  2.68it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  87%|████████▋ | 172/197 [01:03<00:09,  2.71it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  88%|████████▊ | 174/197 [01:04<00:08,  2.73it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  89%|████████▉ | 176/197 [01:04<00:07,  2.75it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  90%|█████████ | 178/197 [01:04<00:06,  2.78it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  91%|█████████▏| 180/197 [01:04<00:06,  2.80it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  92%|█████████▏| 182/197 [01:04<00:05,  2.82it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  93%|█████████▎| 184/197 [01:05<00:04,  2.84it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  94%|█████████▍| 186/197 [01:05<00:03,  2.87it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  95%|█████████▌| 188/197 [01:05<00:03,  2.89it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  96%|█████████▋| 190/197 [01:05<00:02,  2.91it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6:  97%|█████████▋| 192/197 [01:05<00:01,  2.93it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  9.30it/s]\u001b[A\n",
      "Epoch 6:  98%|█████████▊| 194/197 [01:06<00:01,  2.95it/s, loss=0.546, v_num=13, val_loss=1.100, val_acc=0.656]\n",
      "Epoch 6: 100%|██████████| 197/197 [01:06<00:00,  2.98it/s, loss=0.546, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  80%|████████  | 158/197 [01:02<00:15,  2.53it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  81%|████████  | 160/197 [01:03<00:14,  2.55it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  83%|████████▎ | 164/197 [01:03<00:12,  2.60it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  84%|████████▍ | 166/197 [01:03<00:11,  2.62it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  85%|████████▌ | 168/197 [01:03<00:10,  2.64it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  86%|████████▋ | 170/197 [01:04<00:10,  2.67it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  87%|████████▋ | 172/197 [01:04<00:09,  2.69it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  89%|████████▉ | 176/197 [01:04<00:07,  2.74it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  90%|█████████ | 178/197 [01:04<00:06,  2.76it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  91%|█████████▏| 180/197 [01:05<00:06,  2.78it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  92%|█████████▏| 182/197 [01:05<00:05,  2.81it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  93%|█████████▎| 184/197 [01:05<00:04,  2.83it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  94%|█████████▍| 186/197 [01:05<00:03,  2.85it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  95%|█████████▌| 188/197 [01:05<00:03,  2.87it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  96%|█████████▋| 190/197 [01:05<00:02,  2.90it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  97%|█████████▋| 192/197 [01:06<00:01,  2.92it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  98%|█████████▊| 194/197 [01:06<00:01,  2.94it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7:  99%|█████████▉| 196/197 [01:06<00:00,  2.96it/s, loss=0.612, v_num=13, val_loss=1.000, val_acc=0.695]\n",
      "Epoch 7: 100%|██████████| 197/197 [01:06<00:00,  2.96it/s, loss=0.612, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.472, v_num=13, val_loss=0.853, val_acc=0.742]\n",
      "Epoch 8: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.472, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  80%|████████  | 158/197 [01:03<00:15,  2.52it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:05,  7.32it/s]\u001b[A\n",
      "Epoch 9:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  84%|████████▍ | 166/197 [01:03<00:11,  2.61it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  85%|████████▌ | 168/197 [01:04<00:10,  2.64it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  89%|████████▉ | 176/197 [01:04<00:07,  2.73it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.56it/s]\u001b[A\n",
      "Epoch 9:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9:  98%|█████████▊| 194/197 [01:06<00:01,  2.93it/s, loss=0.484, v_num=13, val_loss=0.812, val_acc=0.744]\n",
      "Epoch 9: 100%|██████████| 197/197 [01:06<00:00,  2.96it/s, loss=0.484, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  80%|████████  | 158/197 [01:02<00:15,  2.53it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.69it/s]\u001b[A\n",
      "Epoch 10:  81%|████████  | 160/197 [01:02<00:14,  2.56it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  8.30it/s]\u001b[A\n",
      "Epoch 10:  82%|████████▏ | 162/197 [01:03<00:13,  2.58it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  83%|████████▎ | 164/197 [01:03<00:12,  2.60it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  84%|████████▍ | 166/197 [01:03<00:11,  2.63it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  85%|████████▌ | 168/197 [01:03<00:10,  2.65it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  86%|████████▋ | 170/197 [01:03<00:10,  2.67it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  87%|████████▋ | 172/197 [01:04<00:09,  2.70it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  88%|████████▊ | 174/197 [01:04<00:08,  2.72it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  89%|████████▉ | 176/197 [01:04<00:07,  2.74it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  90%|█████████ | 178/197 [01:04<00:06,  2.76it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  91%|█████████▏| 180/197 [01:05<00:06,  2.78it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  92%|█████████▏| 182/197 [01:05<00:05,  2.81it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01, 10.02it/s]\u001b[A\n",
      "Epoch 10:  93%|█████████▎| 184/197 [01:05<00:04,  2.83it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  94%|█████████▍| 186/197 [01:05<00:03,  2.85it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  95%|█████████▌| 188/197 [01:05<00:03,  2.87it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  96%|█████████▋| 190/197 [01:06<00:02,  2.89it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  97%|█████████▋| 192/197 [01:06<00:01,  2.91it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  98%|█████████▊| 194/197 [01:06<00:01,  2.93it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10:  99%|█████████▉| 196/197 [01:06<00:00,  2.96it/s, loss=0.388, v_num=13, val_loss=1.100, val_acc=0.681]\n",
      "Epoch 10: 100%|██████████| 197/197 [01:06<00:00,  2.97it/s, loss=0.388, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  80%|████████  | 158/197 [01:03<00:15,  2.52it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  81%|████████  | 160/197 [01:03<00:14,  2.55it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  84%|████████▍ | 166/197 [01:03<00:11,  2.62it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating:  22%|██▎       | 9/40 [00:00<00:03,  9.25it/s]\u001b[A\n",
      "Epoch 11:  85%|████████▌ | 168/197 [01:04<00:10,  2.64it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  8.61it/s]\u001b[A\n",
      "Epoch 11:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.18it/s]\u001b[A\n",
      "Epoch 11:  89%|████████▉ | 176/197 [01:04<00:07,  2.72it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.70it/s]\u001b[A\n",
      "Epoch 11:  94%|█████████▍| 186/197 [01:05<00:03,  2.83it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.01it/s]\u001b[A\n",
      "Epoch 11:  96%|█████████▋| 190/197 [01:06<00:02,  2.87it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.385, v_num=13, val_loss=0.602, val_acc=0.802]\n",
      "Epoch 11: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.385, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.389, v_num=13, val_loss=0.707, val_acc=0.780]\n",
      "Epoch 12: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.389, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  84%|████████▍ | 166/197 [01:04<00:11,  2.61it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  9.36it/s]\u001b[A\n",
      "Epoch 13:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.37it/s]\u001b[A\n",
      "Epoch 13:  96%|█████████▋| 190/197 [01:06<00:02,  2.87it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  8.54it/s]\u001b[A\n",
      "Epoch 13:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13:  98%|█████████▊| 194/197 [01:06<00:01,  2.91it/s, loss=0.356, v_num=13, val_loss=0.641, val_acc=0.805]\n",
      "Epoch 13: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.356, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  80%|████████  | 158/197 [01:02<00:15,  2.53it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  81%|████████  | 160/197 [01:03<00:14,  2.55it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  83%|████████▎ | 164/197 [01:03<00:12,  2.60it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  84%|████████▍ | 166/197 [01:03<00:11,  2.62it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  85%|████████▌ | 168/197 [01:03<00:10,  2.64it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  86%|████████▋ | 170/197 [01:04<00:10,  2.67it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  87%|████████▋ | 172/197 [01:04<00:09,  2.69it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  89%|████████▉ | 176/197 [01:04<00:07,  2.74it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  90%|█████████ | 178/197 [01:04<00:06,  2.76it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  91%|█████████▏| 180/197 [01:05<00:06,  2.78it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  92%|█████████▏| 182/197 [01:05<00:05,  2.80it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  93%|█████████▎| 184/197 [01:05<00:04,  2.83it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  94%|█████████▍| 186/197 [01:05<00:03,  2.85it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  95%|█████████▌| 188/197 [01:05<00:03,  2.87it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  96%|█████████▋| 190/197 [01:06<00:02,  2.89it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  97%|█████████▋| 192/197 [01:06<00:01,  2.91it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  98%|█████████▊| 194/197 [01:06<00:01,  2.94it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14:  99%|█████████▉| 196/197 [01:06<00:00,  2.96it/s, loss=0.337, v_num=13, val_loss=0.889, val_acc=0.753]\n",
      "Epoch 14: 100%|██████████| 197/197 [01:06<00:00,  2.96it/s, loss=0.337, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  80%|████████  | 158/197 [01:02<00:15,  2.53it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  7.96it/s]\u001b[A\n",
      "Epoch 15:  81%|████████  | 160/197 [01:03<00:14,  2.55it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.93it/s]\u001b[A\n",
      "Epoch 15:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  83%|████████▎ | 164/197 [01:03<00:12,  2.60it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  84%|████████▍ | 166/197 [01:03<00:11,  2.62it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  85%|████████▌ | 168/197 [01:03<00:10,  2.65it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  86%|████████▋ | 170/197 [01:04<00:10,  2.67it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  87%|████████▋ | 172/197 [01:04<00:09,  2.69it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  88%|████████▊ | 174/197 [01:04<00:08,  2.72it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  89%|████████▉ | 176/197 [01:04<00:07,  2.74it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  90%|█████████ | 178/197 [01:04<00:06,  2.76it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  91%|█████████▏| 180/197 [01:05<00:06,  2.78it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  92%|█████████▏| 182/197 [01:05<00:05,  2.81it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  93%|█████████▎| 184/197 [01:05<00:04,  2.83it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.96it/s]\u001b[A\n",
      "Epoch 15:  94%|█████████▍| 186/197 [01:05<00:03,  2.85it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  95%|█████████▌| 188/197 [01:05<00:03,  2.87it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  96%|█████████▋| 190/197 [01:06<00:02,  2.89it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  97%|█████████▋| 192/197 [01:06<00:01,  2.91it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  98%|█████████▊| 194/197 [01:06<00:01,  2.93it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15:  99%|█████████▉| 196/197 [01:06<00:00,  2.96it/s, loss=0.326, v_num=13, val_loss=0.521, val_acc=0.830]\n",
      "Epoch 15: 100%|██████████| 197/197 [01:06<00:00,  2.97it/s, loss=0.326, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 16:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  84%|████████▍ | 166/197 [01:04<00:11,  2.61it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  89%|████████▉ | 176/197 [01:04<00:07,  2.72it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  93%|█████████▎| 184/197 [01:05<00:04,  2.82it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.26it/s]\u001b[A\n",
      "Epoch 16:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.288, v_num=13, val_loss=0.485, val_acc=0.839]\n",
      "Epoch 16: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.288, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:03,  9.89it/s]\u001b[A\n",
      "Epoch 17:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  84%|████████▍ | 166/197 [01:04<00:11,  2.61it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  9.49it/s]\u001b[A\n",
      "Epoch 17:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  88%|████████▊ | 174/197 [01:04<00:08,  2.69it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.95it/s]\u001b[A\n",
      "Epoch 17:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  92%|█████████▏| 182/197 [01:05<00:05,  2.78it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  93%|█████████▎| 184/197 [01:05<00:04,  2.80it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  96%|█████████▋| 190/197 [01:06<00:02,  2.87it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  98%|█████████▊| 194/197 [01:06<00:01,  2.91it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17:  99%|█████████▉| 196/197 [01:07<00:00,  2.94it/s, loss=0.286, v_num=13, val_loss=0.396, val_acc=0.868]\n",
      "Epoch 17: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.286, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  8.97it/s]\u001b[A\n",
      "Epoch 18:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  8.26it/s]\u001b[A\n",
      "Epoch 18:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  8.98it/s]\u001b[A\n",
      "Epoch 18:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.25, v_num=13, val_loss=0.472, val_acc=0.847]\n",
      "Epoch 18: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.25, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  82%|████████▏ | 162/197 [01:03<00:13,  2.56it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  83%|████████▎ | 164/197 [01:03<00:12,  2.58it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  84%|████████▍ | 166/197 [01:04<00:11,  2.61it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  85%|████████▌ | 168/197 [01:04<00:11,  2.63it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  89%|████████▉ | 176/197 [01:05<00:07,  2.72it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19:  99%|█████████▉| 196/197 [01:06<00:00,  2.95it/s, loss=0.233, v_num=13, val_loss=0.359, val_acc=0.878]\n",
      "Epoch 19: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.233, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  80%|████████  | 158/197 [01:03<00:15,  2.52it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 20:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  84%|████████▍ | 166/197 [01:03<00:11,  2.61it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  85%|████████▌ | 168/197 [01:04<00:10,  2.64it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.62it/s]\u001b[A\n",
      "Epoch 20:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  88%|████████▊ | 174/197 [01:04<00:08,  2.70it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  89%|████████▉ | 176/197 [01:04<00:07,  2.73it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  93%|█████████▎| 184/197 [01:05<00:04,  2.82it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.02it/s]\u001b[A\n",
      "Epoch 20:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.27, v_num=13, val_loss=0.431, val_acc=0.861]\n",
      "Epoch 20: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.27, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  80%|████████  | 158/197 [01:03<00:15,  2.50it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03,  9.40it/s]\u001b[A\n",
      "Epoch 21:  82%|████████▏ | 162/197 [01:03<00:13,  2.55it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  83%|████████▎ | 164/197 [01:04<00:12,  2.57it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  85%|████████▌ | 168/197 [01:04<00:11,  2.62it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  86%|████████▋ | 170/197 [01:04<00:10,  2.64it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  88%|████████▊ | 174/197 [01:05<00:08,  2.69it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  89%|████████▉ | 176/197 [01:05<00:07,  2.71it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  90%|█████████ | 178/197 [01:05<00:06,  2.73it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  92%|█████████▏| 182/197 [01:05<00:05,  2.78it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  93%|█████████▎| 184/197 [01:06<00:04,  2.80it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  94%|█████████▍| 186/197 [01:06<00:03,  2.82it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  96%|█████████▋| 190/197 [01:06<00:02,  2.87it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  98%|█████████▊| 194/197 [01:06<00:01,  2.91it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21:  99%|█████████▉| 196/197 [01:07<00:00,  2.93it/s, loss=0.24, v_num=13, val_loss=0.443, val_acc=0.856]\n",
      "Epoch 21: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.24, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  80%|████████  | 158/197 [01:03<00:15,  2.52it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  84%|████████▍ | 166/197 [01:03<00:11,  2.61it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  85%|████████▌ | 168/197 [01:04<00:10,  2.64it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  89%|████████▉ | 176/197 [01:04<00:07,  2.73it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.37it/s]\u001b[A\n",
      "Epoch 22:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  92%|█████████▏| 182/197 [01:05<00:05,  2.79it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  93%|█████████▎| 184/197 [01:05<00:04,  2.81it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.211, v_num=13, val_loss=0.428, val_acc=0.865]\n",
      "Epoch 22: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.211, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  80%|████████  | 158/197 [01:03<00:15,  2.49it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  81%|████████  | 160/197 [01:03<00:14,  2.52it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  82%|████████▏ | 162/197 [01:04<00:13,  2.54it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  83%|████████▎ | 164/197 [01:04<00:12,  2.56it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  84%|████████▍ | 166/197 [01:04<00:11,  2.59it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  85%|████████▌ | 168/197 [01:04<00:11,  2.61it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  86%|████████▋ | 170/197 [01:04<00:10,  2.63it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  87%|████████▋ | 172/197 [01:05<00:09,  2.66it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  88%|████████▊ | 174/197 [01:05<00:08,  2.68it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.16it/s]\u001b[A\n",
      "Epoch 23:  89%|████████▉ | 176/197 [01:05<00:07,  2.70it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  90%|█████████ | 178/197 [01:05<00:06,  2.72it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:01,  9.62it/s]\u001b[A\n",
      "Epoch 23:  91%|█████████▏| 180/197 [01:05<00:06,  2.74it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  92%|█████████▏| 182/197 [01:06<00:05,  2.76it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  93%|█████████▎| 184/197 [01:06<00:04,  2.79it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  94%|█████████▍| 186/197 [01:06<00:03,  2.81it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Validating:  72%|███████▎  | 29/40 [00:02<00:01, 10.01it/s]\u001b[A\n",
      "Epoch 23:  95%|█████████▌| 188/197 [01:06<00:03,  2.83it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  8.89it/s]\u001b[A\n",
      "Epoch 23:  96%|█████████▋| 190/197 [01:07<00:02,  2.85it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  97%|█████████▋| 192/197 [01:07<00:01,  2.87it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23:  98%|█████████▊| 194/197 [01:07<00:01,  2.89it/s, loss=0.151, v_num=13, val_loss=0.378, val_acc=0.877]\n",
      "Epoch 23: 100%|██████████| 197/197 [01:07<00:00,  2.92it/s, loss=0.151, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  80%|████████  | 158/197 [01:03<00:15,  2.52it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 24:  81%|████████  | 160/197 [01:03<00:14,  2.54it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  82%|████████▏ | 162/197 [01:03<00:13,  2.57it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  83%|████████▎ | 164/197 [01:03<00:12,  2.59it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  84%|████████▍ | 166/197 [01:03<00:11,  2.61it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  85%|████████▌ | 168/197 [01:04<00:10,  2.64it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  86%|████████▋ | 170/197 [01:04<00:10,  2.66it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  87%|████████▋ | 172/197 [01:04<00:09,  2.68it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  88%|████████▊ | 174/197 [01:04<00:08,  2.71it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  89%|████████▉ | 176/197 [01:04<00:07,  2.73it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.86it/s]\u001b[A\n",
      "Epoch 24:  90%|█████████ | 178/197 [01:05<00:06,  2.75it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:01,  9.82it/s]\u001b[A\n",
      "Epoch 24:  91%|█████████▏| 180/197 [01:05<00:06,  2.77it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  92%|█████████▏| 182/197 [01:05<00:05,  2.80it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.95it/s]\u001b[A\n",
      "Epoch 24:  93%|█████████▎| 184/197 [01:05<00:04,  2.82it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.55it/s]\u001b[A\n",
      "Epoch 24:  94%|█████████▍| 186/197 [01:05<00:03,  2.84it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  95%|█████████▌| 188/197 [01:06<00:03,  2.86it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  96%|█████████▋| 190/197 [01:06<00:02,  2.88it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  97%|█████████▋| 192/197 [01:06<00:01,  2.90it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  98%|█████████▊| 194/197 [01:06<00:01,  2.92it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24:  99%|█████████▉| 196/197 [01:06<00:00,  2.94it/s, loss=0.168, v_num=13, val_loss=0.349, val_acc=0.891]\n",
      "Epoch 24: 100%|██████████| 197/197 [01:07<00:00,  2.95it/s, loss=0.168, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  80%|████████  | 158/197 [01:03<00:15,  2.50it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  81%|████████  | 160/197 [01:03<00:14,  2.52it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  82%|████████▏ | 162/197 [01:04<00:13,  2.54it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  83%|████████▎ | 164/197 [01:04<00:12,  2.57it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  84%|████████▍ | 166/197 [01:04<00:11,  2.59it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  85%|████████▌ | 168/197 [01:04<00:11,  2.61it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  86%|████████▋ | 170/197 [01:04<00:10,  2.64it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  87%|████████▋ | 172/197 [01:05<00:09,  2.66it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  88%|████████▊ | 174/197 [01:05<00:08,  2.68it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  89%|████████▉ | 176/197 [01:05<00:07,  2.71it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  90%|█████████ | 178/197 [01:05<00:06,  2.73it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  91%|█████████▏| 180/197 [01:05<00:06,  2.75it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  92%|█████████▏| 182/197 [01:05<00:05,  2.78it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  93%|█████████▎| 184/197 [01:06<00:04,  2.80it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  94%|█████████▍| 186/197 [01:06<00:03,  2.82it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  95%|█████████▌| 188/197 [01:06<00:03,  2.84it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  96%|█████████▋| 190/197 [01:06<00:02,  2.86it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  98%|█████████▊| 194/197 [01:07<00:01,  2.91it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25:  99%|█████████▉| 196/197 [01:07<00:00,  2.93it/s, loss=0.115, v_num=13, val_loss=0.321, val_acc=0.899]\n",
      "Epoch 25: 100%|██████████| 197/197 [01:07<00:00,  2.92it/s, loss=0.115, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 26:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  82%|████████▏ | 162/197 [01:03<00:13,  2.55it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  83%|████████▎ | 164/197 [01:04<00:12,  2.58it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  85%|████████▌ | 168/197 [01:04<00:11,  2.62it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  86%|████████▋ | 170/197 [01:04<00:10,  2.65it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  88%|████████▊ | 174/197 [01:05<00:08,  2.69it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  89%|████████▉ | 176/197 [01:05<00:07,  2.71it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  90%|█████████ | 178/197 [01:05<00:06,  2.74it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  92%|█████████▏| 182/197 [01:05<00:05,  2.78it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  93%|█████████▎| 184/197 [01:05<00:04,  2.80it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  94%|█████████▍| 186/197 [01:06<00:03,  2.83it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  95%|█████████▌| 188/197 [01:06<00:03,  2.85it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  96%|█████████▋| 190/197 [01:06<00:02,  2.87it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.00it/s]\u001b[A\n",
      "Epoch 26:  97%|█████████▋| 192/197 [01:06<00:01,  2.89it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  98%|█████████▊| 194/197 [01:07<00:01,  2.91it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26:  99%|█████████▉| 196/197 [01:07<00:00,  2.93it/s, loss=0.124, v_num=13, val_loss=0.291, val_acc=0.907]\n",
      "Epoch 26: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.124, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  80%|████████  | 158/197 [01:04<00:15,  2.47it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 27:  81%|████████  | 160/197 [01:04<00:14,  2.49it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  82%|████████▏ | 162/197 [01:04<00:13,  2.52it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  83%|████████▎ | 164/197 [01:04<00:12,  2.54it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.82it/s]\u001b[A\n",
      "Epoch 27:  84%|████████▍ | 166/197 [01:05<00:12,  2.56it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  85%|████████▌ | 168/197 [01:05<00:11,  2.58it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  86%|████████▋ | 170/197 [01:05<00:10,  2.61it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  87%|████████▋ | 172/197 [01:05<00:09,  2.63it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  88%|████████▊ | 174/197 [01:05<00:08,  2.65it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  89%|████████▉ | 176/197 [01:06<00:07,  2.68it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  90%|█████████ | 178/197 [01:06<00:07,  2.70it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  91%|█████████▏| 180/197 [01:06<00:06,  2.72it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  92%|█████████▏| 182/197 [01:06<00:05,  2.74it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  93%|█████████▎| 184/197 [01:06<00:04,  2.77it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  94%|█████████▍| 186/197 [01:07<00:03,  2.79it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  95%|█████████▌| 188/197 [01:07<00:03,  2.81it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  96%|█████████▋| 190/197 [01:07<00:02,  2.83it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  97%|█████████▋| 192/197 [01:07<00:01,  2.85it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  98%|█████████▊| 194/197 [01:07<00:01,  2.88it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27:  99%|█████████▉| 196/197 [01:08<00:00,  2.90it/s, loss=0.0966, v_num=13, val_loss=0.289, val_acc=0.911]\n",
      "Epoch 27: 100%|██████████| 197/197 [01:08<00:00,  2.90it/s, loss=0.0966, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  80%|████████  | 158/197 [01:03<00:15,  2.50it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.36it/s]\u001b[A\n",
      "Epoch 28:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.96it/s]\u001b[A\n",
      "Epoch 28:  82%|████████▏ | 162/197 [01:03<00:13,  2.55it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  8.55it/s]\u001b[A\n",
      "Epoch 28:  83%|████████▎ | 164/197 [01:04<00:12,  2.57it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  84%|████████▍ | 166/197 [01:04<00:11,  2.59it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  9.13it/s]\u001b[A\n",
      "Epoch 28:  85%|████████▌ | 168/197 [01:04<00:11,  2.61it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  8.64it/s]\u001b[A\n",
      "Epoch 28:  86%|████████▋ | 170/197 [01:04<00:10,  2.64it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  87%|████████▋ | 172/197 [01:05<00:09,  2.66it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  88%|████████▊ | 174/197 [01:05<00:08,  2.68it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  89%|████████▉ | 176/197 [01:05<00:07,  2.71it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  90%|█████████ | 178/197 [01:05<00:06,  2.73it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  91%|█████████▏| 180/197 [01:05<00:06,  2.75it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  92%|█████████▏| 182/197 [01:05<00:05,  2.77it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  93%|█████████▎| 184/197 [01:06<00:04,  2.79it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.26it/s]\u001b[A\n",
      "Epoch 28:  94%|█████████▍| 186/197 [01:06<00:03,  2.81it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  95%|█████████▌| 188/197 [01:06<00:03,  2.83it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  96%|█████████▋| 190/197 [01:06<00:02,  2.86it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.80it/s]\u001b[A\n",
      "Epoch 28:  97%|█████████▋| 192/197 [01:07<00:01,  2.88it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28:  98%|█████████▊| 194/197 [01:07<00:01,  2.90it/s, loss=0.0732, v_num=13, val_loss=0.271, val_acc=0.918]\n",
      "Epoch 28: 100%|██████████| 197/197 [01:07<00:00,  2.93it/s, loss=0.0732, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  80%|████████  | 158/197 [01:03<00:15,  2.51it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29:  81%|████████  | 160/197 [01:03<00:14,  2.53it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03, 10.14it/s]\u001b[A\n",
      "Epoch 29:  82%|████████▏ | 162/197 [01:03<00:13,  2.55it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  83%|████████▎ | 164/197 [01:04<00:12,  2.58it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03, 10.07it/s]\u001b[A\n",
      "Epoch 29:  84%|████████▍ | 166/197 [01:04<00:11,  2.60it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  22%|██▎       | 9/40 [00:00<00:03,  9.72it/s]\u001b[A\n",
      "Epoch 29:  85%|████████▌ | 168/197 [01:04<00:11,  2.62it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  8.86it/s]\u001b[A\n",
      "Epoch 29:  86%|████████▋ | 170/197 [01:04<00:10,  2.64it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  87%|████████▋ | 172/197 [01:04<00:09,  2.67it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  88%|████████▊ | 174/197 [01:05<00:08,  2.69it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  89%|████████▉ | 176/197 [01:05<00:07,  2.71it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  90%|█████████ | 178/197 [01:05<00:06,  2.73it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  91%|█████████▏| 180/197 [01:05<00:06,  2.76it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  92%|█████████▏| 182/197 [01:05<00:05,  2.78it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  93%|█████████▎| 184/197 [01:06<00:04,  2.80it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.96it/s]\u001b[A\n",
      "Epoch 29:  94%|█████████▍| 186/197 [01:06<00:03,  2.82it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  9.24it/s]\u001b[A\n",
      "Epoch 29:  95%|█████████▌| 188/197 [01:06<00:03,  2.84it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.17it/s]\u001b[A\n",
      "Epoch 29:  96%|█████████▋| 190/197 [01:06<00:02,  2.86it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  97%|█████████▋| 192/197 [01:06<00:01,  2.88it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29:  98%|█████████▊| 194/197 [01:07<00:01,  2.90it/s, loss=0.0782, v_num=13, val_loss=0.252, val_acc=0.923]\n",
      "Epoch 29: 100%|██████████| 197/197 [01:07<00:00,  2.94it/s, loss=0.0782, v_num=13, val_loss=0.238, val_acc=0.931]\n",
      "Epoch 29: 100%|██████████| 197/197 [01:07<00:00,  2.93it/s, loss=0.0782, v_num=13, val_loss=0.238, val_acc=0.931]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "2022-06-30 01:25:04,274 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2022-06-30 01:25:04,275 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - distributed_backend=ddp_subprocess\n",
      "2022-06-30 01:25:04,276 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - All DDP processes registered. Starting ddp with 4 processes\n",
      "2022-06-30 01:25:04,277 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33min 42s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  98%|█████████▊| 39/40 [00:03<00:00, 10.34it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9239000082015991, 'test_loss': 0.26745525002479553}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 40/40 [00:03<00:00, 10.41it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.1, num_processes=4)\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  num_processes=4\n",
    "                  )\n",
    "fit_time_dit = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_dit = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable both distributed training and ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "2022-06-30 01:25:13,097 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2022-06-30 01:25:13,098 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - distributed_backend=ddp_subprocess\n",
      "2022-06-30 01:25:13,099 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - All DDP processes registered. Starting ddp with 4 processes\n",
      "2022-06-30 01:25:13,100 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.712    Total estimated model params size (MB)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/197 [00:00<00:00, 2504.06it/s]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1289] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▉  | 157/197 [01:06<00:16,  2.38it/s, loss=1.58, v_num=14]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  81%|████████  | 159/197 [01:06<00:15,  2.40it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  82%|████████▏ | 161/197 [01:06<00:14,  2.43it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  83%|████████▎ | 163/197 [01:06<00:13,  2.45it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  84%|████████▍ | 165/197 [01:07<00:12,  2.47it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  85%|████████▍ | 167/197 [01:07<00:12,  2.50it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  86%|████████▌ | 169/197 [01:07<00:11,  2.52it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  87%|████████▋ | 171/197 [01:07<00:10,  2.54it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  88%|████████▊ | 173/197 [01:07<00:09,  2.56it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  89%|████████▉ | 175/197 [01:08<00:08,  2.58it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  90%|████████▉ | 177/197 [01:08<00:07,  2.61it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  91%|█████████ | 179/197 [01:08<00:06,  2.63it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  92%|█████████▏| 181/197 [01:08<00:06,  2.65it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  93%|█████████▎| 183/197 [01:08<00:05,  2.67it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  94%|█████████▍| 185/197 [01:09<00:04,  2.69it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  95%|█████████▍| 187/197 [01:09<00:03,  2.72it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  96%|█████████▌| 189/197 [01:09<00:02,  2.74it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  97%|█████████▋| 191/197 [01:09<00:02,  2.76it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  98%|█████████▊| 193/197 [01:09<00:01,  2.78it/s, loss=1.58, v_num=14]\n",
      "Epoch 0:  99%|█████████▉| 195/197 [01:10<00:00,  2.80it/s, loss=1.58, v_num=14]\n",
      "Epoch 0: 100%|██████████| 197/197 [01:10<00:00,  2.81it/s, loss=1.58, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  80%|████████  | 158/197 [01:05<00:15,  2.44it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  81%|████████  | 160/197 [01:05<00:15,  2.46it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.14it/s]\u001b[A\n",
      "Epoch 1:  84%|████████▍ | 166/197 [01:06<00:12,  2.53it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.59it/s]\u001b[A\n",
      "Epoch 1:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  89%|████████▉ | 176/197 [01:07<00:07,  2.64it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  91%|█████████▏| 180/197 [01:07<00:06,  2.68it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  94%|█████████▍| 186/197 [01:08<00:04,  2.75it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.83it/s]\u001b[A\n",
      "Epoch 1:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=1.35, v_num=14, val_loss=1.620, val_acc=0.431]\n",
      "Epoch 1: 100%|██████████| 197/197 [01:09<00:00,  2.86it/s, loss=1.35, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  80%|████████  | 158/197 [01:04<00:15,  2.46it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  81%|████████  | 160/197 [01:04<00:14,  2.49it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  82%|████████▏ | 162/197 [01:04<00:13,  2.51it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  83%|████████▎ | 164/197 [01:05<00:13,  2.53it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  84%|████████▍ | 166/197 [01:05<00:12,  2.55it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  85%|████████▌ | 168/197 [01:05<00:11,  2.58it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  86%|████████▋ | 170/197 [01:05<00:10,  2.60it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  87%|████████▋ | 172/197 [01:05<00:09,  2.62it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  88%|████████▊ | 174/197 [01:06<00:08,  2.65it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  89%|████████▉ | 176/197 [01:06<00:07,  2.67it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  90%|█████████ | 178/197 [01:06<00:07,  2.69it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  91%|█████████▏| 180/197 [01:06<00:06,  2.71it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  92%|█████████▏| 182/197 [01:06<00:05,  2.73it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.60it/s]\u001b[A\n",
      "Epoch 2:  93%|█████████▎| 184/197 [01:07<00:04,  2.75it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  94%|█████████▍| 186/197 [01:07<00:03,  2.77it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  95%|█████████▌| 188/197 [01:07<00:03,  2.80it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  96%|█████████▋| 190/197 [01:07<00:02,  2.82it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  97%|█████████▋| 192/197 [01:07<00:01,  2.84it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2:  98%|█████████▊| 194/197 [01:08<00:01,  2.86it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Validating:  92%|█████████▎| 37/40 [00:03<00:00,  9.91it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 196/197 [01:08<00:00,  2.88it/s, loss=1.03, v_num=14, val_loss=1.460, val_acc=0.513]\n",
      "Epoch 2: 100%|██████████| 197/197 [01:08<00:00,  2.89it/s, loss=1.03, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  82%|████████▏ | 162/197 [01:05<00:14,  2.50it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  86%|████████▋ | 170/197 [01:06<00:10,  2.59it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.74it/s]\u001b[A\n",
      "Epoch 3:  88%|████████▊ | 174/197 [01:06<00:08,  2.63it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.53it/s]\u001b[A\n",
      "Epoch 3:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  7.75it/s]\u001b[A\n",
      "Epoch 3:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  7.43it/s]\u001b[A\n",
      "Epoch 3:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:02,  7.28it/s]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 182/197 [01:07<00:05,  2.70it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:02,  7.31it/s]\u001b[A\n",
      "Epoch 3:  93%|█████████▎| 184/197 [01:07<00:04,  2.72it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  68%|██████▊   | 27/40 [00:03<00:01,  7.26it/s]\u001b[A\n",
      "Epoch 3:  94%|█████████▍| 186/197 [01:08<00:04,  2.74it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  7.23it/s]\u001b[A\n",
      "Epoch 3:  95%|█████████▌| 188/197 [01:08<00:03,  2.76it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  7.14it/s]\u001b[A\n",
      "Epoch 3:  96%|█████████▋| 190/197 [01:08<00:02,  2.78it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  82%|████████▎ | 33/40 [00:04<00:00,  7.31it/s]\u001b[A\n",
      "Epoch 3:  97%|█████████▋| 192/197 [01:09<00:01,  2.80it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Validating:  88%|████████▊ | 35/40 [00:04<00:00,  7.86it/s]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 194/197 [01:09<00:01,  2.82it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3:  99%|█████████▉| 196/197 [01:09<00:00,  2.84it/s, loss=0.895, v_num=14, val_loss=1.050, val_acc=0.632]\n",
      "Epoch 3: 100%|██████████| 197/197 [01:09<00:00,  2.85it/s, loss=0.895, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  80%|████████  | 158/197 [01:05<00:16,  2.43it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  8.80it/s]\u001b[A\n",
      "Epoch 4:  81%|████████  | 160/197 [01:05<00:15,  2.46it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  82%|████████▏ | 162/197 [01:05<00:14,  2.48it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  83%|████████▎ | 164/197 [01:05<00:13,  2.50it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  84%|████████▍ | 166/197 [01:06<00:12,  2.53it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  87%|████████▋ | 172/197 [01:06<00:09,  2.59it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:03,  7.31it/s]\u001b[A\n",
      "Epoch 4:  88%|████████▊ | 174/197 [01:07<00:08,  2.60it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Validating:  42%|████▎     | 17/40 [00:02<00:03,  5.93it/s]\u001b[A\n",
      "Epoch 4:  89%|████████▉ | 176/197 [01:07<00:08,  2.62it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  90%|█████████ | 178/197 [01:07<00:07,  2.64it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  91%|█████████▏| 180/197 [01:08<00:06,  2.66it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  92%|█████████▏| 182/197 [01:08<00:05,  2.68it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  93%|█████████▎| 184/197 [01:08<00:04,  2.70it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  94%|█████████▍| 186/197 [01:08<00:04,  2.72it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  95%|█████████▌| 188/197 [01:08<00:03,  2.75it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  96%|█████████▋| 190/197 [01:09<00:02,  2.77it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  97%|█████████▋| 192/197 [01:09<00:01,  2.79it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  98%|█████████▊| 194/197 [01:09<00:01,  2.81it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4:  99%|█████████▉| 196/197 [01:09<00:00,  2.83it/s, loss=0.788, v_num=14, val_loss=1.170, val_acc=0.634]\n",
      "Epoch 4: 100%|██████████| 197/197 [01:09<00:00,  2.84it/s, loss=0.788, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  80%|████████  | 158/197 [01:05<00:15,  2.44it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:05,  7.55it/s]\u001b[A\n",
      "Epoch 5:  81%|████████  | 160/197 [01:05<00:15,  2.47it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  84%|████████▍ | 166/197 [01:05<00:12,  2.53it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  85%|████████▌ | 168/197 [01:06<00:11,  2.56it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.13it/s]\u001b[A\n",
      "Epoch 5:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  89%|████████▉ | 176/197 [01:06<00:07,  2.64it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.89it/s]\u001b[A\n",
      "Epoch 5:  94%|█████████▍| 186/197 [01:07<00:03,  2.75it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  9.39it/s]\u001b[A\n",
      "Epoch 5:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.747, v_num=14, val_loss=1.200, val_acc=0.638]\n",
      "Epoch 5: 100%|██████████| 197/197 [01:09<00:00,  2.87it/s, loss=0.747, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  80%|████████  | 158/197 [01:04<00:15,  2.46it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  81%|████████  | 160/197 [01:04<00:14,  2.48it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  82%|████████▏ | 162/197 [01:05<00:13,  2.50it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.75it/s]\u001b[A\n",
      "Epoch 6:  84%|████████▍ | 166/197 [01:05<00:12,  2.55it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  22%|██▎       | 9/40 [00:00<00:03,  8.85it/s]\u001b[A\n",
      "Epoch 6:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  86%|████████▋ | 170/197 [01:06<00:10,  2.59it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.41it/s]\u001b[A\n",
      "Epoch 6:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  8.64it/s]\u001b[A\n",
      "Epoch 6:  88%|████████▊ | 174/197 [01:06<00:08,  2.63it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  7.76it/s]\u001b[A\n",
      "Epoch 6:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  90%|█████████ | 178/197 [01:06<00:07,  2.67it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  91%|█████████▏| 180/197 [01:07<00:06,  2.70it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  92%|█████████▏| 182/197 [01:07<00:05,  2.72it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  93%|█████████▎| 184/197 [01:07<00:04,  2.74it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  94%|█████████▍| 186/197 [01:07<00:03,  2.76it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  95%|█████████▌| 188/197 [01:07<00:03,  2.78it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.18it/s]\u001b[A\n",
      "Epoch 6:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  98%|█████████▊| 194/197 [01:08<00:01,  2.84it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6:  99%|█████████▉| 196/197 [01:08<00:00,  2.86it/s, loss=0.551, v_num=14, val_loss=0.930, val_acc=0.695]\n",
      "Epoch 6: 100%|██████████| 197/197 [01:08<00:00,  2.87it/s, loss=0.551, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  80%|████████  | 158/197 [01:04<00:15,  2.46it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  81%|████████  | 160/197 [01:04<00:14,  2.48it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03,  9.87it/s]\u001b[A\n",
      "Epoch 7:  82%|████████▏ | 162/197 [01:05<00:13,  2.50it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  84%|████████▍ | 166/197 [01:05<00:12,  2.55it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  86%|████████▋ | 170/197 [01:05<00:10,  2.59it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.97it/s]\u001b[A\n",
      "Epoch 7:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.75it/s]\u001b[A\n",
      "Epoch 7:  88%|████████▊ | 174/197 [01:06<00:08,  2.64it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.51it/s]\u001b[A\n",
      "Epoch 7:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  90%|█████████ | 178/197 [01:06<00:07,  2.68it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  91%|█████████▏| 180/197 [01:07<00:06,  2.70it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.31it/s]\u001b[A\n",
      "Epoch 7:  92%|█████████▏| 182/197 [01:07<00:05,  2.72it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  93%|█████████▎| 184/197 [01:07<00:04,  2.74it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.01it/s]\u001b[A\n",
      "Epoch 7:  94%|█████████▍| 186/197 [01:07<00:03,  2.76it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  95%|█████████▌| 188/197 [01:07<00:03,  2.78it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.19it/s]\u001b[A\n",
      "Epoch 7:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7:  98%|█████████▊| 194/197 [01:08<00:01,  2.84it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Validating:  92%|█████████▎| 37/40 [00:03<00:00,  9.20it/s]\u001b[A\n",
      "Epoch 7:  99%|█████████▉| 196/197 [01:08<00:00,  2.86it/s, loss=0.64, v_num=14, val_loss=0.722, val_acc=0.752]\n",
      "Epoch 7: 100%|██████████| 197/197 [01:08<00:00,  2.87it/s, loss=0.64, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  80%|████████  | 158/197 [01:05<00:16,  2.44it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  81%|████████  | 160/197 [01:05<00:15,  2.46it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  82%|████████▏ | 162/197 [01:05<00:14,  2.48it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  84%|████████▍ | 166/197 [01:06<00:12,  2.53it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.26it/s]\u001b[A\n",
      "Epoch 8:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  89%|████████▉ | 176/197 [01:07<00:07,  2.64it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  91%|█████████▏| 180/197 [01:07<00:06,  2.68it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  92%|█████████▏| 182/197 [01:07<00:05,  2.70it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.99it/s]\u001b[A\n",
      "Epoch 8:  93%|█████████▎| 184/197 [01:07<00:04,  2.72it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.24it/s]\u001b[A\n",
      "Epoch 8:  94%|█████████▍| 186/197 [01:08<00:04,  2.74it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  95%|█████████▌| 188/197 [01:08<00:03,  2.76it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.33it/s]\u001b[A\n",
      "Epoch 8:  97%|█████████▋| 192/197 [01:08<00:01,  2.80it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  8.69it/s]\u001b[A\n",
      "Epoch 8:  98%|█████████▊| 194/197 [01:09<00:01,  2.82it/s, loss=0.511, v_num=14, val_loss=0.832, val_acc=0.731]\n",
      "Epoch 8: 100%|██████████| 197/197 [01:09<00:00,  2.86it/s, loss=0.511, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  80%|████████  | 158/197 [01:05<00:15,  2.44it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:05,  7.69it/s]\u001b[A\n",
      "Epoch 9:  81%|████████  | 160/197 [01:05<00:15,  2.47it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.99it/s]\u001b[A\n",
      "Epoch 9:  84%|████████▍ | 166/197 [01:05<00:12,  2.53it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  22%|██▎       | 9/40 [00:00<00:03,  8.95it/s]\u001b[A\n",
      "Epoch 9:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.92it/s]\u001b[A\n",
      "Epoch 9:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.02it/s]\u001b[A\n",
      "Epoch 9:  89%|████████▉ | 176/197 [01:07<00:07,  2.64it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  91%|█████████▏| 180/197 [01:07<00:06,  2.68it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.46it/s]\u001b[A\n",
      "Epoch 9:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.65it/s]\u001b[A\n",
      "Epoch 9:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.76it/s]\u001b[A\n",
      "Epoch 9:  94%|█████████▍| 186/197 [01:08<00:04,  2.75it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  9.02it/s]\u001b[A\n",
      "Epoch 9:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  8.47it/s]\u001b[A\n",
      "Epoch 9:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  8.67it/s]\u001b[A\n",
      "Epoch 9:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9:  99%|█████████▉| 196/197 [01:09<00:00,  2.85it/s, loss=0.497, v_num=14, val_loss=0.778, val_acc=0.755]\n",
      "Epoch 9: 100%|██████████| 197/197 [01:09<00:00,  2.86it/s, loss=0.497, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  82%|████████▏ | 162/197 [01:05<00:14,  2.50it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  86%|████████▋ | 170/197 [01:06<00:10,  2.59it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.68it/s]\u001b[A\n",
      "Epoch 10:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.13it/s]\u001b[A\n",
      "Epoch 10:  88%|████████▊ | 174/197 [01:06<00:08,  2.63it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.46it/s]\u001b[A\n",
      "Epoch 10:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  90%|█████████ | 178/197 [01:06<00:07,  2.67it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.81it/s]\u001b[A\n",
      "Epoch 10:  92%|█████████▏| 182/197 [01:07<00:05,  2.72it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  93%|█████████▎| 184/197 [01:07<00:04,  2.74it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  94%|█████████▍| 186/197 [01:07<00:03,  2.76it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  95%|█████████▌| 188/197 [01:07<00:03,  2.78it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  98%|█████████▊| 194/197 [01:08<00:01,  2.84it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10:  99%|█████████▉| 196/197 [01:08<00:00,  2.87it/s, loss=0.386, v_num=14, val_loss=0.773, val_acc=0.749]\n",
      "Epoch 10: 100%|██████████| 197/197 [01:08<00:00,  2.88it/s, loss=0.386, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  81%|████████  | 160/197 [01:05<00:14,  2.48it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  82%|████████▏ | 162/197 [01:05<00:14,  2.50it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.86it/s]\u001b[A\n",
      "Epoch 11:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  86%|████████▋ | 170/197 [01:06<00:10,  2.59it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  88%|████████▊ | 174/197 [01:06<00:08,  2.64it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  89%|████████▉ | 176/197 [01:06<00:07,  2.66it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  90%|█████████ | 178/197 [01:06<00:07,  2.68it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  91%|█████████▏| 180/197 [01:06<00:06,  2.70it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  92%|█████████▏| 182/197 [01:07<00:05,  2.73it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  93%|█████████▎| 184/197 [01:07<00:04,  2.75it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  94%|█████████▍| 186/197 [01:07<00:03,  2.77it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  95%|█████████▌| 188/197 [01:07<00:03,  2.79it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  96%|█████████▋| 190/197 [01:07<00:02,  2.81it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  97%|█████████▋| 192/197 [01:08<00:01,  2.83it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11:  98%|█████████▊| 194/197 [01:08<00:01,  2.85it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Validating:  92%|█████████▎| 37/40 [00:03<00:00,  9.30it/s]\u001b[A\n",
      "Epoch 11:  99%|█████████▉| 196/197 [01:08<00:00,  2.87it/s, loss=0.422, v_num=14, val_loss=0.646, val_acc=0.788]\n",
      "Epoch 11: 100%|██████████| 197/197 [01:08<00:00,  2.88it/s, loss=0.422, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  85%|████████▌ | 168/197 [01:05<00:11,  2.56it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  87%|████████▋ | 172/197 [01:06<00:09,  2.61it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  88%|████████▊ | 174/197 [01:06<00:08,  2.63it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.76it/s]\u001b[A\n",
      "Epoch 12:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  9.03it/s]\u001b[A\n",
      "Epoch 12:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  93%|█████████▎| 184/197 [01:07<00:04,  2.74it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  94%|█████████▍| 186/197 [01:07<00:03,  2.76it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  95%|█████████▌| 188/197 [01:08<00:03,  2.78it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  98%|█████████▊| 194/197 [01:08<00:01,  2.84it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12:  99%|█████████▉| 196/197 [01:08<00:00,  2.86it/s, loss=0.416, v_num=14, val_loss=0.692, val_acc=0.779]\n",
      "Epoch 12: 100%|██████████| 197/197 [01:09<00:00,  2.87it/s, loss=0.416, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03,  9.66it/s]\u001b[A\n",
      "Epoch 13:  82%|████████▏ | 162/197 [01:05<00:14,  2.50it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:03,  9.49it/s]\u001b[A\n",
      "Epoch 13:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  9.52it/s]\u001b[A\n",
      "Epoch 13:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.36it/s]\u001b[A\n",
      "Epoch 13:  85%|████████▌ | 168/197 [01:06<00:11,  2.56it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  88%|████████▊ | 174/197 [01:06<00:08,  2.63it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.84it/s]\u001b[A\n",
      "Epoch 13:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.83it/s]\u001b[A\n",
      "Epoch 13:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  94%|█████████▍| 186/197 [01:07<00:03,  2.75it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  98%|█████████▊| 194/197 [01:08<00:01,  2.84it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13:  99%|█████████▉| 196/197 [01:08<00:00,  2.86it/s, loss=0.348, v_num=14, val_loss=0.701, val_acc=0.784]\n",
      "Epoch 13: 100%|██████████| 197/197 [01:08<00:00,  2.87it/s, loss=0.348, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  80%|████████  | 158/197 [01:05<00:16,  2.42it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.40it/s]\u001b[A\n",
      "Epoch 14:  81%|████████  | 160/197 [01:05<00:15,  2.45it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  8.25it/s]\u001b[A\n",
      "Epoch 14:  82%|████████▏ | 162/197 [01:06<00:14,  2.47it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  8.10it/s]\u001b[A\n",
      "Epoch 14:  83%|████████▎ | 164/197 [01:06<00:13,  2.49it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  84%|████████▍ | 166/197 [01:06<00:12,  2.51it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  85%|████████▌ | 168/197 [01:06<00:11,  2.53it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  8.93it/s]\u001b[A\n",
      "Epoch 14:  86%|████████▋ | 170/197 [01:06<00:10,  2.55it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:03,  8.47it/s]\u001b[A\n",
      "Epoch 14:  87%|████████▋ | 172/197 [01:07<00:09,  2.57it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:03,  8.02it/s]\u001b[A\n",
      "Epoch 14:  88%|████████▊ | 174/197 [01:07<00:08,  2.59it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  89%|████████▉ | 176/197 [01:07<00:08,  2.61it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  90%|█████████ | 178/197 [01:07<00:07,  2.64it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  9.38it/s]\u001b[A\n",
      "Epoch 14:  91%|█████████▏| 180/197 [01:08<00:06,  2.66it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  92%|█████████▏| 182/197 [01:08<00:05,  2.68it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  93%|█████████▎| 184/197 [01:08<00:04,  2.70it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.98it/s]\u001b[A\n",
      "Epoch 14:  94%|█████████▍| 186/197 [01:08<00:04,  2.72it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  95%|█████████▌| 188/197 [01:08<00:03,  2.74it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  96%|█████████▋| 190/197 [01:09<00:02,  2.76it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  97%|█████████▋| 192/197 [01:09<00:01,  2.79it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14:  98%|█████████▊| 194/197 [01:09<00:01,  2.81it/s, loss=0.325, v_num=14, val_loss=0.660, val_acc=0.792]\n",
      "Epoch 14: 100%|██████████| 197/197 [01:09<00:00,  2.84it/s, loss=0.325, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  80%|████████  | 158/197 [01:05<00:16,  2.43it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  81%|████████  | 160/197 [01:05<00:15,  2.46it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  82%|████████▏ | 162/197 [01:05<00:14,  2.48it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  83%|████████▎ | 164/197 [01:05<00:13,  2.50it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  84%|████████▍ | 166/197 [01:06<00:12,  2.53it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  87%|████████▋ | 172/197 [01:06<00:09,  2.59it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  89%|████████▉ | 176/197 [01:07<00:07,  2.64it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  91%|█████████▏| 180/197 [01:07<00:06,  2.68it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  94%|█████████▍| 186/197 [01:08<00:04,  2.75it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  9.83it/s]\u001b[A\n",
      "Epoch 15:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Validating:  92%|█████████▎| 37/40 [00:03<00:00,  9.07it/s]\u001b[A\n",
      "Epoch 15:  99%|█████████▉| 196/197 [01:09<00:00,  2.85it/s, loss=0.329, v_num=14, val_loss=0.485, val_acc=0.842]\n",
      "Epoch 15: 100%|██████████| 197/197 [01:09<00:00,  2.85it/s, loss=0.329, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  80%|████████  | 158/197 [01:07<00:16,  2.36it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:05,  7.19it/s]\u001b[A\n",
      "Epoch 16:  81%|████████  | 160/197 [01:07<00:15,  2.38it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.51it/s]\u001b[A\n",
      "Epoch 16:  82%|████████▏ | 162/197 [01:07<00:14,  2.40it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  8.14it/s]\u001b[A\n",
      "Epoch 16:  83%|████████▎ | 164/197 [01:08<00:13,  2.42it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  84%|████████▍ | 166/197 [01:08<00:12,  2.45it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  9.40it/s]\u001b[A\n",
      "Epoch 16:  85%|████████▌ | 168/197 [01:08<00:11,  2.47it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  86%|████████▋ | 170/197 [01:08<00:10,  2.49it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  87%|████████▋ | 172/197 [01:08<00:09,  2.51it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  88%|████████▊ | 174/197 [01:09<00:09,  2.53it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  89%|████████▉ | 176/197 [01:09<00:08,  2.55it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.77it/s]\u001b[A\n",
      "Epoch 16:  90%|█████████ | 178/197 [01:09<00:07,  2.57it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  91%|█████████▏| 180/197 [01:09<00:06,  2.59it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.18it/s]\u001b[A\n",
      "Epoch 16:  92%|█████████▏| 182/197 [01:10<00:05,  2.61it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.43it/s]\u001b[A\n",
      "Epoch 16:  93%|█████████▎| 184/197 [01:10<00:04,  2.63it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  9.56it/s]\u001b[A\n",
      "Epoch 16:  94%|█████████▍| 186/197 [01:10<00:04,  2.66it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.66it/s]\u001b[A\n",
      "Epoch 16:  95%|█████████▌| 188/197 [01:10<00:03,  2.67it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  8.26it/s]\u001b[A\n",
      "Epoch 16:  96%|█████████▋| 190/197 [01:10<00:02,  2.69it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  97%|█████████▋| 192/197 [01:11<00:01,  2.71it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16:  98%|█████████▊| 194/197 [01:11<00:01,  2.74it/s, loss=0.292, v_num=14, val_loss=0.533, val_acc=0.834]\n",
      "Epoch 16: 100%|██████████| 197/197 [01:11<00:00,  2.77it/s, loss=0.292, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  80%|████████  | 158/197 [01:05<00:16,  2.42it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 17:  81%|████████  | 160/197 [01:05<00:15,  2.45it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  82%|████████▏ | 162/197 [01:05<00:14,  2.47it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  83%|████████▎ | 164/197 [01:06<00:13,  2.49it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  84%|████████▍ | 166/197 [01:06<00:12,  2.52it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  85%|████████▌ | 168/197 [01:06<00:11,  2.54it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  86%|████████▋ | 170/197 [01:06<00:10,  2.56it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.08it/s]\u001b[A\n",
      "Epoch 17:  87%|████████▋ | 172/197 [01:07<00:09,  2.58it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  8.40it/s]\u001b[A\n",
      "Epoch 17:  88%|████████▊ | 174/197 [01:07<00:08,  2.60it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  89%|████████▉ | 176/197 [01:07<00:08,  2.62it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  9.28it/s]\u001b[A\n",
      "Epoch 17:  90%|█████████ | 178/197 [01:07<00:07,  2.64it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  91%|█████████▏| 180/197 [01:07<00:06,  2.66it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  8.59it/s]\u001b[A\n",
      "Epoch 17:  92%|█████████▏| 182/197 [01:08<00:05,  2.68it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  8.14it/s]\u001b[A\n",
      "Epoch 17:  93%|█████████▎| 184/197 [01:08<00:04,  2.70it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  94%|█████████▍| 186/197 [01:08<00:04,  2.72it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  95%|█████████▌| 188/197 [01:08<00:03,  2.75it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  96%|█████████▋| 190/197 [01:09<00:02,  2.77it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  97%|█████████▋| 192/197 [01:09<00:01,  2.79it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17:  98%|█████████▊| 194/197 [01:09<00:01,  2.81it/s, loss=0.283, v_num=14, val_loss=0.491, val_acc=0.836]\n",
      "Epoch 17: 100%|██████████| 197/197 [01:09<00:00,  2.84it/s, loss=0.283, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  80%|████████  | 158/197 [01:04<00:15,  2.46it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  8.94it/s]\u001b[A\n",
      "Epoch 18:  81%|████████  | 160/197 [01:04<00:14,  2.48it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  82%|████████▏ | 162/197 [01:05<00:13,  2.51it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  8.65it/s]\u001b[A\n",
      "Epoch 18:  83%|████████▎ | 164/197 [01:05<00:13,  2.53it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:04,  8.20it/s]\u001b[A\n",
      "Epoch 18:  84%|████████▍ | 166/197 [01:05<00:12,  2.55it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.93it/s]\u001b[A\n",
      "Epoch 18:  85%|████████▌ | 168/197 [01:05<00:11,  2.57it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  86%|████████▋ | 170/197 [01:05<00:10,  2.59it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.72it/s]\u001b[A\n",
      "Epoch 18:  87%|████████▋ | 172/197 [01:06<00:09,  2.62it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  88%|████████▊ | 174/197 [01:06<00:08,  2.64it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.41it/s]\u001b[A\n",
      "Epoch 18:  89%|████████▉ | 176/197 [01:06<00:07,  2.66it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.69it/s]\u001b[A\n",
      "Epoch 18:  90%|█████████ | 178/197 [01:06<00:07,  2.68it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  91%|█████████▏| 180/197 [01:07<00:06,  2.70it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.56it/s]\u001b[A\n",
      "Epoch 18:  92%|█████████▏| 182/197 [01:07<00:05,  2.72it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.47it/s]\u001b[A\n",
      "Epoch 18:  93%|█████████▎| 184/197 [01:07<00:04,  2.74it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.96it/s]\u001b[A\n",
      "Epoch 18:  94%|█████████▍| 186/197 [01:07<00:03,  2.76it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.43it/s]\u001b[A\n",
      "Epoch 18:  95%|█████████▌| 188/197 [01:07<00:03,  2.78it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  96%|█████████▋| 190/197 [01:08<00:02,  2.80it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  97%|█████████▋| 192/197 [01:08<00:01,  2.82it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18:  98%|█████████▊| 194/197 [01:08<00:01,  2.85it/s, loss=0.29, v_num=14, val_loss=0.514, val_acc=0.843]\n",
      "Epoch 18: 100%|██████████| 197/197 [01:08<00:00,  2.88it/s, loss=0.29, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19:  80%|████████  | 158/197 [01:05<00:15,  2.45it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.08it/s]\u001b[A\n",
      "Epoch 19:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03,  9.27it/s]\u001b[A\n",
      "Epoch 19:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  7.88it/s]\u001b[A\n",
      "Epoch 19:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:04,  7.90it/s]\u001b[A\n",
      "Epoch 19:  84%|████████▍ | 166/197 [01:05<00:12,  2.53it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.71it/s]\u001b[A\n",
      "Epoch 19:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  7.81it/s]\u001b[A\n",
      "Epoch 19:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:03,  7.53it/s]\u001b[A\n",
      "Epoch 19:  87%|████████▋ | 172/197 [01:06<00:09,  2.59it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:03,  7.81it/s]\u001b[A\n",
      "Epoch 19:  88%|████████▊ | 174/197 [01:06<00:08,  2.61it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  42%|████▎     | 17/40 [00:02<00:02,  8.60it/s]\u001b[A\n",
      "Epoch 19:  89%|████████▉ | 176/197 [01:07<00:07,  2.63it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.86it/s]\u001b[A\n",
      "Epoch 19:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  9.17it/s]\u001b[A\n",
      "Epoch 19:  91%|█████████▏| 180/197 [01:07<00:06,  2.68it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  8.84it/s]\u001b[A\n",
      "Epoch 19:  92%|█████████▏| 182/197 [01:07<00:05,  2.70it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  62%|██████▎   | 25/40 [00:03<00:01,  7.84it/s]\u001b[A\n",
      "Epoch 19:  93%|█████████▎| 184/197 [01:08<00:04,  2.71it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19:  94%|█████████▍| 186/197 [01:08<00:04,  2.74it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.48it/s]\u001b[A\n",
      "Epoch 19:  95%|█████████▌| 188/197 [01:08<00:03,  2.75it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  7.76it/s]\u001b[A\n",
      "Epoch 19:  96%|█████████▋| 190/197 [01:08<00:02,  2.77it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19:  97%|█████████▋| 192/197 [01:09<00:01,  2.79it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19:  98%|█████████▊| 194/197 [01:09<00:01,  2.82it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19:  99%|█████████▉| 196/197 [01:09<00:00,  2.84it/s, loss=0.248, v_num=14, val_loss=0.461, val_acc=0.851]\n",
      "Epoch 19: 100%|██████████| 197/197 [01:09<00:00,  2.85it/s, loss=0.248, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  80%|████████  | 158/197 [01:04<00:15,  2.45it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  8.30it/s]\u001b[A\n",
      "Epoch 20:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.62it/s]\u001b[A\n",
      "Epoch 20:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  7.37it/s]\u001b[A\n",
      "Epoch 20:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:04,  7.79it/s]\u001b[A\n",
      "Epoch 20:  84%|████████▍ | 166/197 [01:05<00:12,  2.53it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.74it/s]\u001b[A\n",
      "Epoch 20:  85%|████████▌ | 168/197 [01:06<00:11,  2.56it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:03,  9.00it/s]\u001b[A\n",
      "Epoch 20:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  89%|████████▉ | 176/197 [01:06<00:07,  2.64it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.86it/s]\u001b[A\n",
      "Epoch 20:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.48it/s]\u001b[A\n",
      "Epoch 20:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  94%|█████████▍| 186/197 [01:07<00:03,  2.75it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.46it/s]\u001b[A\n",
      "Epoch 20:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.35it/s]\u001b[A\n",
      "Epoch 20:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.255, v_num=14, val_loss=0.380, val_acc=0.876]\n",
      "Epoch 20: 100%|██████████| 197/197 [01:09<00:00,  2.87it/s, loss=0.255, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  80%|████████  | 158/197 [01:05<00:16,  2.44it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 21:  81%|████████  | 160/197 [01:05<00:15,  2.46it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  82%|████████▏ | 162/197 [01:05<00:14,  2.48it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  83%|████████▎ | 164/197 [01:05<00:13,  2.51it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  84%|████████▍ | 166/197 [01:06<00:12,  2.53it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  85%|████████▌ | 168/197 [01:06<00:11,  2.55it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  86%|████████▋ | 170/197 [01:06<00:10,  2.57it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  89%|████████▉ | 176/197 [01:06<00:07,  2.64it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  90%|█████████ | 178/197 [01:07<00:07,  2.66it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  94%|█████████▍| 186/197 [01:07<00:03,  2.75it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  9.90it/s]\u001b[A\n",
      "Epoch 21:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21:  99%|█████████▉| 196/197 [01:08<00:00,  2.86it/s, loss=0.269, v_num=14, val_loss=0.414, val_acc=0.867]\n",
      "Epoch 21: 100%|██████████| 197/197 [01:09<00:00,  2.85it/s, loss=0.269, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  80%|████████  | 158/197 [01:05<00:15,  2.45it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 22:  81%|████████  | 160/197 [01:05<00:14,  2.47it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  82%|████████▏ | 162/197 [01:05<00:14,  2.49it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  83%|████████▎ | 164/197 [01:05<00:13,  2.52it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  84%|████████▍ | 166/197 [01:05<00:12,  2.54it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  85%|████████▌ | 168/197 [01:06<00:11,  2.56it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  86%|████████▋ | 170/197 [01:06<00:10,  2.58it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  87%|████████▋ | 172/197 [01:06<00:09,  2.60it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.24it/s]\u001b[A\n",
      "Epoch 22:  88%|████████▊ | 174/197 [01:06<00:08,  2.62it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.65it/s]\u001b[A\n",
      "Epoch 22:  89%|████████▉ | 176/197 [01:06<00:07,  2.65it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.97it/s]\u001b[A\n",
      "Epoch 22:  90%|█████████ | 178/197 [01:07<00:07,  2.67it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  91%|█████████▏| 180/197 [01:07<00:06,  2.69it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  9.46it/s]\u001b[A\n",
      "Epoch 22:  92%|█████████▏| 182/197 [01:07<00:05,  2.71it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.12it/s]\u001b[A\n",
      "Epoch 22:  93%|█████████▎| 184/197 [01:07<00:04,  2.73it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.21it/s]\u001b[A\n",
      "Epoch 22:  94%|█████████▍| 186/197 [01:08<00:04,  2.75it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.16it/s]\u001b[A\n",
      "Epoch 22:  95%|█████████▌| 188/197 [01:08<00:03,  2.77it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  8.68it/s]\u001b[A\n",
      "Epoch 22:  96%|█████████▋| 190/197 [01:08<00:02,  2.79it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  8.77it/s]\u001b[A\n",
      "Epoch 22:  97%|█████████▋| 192/197 [01:08<00:01,  2.81it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  98%|█████████▊| 194/197 [01:08<00:01,  2.83it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22:  99%|█████████▉| 196/197 [01:09<00:00,  2.85it/s, loss=0.182, v_num=14, val_loss=0.406, val_acc=0.872]\n",
      "Epoch 22: 100%|██████████| 197/197 [01:09<00:00,  2.86it/s, loss=0.182, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  80%|████████  | 158/197 [01:05<00:16,  2.42it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 23:  81%|████████  | 160/197 [01:05<00:15,  2.44it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  82%|████████▏ | 162/197 [01:06<00:14,  2.47it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  83%|████████▎ | 164/197 [01:06<00:13,  2.49it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  84%|████████▍ | 166/197 [01:06<00:12,  2.51it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  85%|████████▌ | 168/197 [01:06<00:11,  2.54it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  86%|████████▋ | 170/197 [01:06<00:10,  2.56it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  87%|████████▋ | 172/197 [01:07<00:09,  2.58it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.89it/s]\u001b[A\n",
      "Epoch 23:  88%|████████▊ | 174/197 [01:07<00:08,  2.60it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  89%|████████▉ | 176/197 [01:07<00:08,  2.62it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  90%|█████████ | 178/197 [01:07<00:07,  2.65it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  91%|█████████▏| 180/197 [01:07<00:06,  2.67it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  92%|█████████▏| 182/197 [01:08<00:05,  2.69it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  93%|█████████▎| 184/197 [01:08<00:04,  2.71it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  94%|█████████▍| 186/197 [01:08<00:04,  2.73it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  95%|█████████▌| 188/197 [01:08<00:03,  2.76it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  96%|█████████▋| 190/197 [01:08<00:02,  2.78it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  97%|█████████▋| 192/197 [01:08<00:01,  2.80it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  98%|█████████▊| 194/197 [01:09<00:01,  2.82it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23:  99%|█████████▉| 196/197 [01:09<00:00,  2.84it/s, loss=0.18, v_num=14, val_loss=0.418, val_acc=0.865]\n",
      "Epoch 23: 100%|██████████| 197/197 [01:09<00:00,  2.83it/s, loss=0.18, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  80%|████████  | 158/197 [01:07<00:16,  2.36it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.28it/s]\u001b[A\n",
      "Epoch 24:  81%|████████  | 160/197 [01:07<00:15,  2.39it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:03,  9.47it/s]\u001b[A\n",
      "Epoch 24:  82%|████████▏ | 162/197 [01:07<00:14,  2.41it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:03,  8.99it/s]\u001b[A\n",
      "Epoch 24:  83%|████████▎ | 164/197 [01:07<00:13,  2.43it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  8.46it/s]\u001b[A\n",
      "Epoch 24:  84%|████████▍ | 166/197 [01:08<00:12,  2.45it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.04it/s]\u001b[A\n",
      "Epoch 24:  85%|████████▌ | 168/197 [01:08<00:11,  2.47it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  86%|████████▋ | 170/197 [01:08<00:10,  2.49it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  87%|████████▋ | 172/197 [01:08<00:09,  2.52it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  88%|████████▊ | 174/197 [01:08<00:09,  2.54it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  89%|████████▉ | 176/197 [01:09<00:08,  2.56it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  90%|█████████ | 178/197 [01:09<00:07,  2.58it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  91%|█████████▏| 180/197 [01:09<00:06,  2.60it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  92%|█████████▏| 182/197 [01:09<00:05,  2.62it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  93%|█████████▎| 184/197 [01:09<00:04,  2.65it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  94%|█████████▍| 186/197 [01:10<00:04,  2.67it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  95%|█████████▌| 188/197 [01:10<00:03,  2.69it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  96%|█████████▋| 190/197 [01:10<00:02,  2.71it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  97%|█████████▋| 192/197 [01:10<00:01,  2.73it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24:  98%|█████████▊| 194/197 [01:10<00:01,  2.75it/s, loss=0.175, v_num=14, val_loss=0.322, val_acc=0.896]\n",
      "Epoch 24: 100%|██████████| 197/197 [01:11<00:00,  2.78it/s, loss=0.175, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  80%|████████  | 158/197 [01:06<00:16,  2.40it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 25:  81%|████████  | 160/197 [01:06<00:15,  2.43it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  82%|████████▏ | 162/197 [01:06<00:14,  2.45it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  83%|████████▎ | 164/197 [01:06<00:13,  2.47it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  84%|████████▍ | 166/197 [01:06<00:12,  2.49it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  85%|████████▌ | 168/197 [01:07<00:11,  2.52it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  86%|████████▋ | 170/197 [01:07<00:10,  2.54it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  87%|████████▋ | 172/197 [01:07<00:09,  2.56it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  9.20it/s]\u001b[A\n",
      "Epoch 25:  88%|████████▊ | 174/197 [01:07<00:08,  2.58it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.55it/s]\u001b[A\n",
      "Epoch 25:  89%|████████▉ | 176/197 [01:08<00:08,  2.60it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  90%|█████████ | 178/197 [01:08<00:07,  2.62it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  91%|█████████▏| 180/197 [01:08<00:06,  2.64it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  92%|█████████▏| 182/197 [01:08<00:05,  2.66it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  9.47it/s]\u001b[A\n",
      "Epoch 25:  93%|█████████▎| 184/197 [01:08<00:04,  2.68it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.57it/s]\u001b[A\n",
      "Epoch 25:  94%|█████████▍| 186/197 [01:09<00:04,  2.70it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  7.94it/s]\u001b[A\n",
      "Epoch 25:  95%|█████████▌| 188/197 [01:09<00:03,  2.72it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  7.54it/s]\u001b[A\n",
      "Epoch 25:  96%|█████████▋| 190/197 [01:09<00:02,  2.74it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  97%|█████████▋| 192/197 [01:09<00:01,  2.76it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  98%|█████████▊| 194/197 [01:10<00:01,  2.78it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25:  99%|█████████▉| 196/197 [01:10<00:00,  2.80it/s, loss=0.121, v_num=14, val_loss=0.301, val_acc=0.906]\n",
      "Epoch 25: 100%|██████████| 197/197 [01:10<00:00,  2.81it/s, loss=0.121, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  80%|████████  | 158/197 [01:05<00:16,  2.41it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.20it/s]\u001b[A\n",
      "Epoch 26:  81%|████████  | 160/197 [01:06<00:15,  2.43it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.71it/s]\u001b[A\n",
      "Epoch 26:  82%|████████▏ | 162/197 [01:06<00:14,  2.45it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  83%|████████▎ | 164/197 [01:06<00:13,  2.48it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  84%|████████▍ | 166/197 [01:06<00:12,  2.50it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  9.49it/s]\u001b[A\n",
      "Epoch 26:  85%|████████▌ | 168/197 [01:07<00:11,  2.52it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  86%|████████▋ | 170/197 [01:07<00:10,  2.54it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:02,  9.46it/s]\u001b[A\n",
      "Epoch 26:  87%|████████▋ | 172/197 [01:07<00:09,  2.56it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:02,  8.76it/s]\u001b[A\n",
      "Epoch 26:  88%|████████▊ | 174/197 [01:07<00:08,  2.59it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  8.27it/s]\u001b[A\n",
      "Epoch 26:  89%|████████▉ | 176/197 [01:07<00:08,  2.60it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  90%|█████████ | 178/197 [01:08<00:07,  2.63it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  9.01it/s]\u001b[A\n",
      "Epoch 26:  91%|█████████▏| 180/197 [01:08<00:06,  2.65it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  92%|█████████▏| 182/197 [01:08<00:05,  2.67it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  93%|█████████▎| 184/197 [01:08<00:04,  2.69it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  94%|█████████▍| 186/197 [01:08<00:04,  2.71it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01, 10.06it/s]\u001b[A\n",
      "Epoch 26:  95%|█████████▌| 188/197 [01:09<00:03,  2.73it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:00,  9.69it/s]\u001b[A\n",
      "Epoch 26:  96%|█████████▋| 190/197 [01:09<00:02,  2.75it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  9.28it/s]\u001b[A\n",
      "Epoch 26:  97%|█████████▋| 192/197 [01:09<00:01,  2.77it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  88%|████████▊ | 35/40 [00:03<00:00,  9.52it/s]\u001b[A\n",
      "Epoch 26:  98%|█████████▊| 194/197 [01:09<00:01,  2.79it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26:  99%|█████████▉| 196/197 [01:10<00:00,  2.81it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 26: 100%|██████████| 197/197 [01:10<00:00,  2.82it/s, loss=0.129, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  80%|████████  | 158/197 [01:05<00:16,  2.43it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:05,  7.75it/s]\u001b[A\n",
      "Epoch 27:  81%|████████  | 160/197 [01:05<00:15,  2.45it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:   8%|▊         | 3/40 [00:00<00:04,  7.87it/s]\u001b[A\n",
      "Epoch 27:  82%|████████▏ | 162/197 [01:06<00:14,  2.47it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  12%|█▎        | 5/40 [00:00<00:04,  8.40it/s]\u001b[A\n",
      "Epoch 27:  83%|████████▎ | 164/197 [01:06<00:13,  2.49it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  18%|█▊        | 7/40 [00:00<00:03,  8.71it/s]\u001b[A\n",
      "Epoch 27:  84%|████████▍ | 166/197 [01:06<00:12,  2.51it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  22%|██▎       | 9/40 [00:01<00:03,  8.75it/s]\u001b[A\n",
      "Epoch 27:  85%|████████▌ | 168/197 [01:06<00:11,  2.53it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  28%|██▊       | 11/40 [00:01<00:03,  8.87it/s]\u001b[A\n",
      "Epoch 27:  86%|████████▋ | 170/197 [01:06<00:10,  2.55it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  32%|███▎      | 13/40 [00:01<00:03,  8.41it/s]\u001b[A\n",
      "Epoch 27:  87%|████████▋ | 172/197 [01:07<00:09,  2.57it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  38%|███▊      | 15/40 [00:01<00:03,  8.12it/s]\u001b[A\n",
      "Epoch 27:  88%|████████▊ | 174/197 [01:07<00:08,  2.59it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  42%|████▎     | 17/40 [00:02<00:02,  8.56it/s]\u001b[A\n",
      "Epoch 27:  89%|████████▉ | 176/197 [01:07<00:08,  2.62it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.73it/s]\u001b[A\n",
      "Epoch 27:  90%|█████████ | 178/197 [01:07<00:07,  2.64it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  8.80it/s]\u001b[A\n",
      "Epoch 27:  91%|█████████▏| 180/197 [01:08<00:06,  2.66it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:01,  8.62it/s]\u001b[A\n",
      "Epoch 27:  92%|█████████▏| 182/197 [01:08<00:05,  2.68it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  8.23it/s]\u001b[A\n",
      "Epoch 27:  93%|█████████▎| 184/197 [01:08<00:04,  2.70it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Validating:  68%|██████▊   | 27/40 [00:03<00:01,  8.10it/s]\u001b[A\n",
      "Epoch 27:  94%|█████████▍| 186/197 [01:08<00:04,  2.72it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  95%|█████████▌| 188/197 [01:09<00:03,  2.74it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  96%|█████████▋| 190/197 [01:09<00:02,  2.76it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  97%|█████████▋| 192/197 [01:09<00:01,  2.78it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  98%|█████████▊| 194/197 [01:09<00:01,  2.80it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27:  99%|█████████▉| 196/197 [01:09<00:00,  2.82it/s, loss=0.0819, v_num=14, val_loss=0.316, val_acc=0.902]\n",
      "Epoch 27: 100%|██████████| 197/197 [01:09<00:00,  2.83it/s, loss=0.0819, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  80%|████████  | 158/197 [01:05<00:16,  2.41it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 28:  81%|████████  | 160/197 [01:06<00:15,  2.43it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  82%|████████▏ | 162/197 [01:06<00:14,  2.45it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  83%|████████▎ | 164/197 [01:06<00:13,  2.48it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  84%|████████▍ | 166/197 [01:06<00:12,  2.50it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  85%|████████▌ | 168/197 [01:06<00:11,  2.52it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  86%|████████▋ | 170/197 [01:07<00:10,  2.55it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  87%|████████▋ | 172/197 [01:07<00:09,  2.57it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  88%|████████▊ | 174/197 [01:07<00:08,  2.59it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02,  9.40it/s]\u001b[A\n",
      "Epoch 28:  89%|████████▉ | 176/197 [01:07<00:08,  2.61it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Validating:  48%|████▊     | 19/40 [00:02<00:02,  8.51it/s]\u001b[A\n",
      "Epoch 28:  90%|█████████ | 178/197 [01:08<00:07,  2.63it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  91%|█████████▏| 180/197 [01:08<00:06,  2.65it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  92%|█████████▏| 182/197 [01:08<00:05,  2.67it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  93%|█████████▎| 184/197 [01:08<00:04,  2.69it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  94%|█████████▍| 186/197 [01:08<00:04,  2.72it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  95%|█████████▌| 188/197 [01:09<00:03,  2.74it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  96%|█████████▋| 190/197 [01:09<00:02,  2.76it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  97%|█████████▋| 192/197 [01:09<00:01,  2.78it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  98%|█████████▊| 194/197 [01:09<00:01,  2.80it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28:  99%|█████████▉| 196/197 [01:09<00:00,  2.82it/s, loss=0.0639, v_num=14, val_loss=0.277, val_acc=0.918]\n",
      "Epoch 28: 100%|██████████| 197/197 [01:10<00:00,  2.83it/s, loss=0.0639, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  80%|████████  | 158/197 [01:06<00:16,  2.41it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      "Validating:   2%|▎         | 1/40 [00:00<00:04,  9.07it/s]\u001b[A\n",
      "Epoch 29:  81%|████████  | 160/197 [01:06<00:15,  2.43it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  82%|████████▏ | 162/197 [01:06<00:14,  2.45it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  83%|████████▎ | 164/197 [01:06<00:13,  2.47it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  84%|████████▍ | 166/197 [01:06<00:12,  2.50it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  85%|████████▌ | 168/197 [01:07<00:11,  2.52it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  86%|████████▋ | 170/197 [01:07<00:10,  2.54it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  87%|████████▋ | 172/197 [01:07<00:09,  2.56it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  88%|████████▊ | 174/197 [01:07<00:08,  2.59it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  42%|████▎     | 17/40 [00:01<00:02, 10.00it/s]\u001b[A\n",
      "Epoch 29:  89%|████████▉ | 176/197 [01:07<00:08,  2.61it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  48%|████▊     | 19/40 [00:01<00:02,  9.65it/s]\u001b[A\n",
      "Epoch 29:  90%|█████████ | 178/197 [01:08<00:07,  2.63it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  52%|█████▎    | 21/40 [00:02<00:02,  8.52it/s]\u001b[A\n",
      "Epoch 29:  91%|█████████▏| 180/197 [01:08<00:06,  2.65it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  57%|█████▊    | 23/40 [00:02<00:02,  8.28it/s]\u001b[A\n",
      "Epoch 29:  92%|█████████▏| 182/197 [01:08<00:05,  2.67it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  62%|██████▎   | 25/40 [00:02<00:01,  8.46it/s]\u001b[A\n",
      "Epoch 29:  93%|█████████▎| 184/197 [01:08<00:04,  2.69it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  68%|██████▊   | 27/40 [00:02<00:01,  8.56it/s]\u001b[A\n",
      "Epoch 29:  94%|█████████▍| 186/197 [01:09<00:04,  2.71it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  72%|███████▎  | 29/40 [00:03<00:01,  8.60it/s]\u001b[A\n",
      "Epoch 29:  95%|█████████▌| 188/197 [01:09<00:03,  2.73it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  78%|███████▊  | 31/40 [00:03<00:01,  8.71it/s]\u001b[A\n",
      "Epoch 29:  96%|█████████▋| 190/197 [01:09<00:02,  2.74it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Validating:  82%|████████▎ | 33/40 [00:03<00:00,  8.12it/s]\u001b[A\n",
      "Epoch 29:  97%|█████████▋| 192/197 [01:09<00:01,  2.76it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  98%|█████████▊| 194/197 [01:10<00:01,  2.78it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29:  99%|█████████▉| 196/197 [01:10<00:00,  2.81it/s, loss=0.0624, v_num=14, val_loss=0.259, val_acc=0.924]\n",
      "Epoch 29: 100%|██████████| 197/197 [01:10<00:00,  2.82it/s, loss=0.0624, v_num=14, val_loss=0.255, val_acc=0.925]\n",
      "Epoch 29: 100%|██████████| 197/197 [01:10<00:00,  2.80it/s, loss=0.0624, v_num=14, val_loss=0.255, val_acc=0.925]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "2022-06-30 02:00:17,710 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n",
      "2022-06-30 02:00:17,711 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - distributed_backend=ddp_subprocess\n",
      "2022-06-30 02:00:17,712 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - All DDP processes registered. Starting ddp with 4 processes\n",
      "2022-06-30 02:00:17,713 - bigdl.nano.pytorch.plugins.ddp_subprocess - INFO - ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Global seed set to 7\n",
      "Global seed set to 7\n",
      "initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All DDP processes registered. Starting ddp with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:106: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  98%|█████████▊| 39/40 [00:03<00:00, 10.29it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9186999797821045, 'test_loss': 0.2728155851364136}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 40/40 [00:04<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:288: UserWarning: cleaning up ddp environment...\n",
      "  rank_zero_warn(\"cleaning up ddp environment...\")\n",
      "/opt/conda/envs/testNotebook/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.teardown has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.teardown.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet(learning_rate=0.1, num_processes=4)\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  num_processes=4,\n",
    "                  use_ipex=True)\n",
    "fit_time_dit_ipex = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_dit_ipex = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|      Precision    | Fit Time(s)       | Accuracy(%) |\n",
      "|        Basic      |       3066.26       |    90.81    |\n",
      "|        Ipex       |       3169.98       |    91.56    |\n",
      "|     Distributed   |       2022.82       |    92.39    |\n",
      "|   Dist with IPEX  |       2104.62       |    91.87    |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "|      Precision    | Fit Time(s)       | Accuracy(%) |\n",
    "|        Basic      |       {:5.2f}       |    {:5.2f}    |\n",
    "|        Ipex       |       {:5.2f}       |    {:5.2f}    |\n",
    "|     Distributed   |       {:5.2f}       |    {:5.2f}    |\n",
    "|   Dist with IPEX  |       {:5.2f}       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    fit_time_basic.best, metric_basic[0]['test_acc']*100,\n",
    "    fit_time_ipex.best, metric_ipex[0]['test_acc']*100,\n",
    "    fit_time_dit.best, metric_dit[0]['test_acc']*100,\n",
    "    fit_time_dit_ipex.best, metric_dit_ipex[0]['test_acc']*100\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('testNotebook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f74e20bd6be619d8bad0cfdf7f83080da032bdcafc0d029c8a063b1b072aff02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
