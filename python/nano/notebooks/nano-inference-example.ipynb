{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10a585bf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bigdl-nao onnxruntime example\n",
    "--- \n",
    "This example shows the usage of bigdl-nano pytorch onnxtuntime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6dfab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchmetrics.functional import accuracy\n",
    "from bigdl.nano.pytorch.trainer import Trainer\n",
    "from bigdl.nano.pytorch.vision import transforms\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57014979",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### CIFAR10 Data Module\n",
    "---\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35edbaa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(data_path, batch_size, num_workers):\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            cifar10_normalization()\n",
    "        ]\n",
    "    )\n",
    "    cifar10_dm = CIFAR10DataModule(\n",
    "        data_dir=data_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        train_transforms=train_transforms,\n",
    "        test_transforms=test_transforms,\n",
    "        val_transforms=test_transforms\n",
    "    )\n",
    "    return cifar10_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780de39c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Resnet\n",
    "___\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d2f1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7f93f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Lightning Module\n",
    "___\n",
    "Check out the [configure_optimizers](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#configure-optimizers) method to use custom Learning Rate schedulers. The OneCycleLR with SGD will get you to around 92-93% accuracy in 20-30 epochs and 93-94% accuracy in 40-50 epochs. Feel free to experiment with different LR schedules from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f728795",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LitResnet(LightningModule):\n",
    "\n",
    "    def __init__(self, learning_rate=0.05):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "        self.example_input_array = torch.Tensor(64, 3, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9da53d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything(7)\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)\n",
    "data_module = prepare_data(PATH_DATASETS, BATCH_SIZE, NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pl_model = LitResnet(learning_rate=0.05)\n",
    "pl_model.datamodule = data_module\n",
    "trainer = Trainer(num_processes = 1,\n",
    "                  use_ipex = False,\n",
    "                  progress_bar_refresh_rate=10,\n",
    "                  max_epochs=30,\n",
    "                  logger=TensorBoardLogger(\"lightning_logs/\", name=\"basic\"),\n",
    "                  callbacks=[LearningRateMonitor(logging_interval=\"step\")])\n",
    "trainer.fit(pl_model, datamodule=data_module)\n",
    "trainer.test(pl_model, datamodule=data_module)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "01c93f5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Get Accelerated Module\n",
    "---\n",
    "Use Train.trace from bigdl.nano.pytorch.trainer to convert a model into an accelerated module for inference.\n",
    "The definition of trace is:\n",
    "```\n",
    "trace(model: nn.Module, input_sample=None, accelerator=None)\n",
    "\n",
    "      :param model: An torch.nn.Module model, including pl.LightningModule.\n",
    "      \n",
    "      :param input_sample: A set of inputs for trace, defaults to None if you have trace before or\n",
    "                             model is a LightningModule with an example_input_array.\n",
    "                             \n",
    "      :param accelerator: The accelerator to use, defaults to None meaning staying in Pytorch\n",
    "                            backend. 'openvino' and 'onnxruntime' are supported for now.\n",
    "                            \n",
    "      :return: Model with different acceleration(OpenVINO/ONNX Runtime).\n",
    "```\n",
    "- *Note* <br>\n",
    "trace is a class method. You should use your Trainer class to call it instead of the Trainer instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6d517",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model = Trainer.trace(pl_model, accelerator=\"onnxruntime\")\n",
    "start = time()\n",
    "for x, _ in pred_loader:\n",
    "    inference_res_onnx = onnx_model(x)\n",
    "onnx_infer_time = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb6a8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "openvino_model = Trainer.trace(pl_model, accelerator=\"openvino\")\n",
    "start = time()\n",
    "for x, _ in data_module.test_dataloader():\n",
    "    inference_res_openvino = openvino_model(x)\n",
    "openvino_infer_time = time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1ed8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "|    Precision   | Inference Time(s) |\n",
    "|     Pytorch    |       {:5.2f}       |\n",
    "|      ONNX      |       {:5.2f}       |\n",
    "|    Openvino    |       {:5.2f}       |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    \n",
    "    infer_time,\n",
    "    onnx_infer_time,\n",
    "    openvino_infer_time\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccf556",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Calibrate Model\n",
    "Use Trainer.quantize from bigdl.nano.pytorch.trainer to calibrate a Pytorch-Lightning model for post-training quantization. Here are some important paramters:\n",
    "```\n",
    ":param pl_model:         A Pytorch-Lightning model to be quantized.\n",
    ":param calib_dataloader:         A torch.utils.data.dataloader.DataLoader object for calibration.     \n",
    "                                 Required for static quantization.\n",
    ":param approach:         'static' or 'dynamic'.\n",
    "                         'static': post_training_static_quant,\n",
    "                         'dynamic': post_training_dynamic_quant.\n",
    "                          Default: 'static'.\n",
    "\n",
    "```\n",
    "Access more details from [Source](https://github.com/intel-analytics/BigDL/blob/main/python/nano/src/bigdl/nano/pytorch/trainer/Trainer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b4292",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i8_model = trainer.quantize(pl_model, calib_dataloader=data_module.test_dataloader())\n",
    "start = time()\n",
    "for x, _ in data_module.test_dataloader():\n",
    "    inference_res_i8 = i8_model(x)\n",
    "i8_inference_time = time() - start\n",
    "outputs = trainer.test(i8_model, datamodule=data_module)\n",
    "i8_acc = outputs[0]['test_acc'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152526c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "|    Precision   | Inference Time(s) | Accuracy(%) |\n",
    "|      FP32      |       {:5.2f}       |    {:5.2f}    |\n",
    "|      INT8      |       {:5.2f}       |    {:5.2f}    |\n",
    "| Improvement(%) |       {:5.2f}       |    {:5.2f}    |\n",
    "\"\"\"\n",
    "summary = template.format(\n",
    "    infer_time, fp32_acc,\n",
    "    i8_inference_time, i8_acc,\n",
    "    (1 - i8_inference_time /infer_time) * 100,\n",
    "    i8_acc - fp32_acc\n",
    ")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}