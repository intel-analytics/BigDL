{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "\n",
    "    .rst-content blockquote {\n",
    "\n",
    "        margin-left: 0px;\n",
    "\n",
    "    }\n",
    "\n",
    "   \n",
    "\n",
    "    blockquote > div {\n",
    "\n",
    "        margin: 1.5625em auto;\n",
    "\n",
    "        padding: 20px 15px 1px;\n",
    "\n",
    "        border-left: 0.2rem solid rgb(59, 136, 219);  \n",
    "\n",
    "        border-radius: 0.2rem;\n",
    "\n",
    "        box-shadow: 0 0.2rem 0.5rem rgb(0 0 0 / 5%), 0 0 0.0625rem rgb(0 0 0 / 10%);\n",
    "\n",
    "    }\n",
    "\n",
    "</style>\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Examples for InferenceOptimizer\n",
    "\n",
    "Today, vison Transformer is becoming more and more popular among computer vision community. On the one hand, people are constantly searching for larger pre-training corpus and pre-training model, on the other hand, how to land the vision transformer in the industrial scene is also a very concerned issue.\n",
    "\n",
    "Here we take several popular vision Transformer architectures as examples to demonstrate how to use InferenceOptimizer in BigDL-Nano to accelerate inference pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  step 0 : Prepare the environment\n",
    "We recommend you to use [Anaconda](https://www.anaconda.com/distribution/#linux) to prepare the environment.\n",
    "\n",
    "**Note**: during your installation, there may be some warnings or errors about version, just ignore them.\n",
    "```bash\n",
    "conda create -n nano python=3.7  # \"nano\" is conda environment name, you can use any name you like.\n",
    "conda activate nano\n",
    "pip install jsonargparse[signatures]\n",
    "pip install --pre --upgrade bigdl-nano[pytorch]\n",
    "\n",
    "# bf16 is available only on torch1.12\n",
    "pip install torch==1.12.0 torchvision --extra-index-url https://download.pytorch.org/whl/cpu \n",
    "# Necessary packages for inference accelaration\n",
    "pip install --upgrade intel-extension-for-pytorch\n",
    "pip install onnx==1.12.0 onnxruntime==1.12.1 onnxruntime-extensions\n",
    "pip install openvino-dev\n",
    "pip install neural-compressor==1.12\n",
    "pip install --upgrade numpy==1.21.6\n",
    "```\n",
    "\n",
    "Initialize environment variables with script `bigdl-nano-init` installed with bigdl-nano, then unset environment variable `KMP_AFFINITY`.\n",
    "\n",
    "```bash\n",
    "source bigdl-nano-init\n",
    "``` \n",
    "\n",
    "You may find environment variables set like follows:\n",
    "\n",
    "```\n",
    "conda dir found: /opt/anaconda3/envs/nano/bin/..\n",
    "OpenMP library found...\n",
    "Setting OMP_NUM_THREADS...\n",
    "Setting OMP_NUM_THREADS specified for pytorch...\n",
    "Setting KMP_AFFINITY...\n",
    "Setting KMP_BLOCKTIME...\n",
    "Setting MALLOC_CONF...\n",
    "Setting LD_PRELOAD...\n",
    "nano_vars.sh already exists\n",
    "+++++ Env Variables +++++\n",
    "LD_PRELOAD=/opt/anaconda3/envs/nano/bin/../lib/libiomp5.so /opt/anaconda3/envs/nano/lib/python3.7/site-packages/bigdl/nano//libs/libtcmalloc.so\n",
    "MALLOC_CONF=\n",
    "OMP_NUM_THREADS=112\n",
    "KMP_AFFINITY=granularity=fine\n",
    "KMP_BLOCKTIME=1\n",
    "TF_ENABLE_ONEDNN_OPTS=1\n",
    "ENABLE_TF_OPTS=1\n",
    "NANO_TF_INTER_OP=1\n",
    "+++++++++++++++++++++++++\n",
    "Complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 : Prepare Dataset\n",
    "\n",
    "As InferenceOptimizer needs validation data to calculate accuracy metric, we need to download [ImageNet validation dataset](https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar) and [development kit](https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz), and place them under directory `./img_data`.\n",
    "\n",
    "Here we provide a helper function `create_imagenet_val_dataset` to help users create a subset of ImageNet validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def create_imagenet_val_dataset(limit_num_samples=None):\n",
    "    dataset = ImageNet(root=\"img_data\", split=\"val\")\n",
    "    if limit_num_samples is not None:\n",
    "        indices = np.random.permutation(len(dataset))[:limit_num_samples]\n",
    "        dataset = Subset(dataset, indices)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2 : Import related package\n",
    "\n",
    "[PyTorch Image Models (timm)](https://github.com/rwightman/pytorch-image-models) provides a collection of image models. Here we use some vision Transformer models with pre-trained weights provided by timm to demonstrate acceleration of InferenceOptimizer in BigDL-Nano. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:04:53.262419: I tensorflow/core/util/util.cc:159] Experimental oneDNN custom operations are on. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-15 11:04:53.266061: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-15 11:04:53.266075: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from bigdl.nano.pytorch import InferenceOptimizer\n",
    "import timm\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3 : Define dataloader, model then optimize\n",
    "> 📝 **Note**\n",
    ">\n",
    "> Actually we highly recommand users pass real training dataloader to `training_data` for calibration of quantization. But as ImageNet training set is too large to download, we just use validation dataset as faked training dataset in blow cases.\n",
    "> \n",
    "> If you want to get real performance on ImageNet validation set, you can just set `limit_num_samples=None`. Here we choose a subset to make inference pipeline faster and we just want to get a rough metric to evaluate the effect of quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MobileViT\n",
    "\n",
    "[MobileViT](https://arxiv.org/abs/2110.02178) is a light-weight, general-purpose, and mobile-friendly vision Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import Interpolation\n",
    "from timm.data.loader import create_loader\n",
    "\n",
    "fake_train_dataset = create_imagenet_val_dataset()\n",
    "val_dataset = create_imagenet_val_dataset(limit_num_samples=320)\n",
    "faked_train_dataloader = create_loader(fake_train_dataset,\n",
    "                                       input_size=256,\n",
    "                                       # in case we want to evaluate single sample latency, so set batch_size to 1\n",
    "                                       batch_size=1,\n",
    "                                       use_prefetcher=False,\n",
    "                                       no_aug=True,\n",
    "                                       crop_pct=0.9,\n",
    "                                       interpolation=\"bicubic\",\n",
    "                                       mean=(0.0, 0.0, 0.0),\n",
    "                                       std=(1.0, 1.0, 1.0),\n",
    "                                       persistent_workers=False)\n",
    "val_dataloader = create_loader(val_dataset,\n",
    "                               input_size=256,\n",
    "                               batch_size=32,\n",
    "                               use_prefetcher=False,\n",
    "                               no_aug=True,\n",
    "                               crop_pct=0.9,\n",
    "                               interpolation=\"bicubic\",\n",
    "                               mean=(0.0, 0.0, 0.0),\n",
    "                               std=(1.0, 1.0, 1.0),\n",
    "                               persistent_workers=False)\n",
    "val_dataloader.dataset.dataset.transform = val_dataloader.dataset.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate latency using 1 thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:05:04,288 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpjdsbic6b\n",
      "2022-09-15 11:05:04,290 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpjdsbic6b/_remote_module_non_scriptable.py\n",
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/intel_extension_for_pytorch/frontend.py:262: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:05:06,763 - bigdl.nano.utils.log4Error - ERROR - \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:05:06,766 - bigdl.nano.utils.log4Error - ERROR - \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:05:06,767 - bigdl.nano.utils.log4Error - ERROR - \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/intel_extension_for_pytorch/frontend.py:262: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n",
      "[W LegacyTypeDispatch.h:74] Warning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Finish test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n",
      "----------Finish test int8 model (5/13)----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/timm/models/mobilevit.py:296: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w\n",
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/timm/models/mobilevit.py:300: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if new_h != H or new_w != W:\n",
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/timm/models/vision_transformer.py:201: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Finish test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nano/lib/python3.7/site-packages/intel_extension_for_pytorch/frontend.py:262: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Finish test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Finish test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp5e_xk2wo/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp5e_xk2wo/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.80 seconds. \n",
      "[ SUCCESS ] Memory consumed: 119 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpk7d76jjh/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpk7d76jjh/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.79 seconds. \n",
      "[ SUCCESS ] Memory consumed: 119 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:07:57 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 11:07:57 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:07:57 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:07:57 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |    25.006    |        0.691         |\n",
      "|           fp32_ipex            |      successful      |    26.143    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |      successful      |   177.674    |        0.697         |\n",
      "|              int8              |      successful      |    25.727    |         0.0          |\n",
      "|            jit_fp32            |      successful      |    19.768    |    not recomputed    |\n",
      "|         jit_fp32_ipex          |      successful      |    19.426    |    not recomputed    |\n",
      "|  jit_fp32_ipex_channels_last   |      successful      |    15.35     |    not recomputed    |\n",
      "|         openvino_fp32          |      successful      |    10.97     |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |    87.527    |        0.684         |\n",
      "|        onnxruntime_fp32        |      successful      |    12.766    |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |    11.444    |         0.0          |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 1.79e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"mobilevit_xxs\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=1)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)\n",
    "# print(acc_model(next(iter(val_dataloader))[0]).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate latency using 8 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n",
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:42:58 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:42:58 [ERROR] \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "[ ERROR ]  \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:42:58 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n",
      "----------Finish test int8 model (5/13)----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n",
      "----------Finish test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n",
      "----------Finish test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Finish test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpdnkunou9/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpdnkunou9/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.80 seconds. \n",
      "[ SUCCESS ] Memory consumed: 119 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpdmwkkfwh/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpdmwkkfwh/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.77 seconds. \n",
      "[ SUCCESS ] Memory consumed: 120 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:45:55 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 11:45:55 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:45:55 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:45:55 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |    30.454    |         0.7          |\n",
      "|           fp32_ipex            |      successful      |    33.782    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |    early stopped     |   796.665    |         None         |\n",
      "|              int8              |      successful      |    33.688    |        0.003         |\n",
      "|            jit_fp32            |      successful      |    25.913    |    not recomputed    |\n",
      "|         jit_fp32_ipex          |      successful      |    25.658    |    not recomputed    |\n",
      "|  jit_fp32_ipex_channels_last   |      successful      |    21.739    |    not recomputed    |\n",
      "|         openvino_fp32          |      successful      |    3.806     |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |    62.545    |        0.681         |\n",
      "|        onnxruntime_fp32        |      successful      |    5.316     |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |     5.62     |         0.0          |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 1.93e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"mobilevit_xxs\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=8)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)\n",
    "# print(acc_model(next(iter(val_dataloader))[0]).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PoolFormer\n",
    "\n",
    "[PoolFormer](https://arxiv.org/abs/2111.11418) verifys that the general architecture of the Transformers, instead of the specific token mixer module, is more essential to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import Interpolation\n",
    "from timm.data.loader import create_loader\n",
    "\n",
    "fake_train_dataset = create_imagenet_val_dataset()\n",
    "val_dataset = create_imagenet_val_dataset(limit_num_samples=320)\n",
    "faked_train_dataloader = create_loader(fake_train_dataset,\n",
    "                               input_size=224,\n",
    "                               batch_size=1,\n",
    "                               use_prefetcher=False,\n",
    "                               no_aug=True,\n",
    "                               crop_pct=0.9,\n",
    "                               interpolation=\"bicubic\",\n",
    "                               mean=(0.485, 0.456, 0.406),\n",
    "                               std=(0.229, 0.224, 0.225),\n",
    "                               persistent_workers=False)\n",
    "val_dataloader = create_loader(val_dataset,\n",
    "                               input_size=224,\n",
    "                               batch_size=32,\n",
    "                               use_prefetcher=False,\n",
    "                               no_aug=True,\n",
    "                               crop_pct=0.9,\n",
    "                               interpolation=\"bicubic\",\n",
    "                               mean=(0.485, 0.456, 0.406),\n",
    "                               std=(0.229, 0.224, 0.225),\n",
    "                               persistent_workers=False)\n",
    "val_dataloader.dataset.dataset.transform = val_dataloader.dataset.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate latency using 1 thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n",
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:28:53 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:28:53 [ERROR] \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "[ ERROR ]  \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:28:53 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:29:12 [ERROR] Unexpected exception AttributeError(\"'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\") happened during tuning.\n",
      "[ ERROR ]  Unexpected exception AttributeError(\"'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\") happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 2658, in quantize\n",
      "    self._get_scale_zeropoint(q_model._model, q_model.q_config)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 3036, in _get_scale_zeropoint\n",
      "    self._get_module_scale_zeropoint(model, tune_cfg)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 2996, in _get_module_scale_zeropoint\n",
      "    tune_cfg['get_attr'][sub_name] = float(getattr(model, node.target))\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1208, in __getattr__\n",
      "    type(self).__name__, name))\n",
      "AttributeError: 'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\n",
      "2022-09-15 11:29:12 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:29:12 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:29:12 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to int8----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpdifinfnk/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpdifinfnk/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.69 seconds. \n",
      "[ SUCCESS ] Memory consumed: 176 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpj7iztjcf/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpj7iztjcf/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.70 seconds. \n",
      "[ SUCCESS ] Memory consumed: 177 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:31:25 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 11:31:25 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:31:25 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:31:25 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |    47.704    |        0.753         |\n",
      "|           fp32_ipex            |      successful      |    47.372    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |    early stopped     |   961.517    |         None         |\n",
      "|              int8              |   fail to convert    |     None     |         None         |\n",
      "|            jit_fp32            |    early stopped     |   442.003    |         None         |\n",
      "|         jit_fp32_ipex          |    early stopped     |   454.269    |         None         |\n",
      "|  jit_fp32_ipex_channels_last   |    early stopped     |   895.506    |         None         |\n",
      "|         openvino_fp32          |      successful      |    23.924    |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |    39.975    |        0.731         |\n",
      "|        onnxruntime_fp32        |      successful      |    33.992    |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |    30.705    |        0.709         |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 1.73e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"poolformer_s12\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=1)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)\n",
    "# print(acc_model(next(iter(val_dataloader))[0]).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate latency using 4 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n",
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:33:42 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:33:42 [ERROR] \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "[ ERROR ]  \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:33:42 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:34:01 [ERROR] Unexpected exception AttributeError(\"'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\") happened during tuning.\n",
      "[ ERROR ]  Unexpected exception AttributeError(\"'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\") happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 2658, in quantize\n",
      "    self._get_scale_zeropoint(q_model._model, q_model.q_config)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 3036, in _get_scale_zeropoint\n",
      "    self._get_module_scale_zeropoint(model, tune_cfg)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/pytorch.py\", line 2996, in _get_module_scale_zeropoint\n",
      "    tune_cfg['get_attr'][sub_name] = float(getattr(model, node.target))\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1208, in __getattr__\n",
      "    type(self).__name__, name))\n",
      "AttributeError: 'GraphModule' object has no attribute 'network.0.0.layer_scale_1'\n",
      "2022-09-15 11:34:01 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:34:01 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:34:01 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to int8----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp4s_q8_ou/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp4s_q8_ou/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.71 seconds. \n",
      "[ SUCCESS ] Memory consumed: 176 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmphdzt5446/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmphdzt5446/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 0.72 seconds. \n",
      "[ SUCCESS ] Memory consumed: 176 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:36:09 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 11:36:09 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:36:09 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:36:09 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |    28.383    |        0.753         |\n",
      "|           fp32_ipex            |      successful      |    22.975    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |    early stopped     |   1030.989   |         None         |\n",
      "|              int8              |   fail to convert    |     None     |         None         |\n",
      "|            jit_fp32            |    early stopped     |   448.226    |         None         |\n",
      "|         jit_fp32_ipex          |    early stopped     |   458.327    |         None         |\n",
      "|  jit_fp32_ipex_channels_last   |    early stopped     |   846.178    |         None         |\n",
      "|         openvino_fp32          |      successful      |    9.039     |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |    47.919    |        0.716         |\n",
      "|        onnxruntime_fp32        |      successful      |    13.654    |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |    17.896    |        0.709         |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 1.7e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"poolformer_s12\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=4)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Swin Transformer\n",
    "\n",
    "[Swin Transformer](https://arxiv.org/abs/2103.14030) proposes hierarchical vision Transformer using shifted windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning**\n",
    ">\n",
    "> Swin don't support dynamic batch, so the batch_size of faked_train_dataloader must be the same with val_dataloader.\n",
    ">\n",
    "> Otherwise the accuracy will be really slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import Interpolation\n",
    "from timm.data.loader import create_loader\n",
    "\n",
    "fake_train_dataset = create_imagenet_val_dataset()\n",
    "val_dataset = create_imagenet_val_dataset(limit_num_samples=20)\n",
    "faked_train_dataloader = create_loader(fake_train_dataset,\n",
    "                               input_size=224,\n",
    "                               batch_size=1,\n",
    "                               use_prefetcher=False,\n",
    "                               no_aug=True,\n",
    "                               crop_pct=0.9,\n",
    "                               interpolation=\"bicubic\",\n",
    "                               mean=(0.485, 0.456, 0.406),\n",
    "                               std=(0.229, 0.224, 0.225),\n",
    "                               persistent_workers=False)\n",
    "val_dataloader = create_loader(val_dataset,\n",
    "                               input_size=224,\n",
    "                               batch_size=1,\n",
    "                               use_prefetcher=False,\n",
    "                               no_aug=True,\n",
    "                               crop_pct=0.9,\n",
    "                               interpolation=\"bicubic\",\n",
    "                               mean=(0.485, 0.456, 0.406),\n",
    "                               std=(0.229, 0.224, 0.225),\n",
    "                               persistent_workers=False)\n",
    "val_dataloader.dataset.dataset.transform = val_dataloader.dataset.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate latency using 1 thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n",
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:15:00 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:15:00 [ERROR] \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "[ ERROR ]  \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:15:00 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n",
      "----------Finish test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n",
      "----------Finish test int8 model (5/13)----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n",
      "----------Finish test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n",
      "----------Finish test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Finish test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmp98qvhu2u/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmp98qvhu2u/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 3.88 seconds. \n",
      "[ SUCCESS ] Memory consumed: 802 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpkgm0enqd/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpkgm0enqd/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 3.90 seconds. \n",
      "[ SUCCESS ] Memory consumed: 801 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:27:11 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 11:27:11 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 11:27:11 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 11:27:11 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |   306.299    |         0.8          |\n",
      "|           fp32_ipex            |      successful      |   312.235    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |      successful      |   584.191    |         0.8          |\n",
      "|              int8              |      successful      |   189.358    |         0.85         |\n",
      "|            jit_fp32            |      successful      |   279.574    |    not recomputed    |\n",
      "|         jit_fp32_ipex          |      successful      |   294.765    |    not recomputed    |\n",
      "|  jit_fp32_ipex_channels_last   |      successful      |   303.548    |    not recomputed    |\n",
      "|         openvino_fp32          |      successful      |   161.356    |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |    218.29    |         0.0          |\n",
      "|        onnxruntime_fp32        |      successful      |   276.916    |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |   137.578    |         0.6          |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 7.41e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=1)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)\n",
    "# print(acc_model(next(iter(val_dataloader))[0]).argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate latency using 8 threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Start Optimization==========================\n",
      "----------Start test original model (1/13)----------\n",
      "----------Finish test original model (1/13)----------\n",
      "----------Start test fp32_ipex model (2/13)----------\n",
      "----------Finish test fp32_ipex model (2/13)----------\n",
      "----------Start test bf16 model (3/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 11:47:16 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Your machine or OS doesn't support BF16 instructions.\n",
      "2022-09-15 11:47:16 [ERROR] \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "[ ERROR ]  \n",
      "\n",
      "**************************How to fix***********************\n",
      "Please check your machine and OS to make sure BF16 support is available.\n",
      "2022-09-15 11:47:16 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Start test bf16_ipex model (4/13)----------\n",
      "----------Finish test bf16_ipex model (4/13)----------\n",
      "----------Start test int8 model (5/13)----------\n",
      "----------Finish test int8 model (5/13)----------\n",
      "----------Start test jit_fp32 model (6/13)----------\n",
      "----------Finish test jit_fp32 model (6/13)----------\n",
      "----------Start test jit_fp32_ipex model (7/13)----------\n",
      "----------Finish test jit_fp32_ipex model (7/13)----------\n",
      "----------Start test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Finish test jit_fp32_ipex_channels_last model (8/13)----------\n",
      "----------Start test openvino_fp32 model (9/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpt9lqlc2p/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpt9lqlc2p/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 3.86 seconds. \n",
      "[ SUCCESS ] Memory consumed: 802 MB. \n",
      "----------Finish test openvino_fp32 model (9/13)----------\n",
      "----------Start test openvino_int8 model (10/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\tnumpy: installed: 1.21.6, required: < 1.20\n",
      "\n",
      "Please install required versions of components or run pip installation\n",
      "pip install openvino-dev\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /tmp/tmpwinj2jyi/tmp.xml\n",
      "[ SUCCESS ] BIN file: /tmp/tmpwinj2jyi/tmp.bin\n",
      "[ SUCCESS ] Total execution time: 3.87 seconds. \n",
      "[ SUCCESS ] Memory consumed: 801 MB. \n",
      "----------Finish test openvino_int8 model (10/13)----------\n",
      "----------Start test onnxruntime_fp32 model (11/13)----------\n",
      "----------Finish test onnxruntime_fp32 model (11/13)----------\n",
      "----------Start test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Finish test onnxruntime_int8_qlinear model (12/13)----------\n",
      "----------Start test onnxruntime_int8_integer model (13/13)----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-15 12:00:14 [ERROR] Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "[ ERROR ]  Unexpected exception IndexError('list index (2) out of range') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/experimental/quantization.py\", line 148, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/strategy/strategy.py\", line 402, in traverse\n",
      "    tune_cfg, self.model, self.calib_dataloader, self.q_func)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/utils/utility.py\", line 262, in fi\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/onnxrt.py\", line 168, in quantize\n",
      "    quantizer.quantize_model()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 133, in quantize_model\n",
      "    self.convert_qdq_to_operator_oriented()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/quantizer.py\", line 240, in convert_qdq_to_operator_oriented\n",
      "    op_converter.convert()\n",
      "  File \"/opt/anaconda3/envs/nano/lib/python3.7/site-packages/neural_compressor/adaptor/ox_utils/operators/conv.py\", line 46, in convert\n",
      "    inputs.append(parents[0].output[2])\n",
      "IndexError: list index (2) out of range\n",
      "2022-09-15 12:00:14 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "[ ERROR ]  Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "2022-09-15 12:00:14 [ERROR] \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Usage Error************************\n",
      "Found no quantized model satisfying accuracy criterion.\n",
      "2022-09-15 12:00:14 [ERROR] \n",
      "\n",
      "****************************Call Stack*************************\n",
      "[ ERROR ]  \n",
      "\n",
      "****************************Call Stack*************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found no quantized model satisfying accuracy criterion.\n",
      "----------Failed to convert to onnxruntime_int8_integer----------\n",
      "\n",
      "\n",
      "==========================Optimization Results==========================\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|             method             |        status        | latency(ms)  |       accuracy       |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "|            original            |      successful      |    91.441    |         0.9          |\n",
      "|           fp32_ipex            |      successful      |    86.477    |    not recomputed    |\n",
      "|              bf16              |   fail to forward    |     None     |         None         |\n",
      "|           bf16_ipex            |      successful      |   858.247    |         0.9          |\n",
      "|              int8              |      successful      |    88.907    |         0.9          |\n",
      "|            jit_fp32            |      successful      |    84.177    |    not recomputed    |\n",
      "|         jit_fp32_ipex          |      successful      |    83.393    |    not recomputed    |\n",
      "|  jit_fp32_ipex_channels_last   |      successful      |    97.543    |    not recomputed    |\n",
      "|         openvino_fp32          |      successful      |    38.192    |    not recomputed    |\n",
      "|         openvino_int8          |      successful      |   203.927    |         0.05         |\n",
      "|        onnxruntime_fp32        |      successful      |    76.117    |    not recomputed    |\n",
      "|    onnxruntime_int8_qlinear    |      successful      |    53.496    |         0.6          |\n",
      "|    onnxruntime_int8_integer    |   fail to convert    |     None     |         None         |\n",
      " -------------------------------- ---------------------- -------------- ----------------------\n",
      "\n",
      "Optimization cost 7.92e+02s at all.\n",
      "===========================Stop Optimization===========================\n",
      "best option is  openvino \n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(\"swin_base_patch4_window7_224\", pretrained=True)\n",
    "\n",
    "optimizer = InferenceOptimizer()\n",
    "optimizer.optimize(model,\n",
    "                   training_data=faked_train_dataloader,\n",
    "                   validation_data=val_dataloader,\n",
    "                   metric=Accuracy(),\n",
    "                   direction=\"max\",\n",
    "                   thread_num=8)\n",
    "acc_model, option = optimizer.get_best_model(accuracy_criterion=0.05)\n",
    "print(\"best option is \", option)\n",
    "# print(acc_model(next(iter(val_dataloader))[0]).argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "75c4387adfc215da0f2d9d02c27ad9a4df553a9f0187eec0365fe565a2e50216"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
