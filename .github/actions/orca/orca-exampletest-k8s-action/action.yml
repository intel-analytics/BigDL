name: 'ORCA-Spark-K8s-Example-Prvn'
description: 'ORCA-Spark-K8s-Example-Prvn'
inputs:
  image:
    description: 'image'
    required: true
    default: 'intelanalytics/bigdl-k8s'
  image-tag:
    description: 'image tag'
    required: true
    default: 'latest'
runs:
  using: "composite"
  steps:
    - uses: actions/checkout@v3
    - name: Set Variable
      shell: bash
      env:
        DEFAULT_IMAGE: ${{ inputs.image }}:${{ inputs.image-tag }}
      run: |
        echo "CONTAINER_NAME=orca-spark-k8s-exmaples-test" >> $GITHUB_ENV
        echo "IMAGE=${{ env.DEFAULT_IMAGE }}" >> $GITHUB_ENV

    - name: Start Container
      shell: bash
      run: |
        set -x
        docker pull $IMAGE
        docker stop $CONTAINER_NAME
        docker rm $CONTAINER_NAME
        docker run -id \
        --net=host \
        --shm-size 15g \
        --name $CONTAINER_NAME \
        -v /etc/kubernetes:/etc/kubernetes \
        -v /root/.kube:/root/.kube \
        -e RUNTIME_SPARK_MASTER=k8s://https://$LOCAL_IP:6443 \
        -e RUNTIME_K8S_SERVICE_ACCOUNT=spark \
        -e RUNTIME_K8S_SPARK_IMAGE=$IMAGE \
        -e RUNTIME_PERSISTENT_VOLUME_CLAIM=nfsvolumeclaim \
        -e RUNTIME_DRIVER_HOST=$LOCAL_IP \
        -e RUNTIME_DRIVER_PORT=54321 \
        -e RUNTIME_EXECUTOR_INSTANCES=4 \
        -e RUNTIME_EXECUTOR_CORES=8 \
        -e RUNTIME_EXECUTOR_MEMORY=10g \
        -e RUNTIME_TOTAL_EXECUTOR_CORES=32 \
        -e RUNTIME_DRIVER_CORES=4 \
        -e RUNTIME_DRIVER_MEMORY=10g \
        -v /disk1/nfsdata/default-nfsvolumeclaim-pvc-d77a2e75-905f-49c1-ad1d-4eaee67c0967:/bigdl2.0/data
        $IMAGE bash
    - name: TF1 Examples Test
      shell: bash
      run: |
        docker exec -i $CONTAINER_NAME bash -c "/opt/spark/bin/spark-submit \
        --master ${RUNTIME_SPARK_MASTER} \
        --deploy-mode client \
        --conf spark.driver.host=${RUNTIME_DRIVER_HOST} \
        --conf spark.driver.port=54321 \
        --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
        --name basic_text_classification \
        --conf spark.kubernetes.container.image=$CONTAINER_NAME \
        --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.nfsvolumeclaim.options.claimName=nfsvolumeclaim \
        --conf spark.kubernetes.driver.volumes.persistentVolumeClaim.nfsvolumeclaim.mount.path=/bigdl2.0/data \
        --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.nfsvolumeclaim.options.claimName=nfsvolumeclaim \
        --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.nfsvolumeclaim.mount.path=/bigdl2.0/data \
        --conf spark.kubernetes.driverEnv.http_proxy=http://child-prc.intel.com:913 \
        --conf spark.kubernetes.driverEnv.https_proxy=http://child-prc.intel.com:913 \
        --conf spark.kubernetes.executorEnv.http_proxy=http://child-prc.intel.com:913 \
        --conf spark.kubernetes.executorEnv.https_proxy=http://child-prc.intel.com:913 \
        --conf spark.kubernetes.container.image.pullPolicy=Always \
        --conf spark.pyspark.driver.python=/usr/local/envs/pytf1/bin/python \
        --conf spark.pyspark.python=/usr/local/envs/pytf1/bin/python \
        --conf spark.executorEnv.PYTHONHOME=/usr/local/envs/pytf1 \
        --conf spark.executor.instances=4 \
        --executor-cores 16 \
        --executor-memory 50g \
        --total-executor-cores 64 \
        --driver-cores 4 \
        --driver-memory 50g \
        --properties-file /opt/bigdl-2.3.0-SNAPSHOT/conf/spark-bigdl.conf \
        --py-files /opt/bigdl-2.3.0-SNAPSHOT/python/bigdl-orca-spark_3.1.3-2.3.0-SNAPSHOT-python-api.zip,/opt/bigdl-2.3.0-SNAPSHOT/examples/orca/learn/tf/basic_text_classification/basic_text_classification.py  \
        --conf spark.driver.extraJavaOptions=-Dderby.stream.error.file=/tmp \
        --conf spark.sql.catalogImplementation='in-memory' \
        --conf spark.driver.extraClassPath=local:///opt/bigdl-2.3.0-SNAPSHOT/jars/* \
        --conf spark.executor.extraClassPath=local:///opt/bigdl-2.3.0-SNAPSHOT/jars/* \
        local:///opt/bigdl-2.3.0-SNAPSHOT/examples/orca/learn/tf/basic_text_classification/basic_text_classification.py \
        --cluster_mode 'spark-submit'"