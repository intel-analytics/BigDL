name: 'Run Dllib Pythontest Spark3.1'
description: 'Run Dllib Pythontest Spark3.1'
runs:
  using: "composite"
  steps:
    - name: Set up Python 
      uses: actions/setup-python@v2
      with:
        python-version: '3.7.10'

    - name: Install dependencies
      shell: bash
      run: |
        apt-get update
        apt-get install -y zip
        python -m pip install --upgrade pip
        python -m pip install --upgrade setuptools==58.0.4

    - name: Setup env
      shell: bash
      run: |
        if [ ! -d "/opt/conda/envs/py37" ]
        then
          conda create -n py37 -y python==3.7.10 setuptools==58.0.4 -c ${CONDA_CHANNEL} --override-channels
        fi
        source activate py37
        pip install wheel
        pip uninstall -y bigdl-friesian bigdl-friesian-spark3 bigdl-dllib bigdl-dllib-spark3 bigdl-orca pyspark bigdl-orca-spark3 bigdl-chronos bigdl-chronos-spark3 bigdl-friesian bigdl-friesian-spark3
        wget https://raw.githubusercontent.com/Romanticoseu/BigDL/patch-1/python/test/python-requirements/requirements-dllib-python.txt
        pip uninstall -r ${{ github.workspace }}/requirements-dllib-python.txt -y
        pip install -i ${PIP_MIRROR} --trusted-host ${TRUSTED_HOST} -r ${{ github.workspace }}/requirements-dllib-python.txt
      env:
        BIGDL_ROOT: ${{ github.workspace }}
        ANALYTICS_ZOO_ROOT: ${{ github.workspace }}

    - name: Run test
      shell: bash
      run: |
        set -x
        set -e
        ulimit -a
        export _JAVA_OPTIONS="-XX:MaxPermSize=3G -Xmx10G"
        export MAVEN_OPTS="-XX:ReservedCodeCacheSize=512m -XX:MaxPermSize=3G -Xmx10G"
        export CLASSPATH=.:${JAVA_HOME}/lib:${JAVA_HOME}/jre/lib:${JAVA_HOME}/lib/tools.jar:${JAVA_HOME}/lib/dt.jar
        export PATH=${JAVA_HOME}/bin/:${JAVA_HOME}/jre/bin:${PATH}
        export PATH=/opt/work/apache-maven-3.6.3/bin:$PATH
        wget https://archive.apache.org/dist/spark/spark-3.1.3/spark-3.1.3-bin-hadoop2.7.tgz -O ${{ github.workspace }}/spark-3.1.3-bin-hadoop2.7.tgz 
        tar -xf spark-3.1.3-bin-hadoop2.7.tgz -C ${{ github.workspace }}/ 
        rm -rf ${{ github.workspace }}/spark-3.1.3-bin-hadoop2.7.tgz 
        export SPARK_HOME=${{ github.workspace }}/spark-3.1.3-bin-hadoop2.7
        cd scala
        ./make-dist.sh -P spark_3.x -Dspark.version=3.1.3
        cd -
        source activate py37
        pip uninstall -y bigdl bigdl-dllib bigdl-orca pyspark 
        export KERAS_BACKEND=tensorflow
        python/dllib/dev/run-tests
        source deactivate

      env:
        BIGDL_ROOT: ${{ github.workspace }}
        BIGDL_HOME: ${{ github.workspace }}
        ANALYTICS_ZOO_ROOT: ${{ github.workspace }}
        ANALYTICS_ZOO_HOME: ${{ github.workspace }}

    - name: Remove spark
      shell: bash
      if: ${{ always() }}
      run: |
        if [ -d "${{ github.workspace }}/spark-3.1.3-bin-hadoop2.7" ]
        then
          rm -rf ${{ github.workspace }}/spark-3.1.3-bin-hadoop2.7
        fi