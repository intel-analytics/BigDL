#!/bin/bash

set -e

# usage
usage() { echo "Usage: $0 [-a|--archives <string#string,...> -d|--deploy-mode <string> -c|--conf <string=string> -h|--help]" 1>&2; exit 1; }

cmd=$@
# get options
set +e
ARGS=`getopt -o a:d:c:h: --long archives:,deploy-mode:,conf:,help -- "$@"`
set -e
eval set -- "$ARGS"

# extract options and their arguments into variables.
declare -A archives_map
declare -A conf_map
dir="environment"
while true ; do
    case "$1" in
        -a|--archives)
            archives=(${2//,/ })
            for archive in $archives
            do
                array=(${archive//#/ })
                archives_map[${array[1]}]=${array[0]}
            done
            shift 2
            ;;
        -d|--deploy-mode)
            deploy_mode=$2
            shift 2
            ;;
        -c|--conf)
            array=(${2//=/ })
            array2=(${array[1]///bin/ })
            conf_map[${array[0]}]=${array2[0]}
            shift 2
            ;;
        -h|--help)
            usage ; break ;;
        --)
              shift
              break
              ;;
    esac
done

if [ $deploy_mode == "client" ]
then
    dir=${conf_map["spark.pyspark.driver.python"]} # todo: deal with corner case.
else
    dir=${conf_map["spark.yarn.appMasterEnv.PYSPARK_PYTHON"]}
fi

archive=${archives_map[$dir]}

set +e
which "conda" >/dev/null 2>&1
if [ $? -eq 0 ]
    # check if environment exists
    if [ ! -d "${dir}" ]
    then
        # extract environment
        echo "extracting ${archive} to ${dir} ..."
        mkdir -p ${dir}
        which "pv" >/dev/null 2>&1
        if [ $? -eq 0 ]
        then
            echo "extracting with progress bar..."
            pv ${archive}|tar -xz -C ${dir}
        else
            echo "extracting without progress bar...please install pv if you need progress bar."
            tar -xzf ${archive} -C ${dir}
        fi
    fi
    # activate environment
    source ${dir}/bin/activate
fi
set -e

# detect paths
export BIGDL_ENV=`python -c """from bigdl.dllib.utils.engine import *
print(' ' + get_bigdl_conf(), end=' ')
bigdl_jars = get_bigdl_jars()
print(','.join(bigdl_jars), end=' ')
"""
`

# setup env
IFS=$' ' array=($(echo $BIGDL_ENV))
len=${#array[@]}
export BIGDL_CONF="${array[$len-2]}"
export BIGDL_JARS="${array[$len-1]}"

# check env
if [ -z ${BIGDL_CONF} ]; then
    echo "Cannot find BigDL configuration file, please check your BigDL installation"
    exit 1
fi

if [ -z $BIGDL_JARS ]; then
    echo "Cannot find BigDL jar files, please check your BigDL installation"
    exit 1
fi

spark-submit \
  --properties-file ${BIGDL_CONF} \
  --jars ${BIGDL_JARS} \
  --conf spark.driver.extraClassPath=${BIGDL_JARS} \
  --conf spark.executor.extraClassPath=${BIGDL_JARS} \
  ${cmd}
