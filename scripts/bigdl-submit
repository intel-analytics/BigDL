#!/bin/bash

set -e

# usage
usage() { echo "Usage: $0 [-a|--archives <string#string,...> -d|--deploy-mode <string> -c|--conf <string=string> -h|--help]" 1>&2; exit 1; }

cmd=$@
# get options
set +e
ARGS=`getopt -o a:d:c:h: --long archives:,deploy-mode:,conf:,help -- "$@"`
set -e
eval set -- "$ARGS"

# extract options and their arguments into variables.
# declare dir -> archive map
declare -A archives_map
# declare spark conf map
declare -A conf_map
# parse PYSPARK_PYTHON for default value of environment directory.
array=(${PYSPARK_PYTHON///bin/ })
dir=${array[0]}
while true ; do
    case "$1" in
        -a|--archives)
            archives=(${2//,/ })
            for archive in $archives
            do
                array=(${archive//#/ })
                archives_map[${array[1]}]=${array[0]}
            done
            shift 2
            ;;
        -d|--deploy-mode)
            deploy_mode=$2
            shift 2
            ;;
        -c|--conf)
            array=(${2//=/ })
            array2=(${array[1]///bin/ })
            conf_map[${array[0]}]=${array2[0]}
            shift 2
            ;;
        -h|--help)
            usage ; break ;;
        --)
              shift
              break
              ;;
    esac
done

flag=0
set +e
# check bigdl.dllib
python -c "from bigdl import dllib" >/dev/null 2>&1
if [ $? -ne 0 ]
then
    echo "no bigdl.dllib in local environment."
    flag=1
fi

# check spark-submit
which "spark-submit" >/dev/null 2>&1
if [ $? -ne 0 ]
then
    echo "no spark-submit in local environment."
    flag=1
fi

set -e
if [ $flag -eq 1 ]
    if [ $deploy_mode == "client" ]
    then
        if [ conf_map["spark.pyspark.driver.python"] ]
        then
            dir=${conf_map["spark.pyspark.driver.python"]}
        else
            if [ conf_map["spark.pyspark.python"] ]
            then
                dir=${conf_map["spark.pyspark.python"]}
            fi
        fi
    else
        if [ $deploy_mode == "cluster" ]
        then
            if [ conf_map["spark.yarn.appMasterEnv.PYSPARK_PYTHON"]]
                dir=${conf_map["spark.yarn.appMasterEnv.PYSPARK_PYTHON"]}
            fi
        fi
    fi
    # check environment directory.
    if [ ! $dir ]
    then
        echo "Please specify python environment directory." 
        echo "You can specify environment variable 'PYSPARK_PYTHON' or specify it in SparkConf."
        exit 1
    fi
    # obtain archive file.
    archive=${archives_map[$dir]}
    # check archive file.
    if [ ! archive ]
    then
        echo "Please specify the archive file for environment."
        exit 1
    fi
    # check if environment exists
    if [ ! -d "${dir}" ]
    then
        # extract environment
        echo "Extracting ${archive} to ${dir} ..."
        mkdir -p ${dir}
        set +e
        which "pv" >/dev/null 2>&1
        if [ $? -eq 0 ]
        then
            echo "Extracting with progress bar..."
            pv ${archive}|tar -xz -C ${dir}
        else
            echo "Extracting without progress bar...please install pv if you need progress bar."
            tar -xzf ${archive} -C ${dir}
        fi
    fi
    set -e
    # activate environment
    source ${dir}/bin/activate
fi

# detect paths
export BIGDL_ENV=`python -c """from bigdl.dllib.utils.engine import *
print(' ' + get_bigdl_conf(), end=' ')
bigdl_jars = get_bigdl_jars()
print(','.join(bigdl_jars), end=' ')
"""
`

# setup env
IFS=$' ' array=($(echo $BIGDL_ENV))
len=${#array[@]}
export BIGDL_CONF="${array[$len-2]}"
export BIGDL_JARS="${array[$len-1]}"

# check env
if [ -z ${BIGDL_CONF} ]; then
    echo "Cannot find BigDL configuration file, please check your BigDL installation"
    exit 1
fi

if [ -z $BIGDL_JARS ]; then
    echo "Cannot find BigDL jar files, please check your BigDL installation"
    exit 1
fi

spark-submit \
  --properties-file ${BIGDL_CONF} \
  --jars ${BIGDL_JARS} \
  --conf spark.driver.extraClassPath=${BIGDL_JARS} \
  --conf spark.executor.extraClassPath=${BIGDL_JARS} \
  ${cmd}
