# Trusted BigDL-LLM using fastchat with Occlum
For running fastchat using bigdl-llm transformers int4 in Occlum

## Prerequisites
1.Check SGX and Kubernetes env.

2.Pull image from dockerhub.
```bash
docker pull intelanalytics/bigdl-ppml-trusted-llm-fastchat-occlum:2.4.0-SNAPSHOT
```

## Deploy fastchat with openAI restful API in K8S cluster

0. prepare model and models_path(host or nfs), change model_name with bigdl.
Refer to [bigdl-llm](https://github.com/intel-analytics/BigDL/tree/main/python/llm) for more  information.
```bash
mv vicuna-7b-hf vicuna-7b-bigdl
```
1. get `controller-service.yaml` and `controller.yaml` and `worker.yaml`, and update the `nodeSelector`.
2. deploy controller-service and controller.
```bash
kubectl apply -f controller-service.yaml
kubectl apply -f controller.yaml
kcbectl get service | grep bigdl
# get controller-service's cluster-ip
```
3. modify `worker.yaml`, set CONTROLLER_HOST=controller-service's cluster-ip, set models mount path and `MODEL_PATH`.
```bash
kubectl apply -f worker.yaml
```
4. using openAI api to access
```
curl http://$controller_ip:8000/v1/models
# choose a model
curl http://$controller_ip:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vicuna-7b-bigdl",
    "prompt": "Once upon a time",
    "max_tokens": 64,
    "temperature": 0.5
  }'
```
More api details refer to [here](https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md)

## Deploy fastchat in Docker
Please refer to [here](https://github.com/intel-analytics/BigDL/tree/main/python/llm/src/bigdl/llm/serving#start-the-service)

To run inside SGX, need to make corresponding changes like k8s deployment.

## Using BigDL Remote Attestation Server
Bigdl ppml use BigDL Remote Attestation Server as reference AS, you can deploy it following the [guide](https://github.com/intel-analytics/BigDL/tree/main/scala/ppml/src/main/scala/com/intel/analytics/bigdl/ppml/attestation#how-to-deploy-a-bigdl-remote-attestation-service)
We assume you have already set up environment and enroll yourself on BigDL Remote Attestation Server.

In [start-spark-local.sh](https://github.com/intel-analytics/BigDL/blob/main/ppml/trusted-big-data-ml/scala/docker-occlum/llm/start-llm-local.sh). Set `ATTESTATION` = true and modify `PCCL_URL`, `ATTESTATION_URL` to the env value you have set,
and modify `APP_ID`, `API_KEY` to the value you have get  when enroll, and then you can add `REPORT_DATA` for attestation.

``` bash
#start-ll-local.sh
-e ATTESTATION=false \   set to true to start attestation.
-e PCCS_URL=https://PCCS_IP:PCCS_PORT \  PCCS URL, obtained from KMS services or a self-deployed one. Should match the format https://<ip_address>:<port>.
-e ATTESTATION_URL=AS_IP:AS_PORT \  URL of attestation service. Should match the format <ip_address>:<port>.
-e APP_ID=your_app_id \ The appId generated by your attestation service.
-e API_KEY=your_api_key \ The apiKey generated by your attestation service.
-e REPORT_DATA=ppml \ A random String to generator a quote which will be send to attestation service and use for attest. Default is ppml.
```

And you can add the same configs in the k8s yaml to attest.