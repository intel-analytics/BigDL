diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
index 8d9f2be621..3ab622cb5f 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala
@@ -667,6 +667,7 @@ private[spark] object SpecialLengths {
   val END_OF_STREAM = -4
   val NULL = -5
   val START_ARROW_STREAM = -6
+  val FINISHED = -7
 }

 private[spark] object BarrierTaskContextMessageProtocol {
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
index df236ba892..a4d0d1c5d8 100644
--- a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala
@@ -17,7 +17,7 @@

 package org.apache.spark.api.python

-import java.io.{DataInputStream, DataOutputStream, EOFException, InputStream}
+import java.io.{BufferedInputStream, BufferedOutputStream, DataInputStream, DataOutputStream, EOFException, InputStream}
 import java.net.{InetAddress, ServerSocket, Socket, SocketException}
 import java.util.Arrays
 import java.util.concurrent.TimeUnit
@@ -145,12 +145,16 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
     }
   }

+  import PythonWorkerFactory.simpleWorkerBuffer
+  import PythonWorkerFactory.maxSimpleWorker
+  import PythonWorkerFactory.keysIterator
   /**
    * Launch a worker by executing worker.py (by default) directly and telling it to connect to us.
    */
   private def createSimpleWorker(): Socket = {
     var serverSocket: ServerSocket = null
     try {
+      /*
       serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))

       // Create and start the worker
@@ -163,29 +167,92 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
       workerEnv.put("PYTHON_WORKER_FACTORY_PORT", serverSocket.getLocalPort.toString)
       workerEnv.put("PYTHON_WORKER_FACTORY_SECRET", authHelper.secret)
       val worker = pb.start()
+       */
+
+      simpleWorkerBuffer.synchronized {
+        logInfo(s"Thread ${Thread.currentThread().getId}: creating simple worker synchronized")
+        val (worker, ss) = {
+          if (simpleWorkerBuffer.size >= maxSimpleWorker) {
+            val p = keysIterator().next()
+            val buffer = simpleWorkerBuffer.get(p).get
+            logInfo(s"scala1: waiting python's finished result.")
+            val input = new DataInputStream(new BufferedInputStream(buffer._1.getInputStream, 1024))
+            val r = input.readInt()
+            logInfo(s"scala1: python result is ${r}.")
+            require(r == SpecialLengths.FINISHED)
+            val ss = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
+            ss.setSoTimeout(10000)
+            logInfo(s"scala1: new data local port ${ss.getLocalPort}--------")
+            val output = new DataOutputStream(
+              new BufferedOutputStream(buffer._1.getOutputStream, 1024))
+            output.writeInt(ss.getLocalPort)
+            output.flush()
+            (buffer._2, ss)
+          } else {
+            val manageServerSocket =
+              new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
+            manageServerSocket.setSoTimeout(100000)
+            val pb = new ProcessBuilder(Arrays.asList(pythonExec, "-m", workerModule))
+            val workerEnv = pb.environment()
+            workerEnv.putAll(envVars.asJava)
+            workerEnv.put("PYTHONPATH", pythonPath)
+            // This is equivalent to setting the -u flag;
+            // we use it because ipython doesn't support -u:
+            workerEnv.put("PYTHONUNBUFFERED", "YES")
+            workerEnv.put("PYTHON_WORKER_FACTORY_PORT", manageServerSocket.getLocalPort.toString)
+            workerEnv.put("PYTHON_WORKER_FACTORY_SECRET", authHelper.secret)
+            val worker = pb.start()
+            redirectStreamsToStderr(worker.getInputStream, worker.getErrorStream)
+            var manageSocket: Socket = null
+            try {
+              manageSocket = manageServerSocket.accept()
+              val port = manageSocket.getLocalPort
+              authHelper.authClient(manageSocket)
+              PythonWorkerFactory.simpleWorkerBuffer.put(port,
+                (manageSocket, worker, manageServerSocket))
+            } catch {
+              case e: Exception =>
+                throw new SparkException("Python worker failed to connect back.", e)
+            }
+            val ss = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))
+            ss.setSoTimeout(10000)
+            logInfo(s"scala2: new data local port ${ss.getLocalPort}--------")
+            val output = new DataOutputStream(
+              new BufferedOutputStream(manageSocket.getOutputStream, 1024))
+            output.writeInt(ss.getLocalPort)
+            output.flush()
+            (worker, ss)
+          }
+        }
+        serverSocket = ss

       // Redirect worker stdout and stderr
-      redirectStreamsToStderr(worker.getInputStream, worker.getErrorStream)
+      // redirectStreamsToStderr(worker.getInputStream, worker.getErrorStream)
+        // Redirect worker stdout and stderr

       // Wait for it to connect to our socket, and validate the auth secret.
-      serverSocket.setSoTimeout(10000)
-
-      try {
-        val socket = serverSocket.accept()
-        authHelper.authClient(socket)
-        self.synchronized {
-          simpleWorkers.put(socket, worker)
+      // serverSocket.setSoTimeout(10000)
+        // Wait for it to connect to our socket, and validate the auth secret.
+        try {
+          val socket = serverSocket.accept()
+          val port = socket.getLocalPort
+          authHelper.authClient(socket)
+          self.synchronized {
+            simpleWorkers.put(socket, worker)
+          }
+          logInfo(s"Thread ${Thread.currentThread().getId}: creating simple worker finished")
+          return socket
+        } catch {
+          case e: Exception =>
+            throw new SparkException("Python worker failed to connect back.", e)
         }
-        return socket
-      } catch {
-        case e: Exception =>
-          throw new SparkException("Python worker failed to connect back.", e)
       }
     } finally {
       if (serverSocket != null) {
         serverSocket.close()
       }
     }
+
     null
   }

@@ -378,7 +445,17 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }
 }

-private object PythonWorkerFactory {
+protected object PythonWorkerFactory {
+  val simpleWorkerBuffer = mutable.HashMap[Int, (Socket, Process, ServerSocket)]()
+  var simpleWorkerIter: Iterator[Int] = null
+  def keysIterator(): Iterator[Int] = {
+    if (simpleWorkerIter == null || !simpleWorkerIter.hasNext) {
+      simpleWorkerIter = simpleWorkerBuffer.keysIterator
+    }
+    simpleWorkerIter
+  }
+  val maxSimpleWorker = 1
+
   val PROCESS_WAIT_TIMEOUT_MS = 10000
   val IDLE_WORKER_TIMEOUT_NS = TimeUnit.MINUTES.toNanos(1)  // kill idle workers after 1 minute
 }
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
index fa86da9ae9..ceb5dfbe6a 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
@@ -648,10 +648,20 @@ private[spark] class SparkSubmit extends Logging {
         confKey = DRIVER_SUPERVISE.key),
       OptionAssigner(args.ivyRepoPath, STANDALONE, CLUSTER, confKey = "spark.jars.ivy"),

+      // SGX related
+      OptionAssigner(args.sgxEnabled.toString, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_ENABLED.key),
+      OptionAssigner(args.sgxMem, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_MEM_SIZE.key),
+      OptionAssigner(args.sgxJvmMem, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_JVM_MEM_SIZE.key),
+
       // An internal option used only for spark-shell to add user jars to repl's classloader,
       // previously it uses "spark.jars" or "spark.yarn.dist.jars" which now may be pointed to
       // remote jars, so adding a new option to only specify local jars for spark-shell internally.
       OptionAssigner(localJars, ALL_CLUSTER_MGRS, CLIENT, confKey = "spark.repl.local.jars")
+
+
     )

     // In client mode, launch the application main class directly
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
index 9da1a73bba..b09ceb6a18 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
@@ -76,6 +76,11 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
   var keytab: String = null
   private var dynamicAllocationEnabled: Boolean = false

+  // sgx
+  var sgxEnabled = false
+  var sgxMem: String = null
+  var sgxJvmMem: String = null
+
   // Standalone cluster mode only
   var supervise: Boolean = false
   var driverCores: String = null
@@ -210,6 +215,12 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     dynamicAllocationEnabled =
       sparkProperties.get(DYN_ALLOCATION_ENABLED.key).exists("true".equalsIgnoreCase)

+    sgxEnabled = sparkProperties.get(config.SGX_ENABLED.key).exists("true".equalsIgnoreCase)
+    sgxMem = Option(sgxMem)
+      .getOrElse(sparkProperties.get(config.SGX_MEM_SIZE.key).orNull)
+    sgxJvmMem = Option(sgxJvmMem)
+      .getOrElse(sparkProperties.get(config.SGX_JVM_MEM_SIZE.key).orNull)
+
     // Global defaults. These should be keep to minimum to avoid confusing behavior.
     master = Option(master).getOrElse("local[*]")

@@ -321,6 +332,9 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     |  packagesExclusions      $packagesExclusions
     |  repositories            $repositories
     |  verbose                 $verbose
+    |  sgxEnabled              $sgxEnabled
+    |  sgxMem                  $sgxMem
+    |  sgxJvmMem               $sgxJvmMem
     |
     |Spark properties used, including those specified through
     | --conf and those from the properties file $propertiesFile:
diff --git a/core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala b/core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala
index 5682a21e95..df70414a70 100644
--- a/core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala
+++ b/core/src/main/scala/org/apache/spark/executor/ProcfsMetricsGetter.scala
@@ -85,6 +85,8 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = "/proc/") extends L
   }

   private def computePageSize(): Long = {
+    return 4096;
+    /*
     if (testing) {
       return 4096;
     }
@@ -99,6 +101,7 @@ private[spark] class ProcfsMetricsGetter(procfsDir: String = "/proc/") extends L
         isAvailable = false
         0
     }
+     */
   }

   // Exposed for testing
diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala
index 3daa9f5362..9758246e94 100644
--- a/core/src/main/scala/org/apache/spark/internal/config/package.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala
@@ -2023,4 +2023,25 @@ package object config {
       .version("3.1.0")
       .doubleConf
       .createWithDefault(5)
+
+  private[spark] val SGX_ENABLED =
+    ConfigBuilder("spark.kubernetes.sgx.enabled")
+      .doc("If set to true, spark executors on kubernetes will run in sgx.")
+      .version("3.1.2")
+      .booleanConf
+      .createWithDefault(false)
+
+  private[spark] val SGX_MEM_SIZE =
+    ConfigBuilder("spark.kubernetes.sgx.mem")
+      .doc("Amount of memory to use for the sgx initialized, in GiB unless otherwise specified.")
+      .version("3.1.2")
+      .bytesConf(ByteUnit.GiB)
+      .createWithDefaultString("16g")
+
+  private[spark] val SGX_JVM_MEM_SIZE =
+    ConfigBuilder("spark.kubernetes.sgx.jvm.mem")
+      .doc("Amount of memory to use for the jvm run in sgx, in GiB unless otherwise specified.")
+      .version("3.1.2")
+      .bytesConf(ByteUnit.GiB)
+      .createWithDefaultString("16g")
 }
diff --git a/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala b/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
index 1ebd8bd89f..df72ed2dbc 100644
--- a/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
+++ b/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
@@ -353,9 +353,12 @@ object ResourceProfile extends Logging {
     }
     val customResourceNames = execReq.map(_.id.resourceName).toSet
     val customResources = ereqs.requests.filter(v => customResourceNames.contains(v._1))
+    val sgxEnabled = conf.get(SGX_ENABLED)
+    val sgxMem = conf.get(SGX_MEM_SIZE)
+    val sgxJvmMem = conf.get(SGX_JVM_MEM_SIZE)
     defaultProfileExecutorResources =
       Some(DefaultProfileExecutorResources(cores, memory, offheapMem, pysparkMem,
-        overheadMem, customResources))
+        overheadMem, customResources, sgxEnabled, sgxMem, sgxJvmMem))
     ereqs.requests
   }

@@ -408,7 +411,10 @@ object ResourceProfile extends Logging {
       pysparkMemoryMiB: Long,
       memoryOverheadMiB: Long,
       totalMemMiB: Long,
-      customResources: Map[String, ExecutorResourceRequest])
+      customResources: Map[String, ExecutorResourceRequest],
+      sgxEnabled: Boolean,
+      sgxMemGiB: Long,
+      sgxJvmMemGiB: Long)

   private[spark] case class DefaultProfileExecutorResources(
       cores: Int,
@@ -416,7 +422,10 @@ object ResourceProfile extends Logging {
       memoryOffHeapMiB: Long,
       pysparkMemoryMiB: Option[Long],
       memoryOverheadMiB: Option[Long],
-      customResources: Map[String, ExecutorResourceRequest])
+      customResources: Map[String, ExecutorResourceRequest],
+      sgxEnabled: Boolean,
+      sgxMemGiB: Long,
+      sgxJvmMemGiB: Long)

   private[spark] def calculateOverHeadMemory(
       overHeadMemFromConf: Option[Long],
@@ -448,6 +457,7 @@ object ResourceProfile extends Logging {
     var memoryOverheadMiB = calculateOverHeadMemory(defaultResources.memoryOverheadMiB,
       executorMemoryMiB, overheadFactor)

+
     val finalCustomResources = if (rpId != DEFAULT_RESOURCE_PROFILE_ID) {
       val customResources = new mutable.HashMap[String, ExecutorResourceRequest]
       execResources.foreach { case (r, execReq) =>
@@ -482,8 +492,12 @@ object ResourceProfile extends Logging {
     }
     val totalMemMiB =
       (executorMemoryMiB + memoryOverheadMiB + memoryOffHeapMiB + pysparkMemToUseMiB)
+    val sgxEnabled = defaultResources.sgxEnabled
+    val sgxMemGiB = defaultResources.sgxMemGiB
+    val sgxJvmMemGiB = defaultResources.sgxJvmMemGiB
     ExecutorResourcesOrDefaults(cores, executorMemoryMiB, memoryOffHeapMiB,
-      pysparkMemToUseMiB, memoryOverheadMiB, totalMemMiB, finalCustomResources)
+      pysparkMemToUseMiB, memoryOverheadMiB, totalMemMiB, finalCustomResources,
+      sgxEnabled, sgxMemGiB, sgxJvmMemGiB)
   }

   private[spark] val PYSPARK_MEMORY_LOCAL_PROPERTY = "resource.pyspark.memory"
diff --git a/core/src/main/scala/org/apache/spark/util/Utils.scala b/core/src/main/scala/org/apache/spark/util/Utils.scala
index 1643aa68cd..ceeaf9f4e4 100644
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@ -560,7 +560,10 @@ private[spark] object Utils extends Logging {
       }
     }
     // Make the file executable - That's necessary for scripts
-    FileUtil.chmod(targetFile.getAbsolutePath, "a+x")
+    // scalastyle:off
+    println("INFO fork chmod is forbidden !!!" + targetFile.getAbsolutePath)
+    // scalastyle:on
+    // FileUtil.chmod(targetFile.getAbsolutePath, "a+x")

     // Windows does not grant read permission by default to non-admin users
     // Add read permission to owner explicitly
diff --git a/python/pyspark/rdd.py b/python/pyspark/rdd.py
index 34faaacff5..897ade1157 100644
--- a/python/pyspark/rdd.py
+++ b/python/pyspark/rdd.py
@@ -163,6 +163,15 @@ def _load_from_socket(sock_info, serializer):
     usually a generator that yields deserialized data
     """
     sockfile = _create_local_socket(sock_info)
+    import warnings
+    warnings.warn("INFO @rdd.py._load_from_socket: sockfile " + str(sockfile), category=None, stacklevel=1, source=None)
+    import time
+    time.sleep(60)
+    warnings.warn("INFO @rdd.py._load_from_socket: WAIT A MIN", category=None, stacklevel=1, source=None)
+    time.sleep(60)
+    warnings.warn("INFO @rdd.py._load_from_socket: WAIT A MIN", category=None, stacklevel=1, source=None)
+    time.sleep(60)
+    warnings.warn("INFO @rdd.py._load_from_socket: WAIT A MIN", category=None, stacklevel=1, source=None)
     # The socket will be automatically closed when garbage-collected.
     return serializer.load_stream(sockfile)

diff --git a/python/pyspark/serializers.py b/python/pyspark/serializers.py
index 1b434d3931..286f773904 100644
--- a/python/pyspark/serializers.py
+++ b/python/pyspark/serializers.py
@@ -78,6 +78,7 @@ class SpecialLengths(object):
     END_OF_STREAM = -4
     NULL = -5
     START_ARROW_STREAM = -6
+    FINISHED = -7


 class Serializer(object):
diff --git a/python/pyspark/shuffle.py b/python/pyspark/shuffle.py
index 4ba8462271..508b320f44 100644
--- a/python/pyspark/shuffle.py
+++ b/python/pyspark/shuffle.py
@@ -38,23 +38,27 @@ try:

     def get_used_memory():
         """ Return the used memory in MiB """
-        global process
-        if process is None or process._pid != os.getpid():
-            process = psutil.Process(os.getpid())
-        if hasattr(process, "memory_info"):
-            info = process.memory_info()
-        else:
-            info = process.get_memory_info()
-        return info.rss >> 20
+        warnings.warn("INFO @shuffle.py.get_used_memory: ignore call psutil.Process, which is not supported on Graphene")
+        return 0
+        #global process
+        #if process is None or process._pid != os.getpid():
+        #    process = psutil.Process(os.getpid())
+        #if hasattr(process, "memory_info"):
+        #    info = process.memory_info()
+        #else:
+        #    info = process.get_memory_info()
+        #return info.rss >> 20

 except ImportError:

     def get_used_memory():
         """ Return the used memory in MiB """
         if platform.system() == 'Linux':
-            for line in open('/proc/self/status'):
-                if line.startswith('VmRSS:'):
-                    return int(line.split()[1]) >> 10
+            warnings.warn("INFO @shuffle.py.get_used_memory: ignore linux check /proc/self/status, which is not supported on Graphene")
+            return 0
+            #for line in open('/proc/self/status'):
+                #if line.startswith('VmRSS:'):
+                    #return int(line.split()[1]) >> 10

         else:
             warnings.warn("Please install psutil to have better "
diff --git a/python/pyspark/worker.py b/python/pyspark/worker.py
index 8ca4bb37e5..277de30d05 100644
--- a/python/pyspark/worker.py
+++ b/python/pyspark/worker.py
@@ -642,15 +642,34 @@ def main(infile, outfile):
     # check end of stream
     if read_int(infile) == SpecialLengths.END_OF_STREAM:
         write_int(SpecialLengths.END_OF_STREAM, outfile)
+        print(str(os.getpid()) + "worker's task exit with 0")
     else:
         # write a different value to tell JVM to not reuse this worker
         write_int(SpecialLengths.END_OF_DATA_SECTION, outfile)
-        sys.exit(-1)
+        #sys.exit(-1)
+        print(str(os.getpid()) + "worker's task exit with -1")
+    outfile.flush()


 if __name__ == '__main__':
     # Read information about how to connect back to the JVM from the environment.
     java_port = int(os.environ["PYTHON_WORKER_FACTORY_PORT"])
     auth_secret = os.environ["PYTHON_WORKER_FACTORY_SECRET"]
-    (sock_file, _) = local_connect_and_auth(java_port, auth_secret)
-    main(sock_file, sock_file)
+    #(sock_file, _) = local_connect_and_auth(java_port, auth_secret)
+    #main(sock_file, sock_file)
+    print("Python worker's pid is " + str(os.getpid()) + "!!!!!!!!!!!!!")
+    (sock_file, sock) = local_connect_and_auth(java_port, auth_secret)
+    sock.settimeout(None)
+    print("sock set timeout successed!")
+    print("Python worker connected to " + str(java_port) + "!!!!!!!!!!!!!")
+    while True:
+        try:
+            data_port = read_int(sock_file)
+            print("Python new data port is " + str(data_port) + "!!!!!!!!!!!!!")
+            (data_sock_file, data_sock) = local_connect_and_auth(data_port, auth_secret)
+            main(data_sock_file, data_sock_file)
+            data_sock.close()
+            write_int(SpecialLengths.FINISHED, sock_file)
+            sock_file.flush()
+        except Exception as e:
+            print("Python waiting for connect. \n" + str(e))
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
index 543ca12594..9dfb3697f3 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
@@ -65,6 +65,9 @@ private[spark] object Constants {
   val ENV_SPARK_CONF_DIR = "SPARK_CONF_DIR"
   val ENV_SPARK_USER = "SPARK_USER"
   val ENV_RESOURCE_PROFILE_ID = "SPARK_RESOURCE_PROFILE_ID"
+  val ENV_SGX_ENABLED = "SGX_ENABLED"
+  val ENV_SGX_MEM_SIZE = "SGX_MEM_SIZE"
+  val ENV_SGX_JVM_MEM_SIZE = "SGX_JVM_MEM_SIZE"
   // Spark app configs for containers
   val SPARK_CONF_VOLUME_DRIVER = "spark-conf-volume-driver"
   val SPARK_CONF_VOLUME_EXEC = "spark-conf-volume-exec"
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
index a0a17cecf9..6040a421c1 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
@@ -121,7 +121,10 @@ private[spark] class BasicExecutorFeatureStep(
           // This is to set the SPARK_CONF_DIR to be /opt/spark/conf
           (ENV_SPARK_CONF_DIR, SPARK_CONF_DIR_INTERNAL),
           (ENV_EXECUTOR_ID, kubernetesConf.executorId),
-          (ENV_RESOURCE_PROFILE_ID, resourceProfile.id.toString)
+          (ENV_RESOURCE_PROFILE_ID, resourceProfile.id.toString),
+          (ENV_SGX_ENABLED, execResources.sgxEnabled.toString),
+          (ENV_SGX_MEM_SIZE, execResources.sgxMemGiB + "G"),
+          (ENV_SGX_JVM_MEM_SIZE, execResources.sgxJvmMemGiB + "G")
         ) ++ kubernetesConf.environment).map { case (k, v) =>
           new EnvVarBuilder()
             .withName(k)
