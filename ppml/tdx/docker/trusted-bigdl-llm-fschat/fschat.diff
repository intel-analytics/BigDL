325a326,348
> class BigDLLLMAdapter(BaseModelAdapter):
>     "Model adapater for bigdl-llm backend models"
> 
>     def match(self, model_path: str):
>         return "bigdl" in model_path
> 
>     def load_model(self, model_path: str, from_pretrained_kwargs: dict):
>         revision = from_pretrained_kwargs.get("revision", "main")
>         from transformers import LlamaTokenizer
>         tokenizer = LlamaTokenizer.from_pretrained(
>             model_path, use_fast=False, revision=revision
>         )
>         from bigdl.llm.transformers import AutoModelForCausalLM
>         model = AutoModelForCausalLM.from_pretrained(
>             model_path,
>             load_in_4bit=True,
>             low_cpu_mem_usage=True,
>             **from_pretrained_kwargs,
>         )
>         #self.raise_warning_for_old_weights(model)
>         return model, tokenizer
> 
> 
504a528
>         from bigdl.llm.transformers import AutoModel
506c530
<             model_path, trust_remote_code=True, **from_pretrained_kwargs
---
>             model_path, trust_remote_code=True, load_in_4bit=True, **from_pretrained_kwargs
985a1010
> register_model_adapter(BigDLLLMAdapter)
