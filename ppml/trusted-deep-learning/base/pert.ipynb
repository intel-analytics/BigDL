{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ad339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "from torch import nn\n",
    "from contextlib import nullcontext\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "#from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6977a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "RANK = int(os.environ.get(\"RANK\", 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"PyTorch PERT Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa555c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--batch-size\", type=int, default=16, metavar=\"N\",\n",
    "                    help=\"input batch size for training (default: 16)\")\n",
    "parser.add_argument(\"--test-batch-size\", type=int, default=1000, metavar=\"N\",\n",
    "                    help=\"input batch size for testing (default: 1000)\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=1, metavar=\"N\",\n",
    "                    help=\"number of epochs to train (default: 10)\")\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5, metavar=\"LR\",\n",
    "                    help=\"learning rate (default: 0.01)\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\",\n",
    "                    help=\"random seed (default: 1)\")\n",
    "parser.add_argument(\"--dataset\", type=int, default=1, metavar=\"D\",\n",
    "                    help=\"dataset size (default 1 * 9600)\")\n",
    "parser.add_argument(\"--save-model\", action=\"store_true\", default=False,\n",
    "                    help=\"For Saving the current Model\")\n",
    "parser.add_argument(\"--local-only\", action=\"store_true\", default=False,\n",
    "                    help=\"If set to true, then load model from disk\")\n",
    "parser.add_argument(\"--model-path\", type=str, default=\"/ppml/model\",\n",
    "                    help=\"Where to load model\")\n",
    "# Only for test purpose\n",
    "parser.add_argument(\"--load-model\", action=\"store_true\", default=False,\n",
    "                    help=\"For loading the current model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--mini-batch\", type=int, default=0, metavar=\"M\",\n",
    "                    help=\"If set, the PyTorch will conduct M local-batch computation before doing a all_reduce sync\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0d018",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--log-interval\", type=int, default=2, metavar=\"N\",\n",
    "                    help=\"how many batches to wait before logging training status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument(\"--log-path\", type=str, default=\"\",\n",
    "                    help=\"Path to save logs. Print to StdOut if log-path is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c16bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f589d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937957e7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Define dataset, so it is easier to load different split in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdfa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # data_type is actually split, so that we can define dataset for train set/validate set\n",
    "    def __init__(self, data_type, dataset_load):\n",
    "        self.dataset_load = dataset_load\n",
    "        self.data = self.load_data(data_type)\n",
    "\n",
    "    def load_data(self, data_type):\n",
    "        tmp_dataset = load_dataset(path='seamew/ChnSentiCorp', split=data_type)\n",
    "        Data = {}\n",
    "        # So enumerate will return a index, and  the line?\n",
    "        # line is a dict, including 'text', 'label'\n",
    "        if data_type == 'train':\n",
    "            for i in range(self.dataset_load):\n",
    "                for idx, line in enumerate(tmp_dataset):\n",
    "                    sample = line\n",
    "                    Data[idx + i * len(tmp_dataset)] = sample\n",
    "        else:\n",
    "            for idx, line in enumerate(tmp_dataset):\n",
    "                sample = line\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea882648",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.local_only:\n",
    "    checkpoint = args.model_path\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        checkpoint, model_max_length=512, local_files_only=True)\n",
    "else:\n",
    "    checkpoint = 'hfl/chinese-pert-base'\n",
    "    tokenizer = BertTokenizer.from_pretrained(checkpoint, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a batch of data, which is used for training\n",
    "def collate_fn(batch_samples):\n",
    "    batch_text = []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_text.append(sample['text'])\n",
    "        batch_label.append(int(sample['label']))\n",
    "    # The tokenizer will make the data to be a good format for our model to understand\n",
    "    X = tokenizer(\n",
    "        batch_text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a59214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        if args.local_only:\n",
    "            self.bert_encoder = BertModel.from_pretrained(\n",
    "                checkpoint, local_files_only=True)\n",
    "        else:\n",
    "            self.bert_encoder = BertModel.from_pretrained(checkpoint)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cf802",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(args, dataloader, model, loss_fn, optimizer, epoch, total_loss):\n",
    "    # Set to train mode\n",
    "    model.train()\n",
    "    total_dataset = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    enumerator = enumerate(dataloader, start=1)\n",
    "    for batch, (X, y) in enumerator:\n",
    "        my_context = model.no_sync if WORLD_SIZE > 1 and args.mini_batch > 0 and batch % args.mini_batch != 0 else nullcontext\n",
    "        with my_context():\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Forward pass\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "        if args.mini_batch == 0 or batch % args.mini_batch == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_dataset += args.batch_size\n",
    "        if batch % args.log_interval == 0:\n",
    "            msg = \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                epoch, batch, len(dataloader),\n",
    "                100. * batch / len(dataloader), loss.item())\n",
    "            logging.info(msg)\n",
    "\n",
    "    return total_loss, total_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5196705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, mode='Test'):\n",
    "    assert mode in ['Valid', 'Test']\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #correct /= size\n",
    "    #correct *= WORLD_SIZE\n",
    "    correct = correct / (size / WORLD_SIZE)\n",
    "    print(f\"{mode} Accuracy: {(100*correct):>0.1f}%\\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2505e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # parse args\n",
    "    args = _parse_args(myargs)\n",
    "    # get train and valid data\n",
    "    train_data = Dataset('train', args.dataset)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "    # get dataloader\n",
    "    train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data, batch_size=args.test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    # set model\n",
    "    model = NeuralNetwork().to(device)\n",
    "    # train model\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "    for t in range(args.epochs):\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            args, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        valid_acc = test_loop(valid_dataloader, model, mode='Valid')\n",
    "    # model save\n",
    "    torch.save(model.state_dict(), \"pert.bin\")\n",
    "    \n",
    "    if args.log_path == \"\":\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.DEBUG)\n",
    "    else:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "            level=logging.DEBUG,\n",
    "            filename=args.log_path)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if should_distribute():\n",
    "        print(\"Using distributed PyTorch with {} backend\".format(\n",
    "            \"GLOO\"), flush=True)\n",
    "        dist.init_process_group(backend=dist.Backend.GLOO)\n",
    "\n",
    "    # Load the data and dataset\n",
    "    print(\"[INFO]Before data get loaded\", flush=True)\n",
    "    train_data = Dataset('train', args.dataset)\n",
    "    print(\"######train data length:\", len(train_data.data), flush=True)\n",
    "    valid_data = Dataset('validation', 1)\n",
    "\n",
    "    if is_distributed():\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        valid_sampler = DistributedSampler(\n",
    "            valid_data, num_replicas=WORLD_SIZE, rank=RANK, shuffle=True, drop_last=False, seed=args.seed)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, collate_fn=collate_fn, sampler=train_sampler)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, collate_fn=collate_fn, sampler=valid_sampler)\n",
    "    else:\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        valid_dataloader = DataLoader(\n",
    "            valid_data, batch_size=args.test_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    print(\"[INFO]Data get loaded successfully\", flush=True)\n",
    "\n",
    "    model = NeuralNetwork().to(device)\n",
    "    if (args.load_model):\n",
    "        model.load_state_dict(torch.load('./pert.bin'))\n",
    "\n",
    "    if is_distributed():\n",
    "        Distributor = nn.parallel.DistributedDataParallel\n",
    "        model = Distributor(model, find_unused_parameters=True)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "\n",
    "    total_loss = 0.\n",
    "    best_acc = 0.\n",
    "    total_time = 0.\n",
    "    total_throughput = 0.\n",
    "\n",
    "    for t in range(args.epochs):\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1}\\n-------------------------------\")\n",
    "        if is_distributed():\n",
    "            train_dataloader.sampler.set_epoch(t)\n",
    "            valid_dataloader.sampler.set_epoch(t)\n",
    "        start = time.perf_counter()\n",
    "        total_loss, total_dataset = train_loop(\n",
    "            args, train_dataloader, model, loss_fn, optimizer, t+1, total_loss)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Elapsed time:\",\n",
    "              end - start, flush=True)\n",
    "        print(f\"Epoch {t+1}/{args.epochs + 1} Processed dataset length:\",\n",
    "              total_dataset, flush=True)\n",
    "        msg = \"Epoch {}/{} Throughput: {: .4f}\".format(\n",
    "            t+1, args.epochs+1, 1.0 * total_dataset / (end-start))\n",
    "        total_time += (end - start)\n",
    "        total_throughput += total_dataset\n",
    "        print(msg, flush=True)\n",
    "        valid_acc = test_loop(valid_dataloader, model, mode='Valid')\n",
    "\n",
    "    print(\"[INFO]Finish all test\", flush=True)\n",
    "    msg = \"[INFO]Average training time per epoch: {: .4f}\".format(total_time / args.epochs)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    msg = \"[INFO]Average throughput per epoch: {: .4f}\".format(total_throughput / total_time)\n",
    "    print(msg, flush=True)\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(), \"pert.bin\")\n",
    "    if is_distributed():\n",
    "        dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
