<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Forecasters &mdash; BigDL  documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/clipboard.min.js"></script>
        <script src="../../../_static/copybutton.js"></script>
        <script src="../../../_static/tabs.js"></script>
        <script src="../../../_static/design-tabs.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Anomaly Detectors" href="anomaly_detectors.html" />
    <link rel="prev" title="Auto Models" href="automodels.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> BigDL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-tf-quickstart.html">TensorFlow 1.15 Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-keras-quickstart.html">Keras 2.3 Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-tf2keras-quickstart.html">TensorFlow 2 Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-pytorch-quickstart.html">PyTorch Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Ray/QuickStart/ray-quickstart.html">RayOnSpark Quickstart</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/python.html">Python User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/scala.html">Scala User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/colab.html">Colab User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/docker.html">Docker User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/hadoop.html">Hadoop/YARN User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/k8s.html">K8s User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/databricks.html">Databricks User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/develop.html">Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UserGuide/known_issues.html">BigDL Known Issues</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Nano</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/Overview/nano.html">Nano User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/Overview/windows_guide.html">Windows User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/pytorch_train.html">BigDL-Nano PyTorch Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/pytorch_inference.html">BigDL-Nano PyTorch Inference Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/tensorflow_train.html">BigDL-Nano TensorFlow Training Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/tensorflow_inference.html">BigDL-Nano TensorFlow Inference Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/hpo.html">AutoML Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/Overview/known_issues.html">Nano Known Issues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Nano/QuickStart/index.html">Nano Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DLlib</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../DLlib/Overview/dllib.html">DLlib User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../DLlib/Overview/keras-api.html">Keras-Like API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../DLlib/Overview/nnframes.html">Spark ML Pipeline Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Orca</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/orca.html">Orca User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/orca-context.html">Orca Context</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/data-parallel-processing.html">Distributed Data Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/distributed-training-inference.html">Distributed Training and Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/distributed-tuning.html">Distributed Hyper-Parameter Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Ray/Overview/ray.html">RayOnSpark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/Overview/known_issues.html">Orca Known Issues</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Chronos</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Chronos/Overview/chronos.html">Chronos User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Chronos/Overview/windows_guide.html">Windows User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Chronos/Overview/deep_dive.html">Chronos Deep Dive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Chronos/QuickStart/index.html">Chronos Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Chronos/Overview/chronos_known_issue.html">Chronos Known Issue</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PPML</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/Overview/ppml.html">Privacy Preserving Machine Learning (PPML) User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/Overview/trusted_big_data_analytics_and_ml.html">Trusted Big Data Analytics and ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/Overview/trusted_fl.html">Trusted FL (Federated Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/secure_your_services.html">Secure Your Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/build_kernel_with_sgx.html">Building Linux Kernel from Source with SGX Enabled</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/deploy_intel_sgx_device_plugin_for_kubernetes.html">Deploy the Intel SGX Device Plugin for Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/trusted-serving-on-k8s-guide.html">Trusted Cluster Serving with Graphene on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/tpc-h_with_sparksql_on_k8s.html">TPC-H with Trusted SparkSQL on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/QuickStart/tpc-ds_with_sparksql_on_k8s.html">TPC-DS with Trusted SparkSQL on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../PPML/Overview/azure_ppml.html">Privacy Preserving Machine Learning (PPML) on Azure User Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Serving</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/Overview/serving.html">Cluster Serving User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/QuickStart/serving-quickstart.html">Cluster Serving Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/ProgrammingGuide/serving-installation.html">Install Cluster Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/ProgrammingGuide/serving-start.html">Start Cluster Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/ProgrammingGuide/serving-inference.html">Inference by Cluster Serving</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/Example/example.html">Cluster Serving Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/FAQ/faq.html">Cluster Serving FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Serving/FAQ/contribute-guide.html">Contribute to Cluster Serving</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Common Use Case</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-pytorch-distributed-quickstart.html">Use <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> in Orca</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UseCase/spark-dataframe.html">Use Spark Dataframe for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../UseCase/xshards-pandas.html">Use Distributed Pandas for Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-autoestimator-pytorch-quickstart.html">Enable AutoML for PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Orca/QuickStart/orca-autoxgboost-quickstart.html">Use AutoXGBoost to auto-tune XGBoost parameters</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Orca/orca.html">Orca API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Friesian/feature.html">Friesian Feature API</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Chronos API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="autotsestimator.html">AutoTS</a></li>
<li class="toctree-l2"><a class="reference internal" href="automodels.html">Auto Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Forecasters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lstmforecaster">LSTMForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#seq2seqforecaster">Seq2SeqForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tcnforecaster">TCNForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#autoformerforecaster">AutoformerForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#nbeatsforecaster">NBeatsForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tcmfforecaster">TCMFForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mtnetforecaster">MTNetForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#arimaforecaster">ARIMAForecaster</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prophetforecaster">ProphetForecaster</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="anomaly_detectors.html">Anomaly Detectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="tsdataset.html">TSDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="simulator.html">Simulator</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluator.html">Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="autots.html">AutoTS (deprecated)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Nano/index.html">Nano API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Real-World Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Application/presentations.html">Presentations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Application/blogs.html">Blogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Application/powered-by.html">Powered By</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">BigDL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="index.html">Chronos API</a> &raquo;</li>
      <li>Forecasters</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/doc/PythonAPI/Chronos/forecasters.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="forecasters">
<h1>Forecasters<a class="headerlink" href="#forecasters" title="Permalink to this headline">¶</a></h1>
<section id="lstmforecaster">
<h2>LSTMForecaster<a class="headerlink" href="#lstmforecaster" title="Permalink to this headline">¶</a></h2>
<p>Long short-term memory(LSTM) is a special type of recurrent neural network(RNN). We implement the basic version of LSTM - VanillaLSTM for this forecaster for time-series forecasting task. It has two LSTM layers, two dropout layer and a dense layer.</p>
<p>For the detailed algorithm description, please refer to <a class="reference external" href="https://github.com/intel-analytics/BigDL/blob/main/docs/docs/Chronos/Algorithm/LSTMAlgorithm.md">here</a>.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">PyTorch</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Tensorflow</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.lstm_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.lstm_forecaster.</span></span><span class="sig-name descname"><span class="pre">LSTMForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/lstm_forecaster.html#LSTMForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.base_forecaster.BasePytorchForecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1. Initialize Forecaster directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">LSTMForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                                input_feature_num=2,</span>
<span class="go">                                output_feature_num=2,</span>
<span class="go">                                ...)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2. Initialize Forecaster from from_tsdataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">LSTMForecaster</span><span class="o">.</span><span class="n">from_tsdataset</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">to_local</span><span class="p">()</span>  <span class="c1"># if you set distributed=True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a LSTM Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>hidden_dim</strong> – int or list, Specify the hidden dim of each lstm layer.
The value defaults to 32.</p></li>
<li><p><strong>layer_num</strong> – Specify the number of lstm layer to be used. The value
defaults to 1.</p></li>
<li><p><strong>dropout</strong> – int or list, Specify the dropout close possibility
(i.e. the close possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – str or pytorch loss instance, Specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae”, “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.build_onnx">
<span class="sig-name descname"><span class="pre">build_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">thread_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.build_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Build onnx model to speed up inference and reduce latency.
The method is Not required to call before predict_with_onnx,
evaluate_with_onnx or export_onnx_file.
It is recommended to use when you want to:</p>
<div class="line-block">
<div class="line">1. Strictly control the thread to be used during inferencing.</div>
<div class="line">2. Alleviate the cold start problem when you call predict_with_onnx
for the first time.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>thread_num</strong> – int, the num of thread limit. The value is set to None by
default where no limit is set.</p></li>
<li><p><strong>sess_options</strong> – an onnxruntime.SessionOptions instance, if you set this
other than None, a new onnxruntime session will be built on this setting
and ignore other settings you assigned(e.g. thread_num…).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># to pre build onnx sess</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">build_onnx</span><span class="p">(</span><span class="n">thread_num</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># build onnx runtime sess for single thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ------------------------------------------------------</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># directly call onnx related method is also supported</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.build_openvino">
<span class="sig-name descname"><span class="pre">build_openvino</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.build_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Build openvino model to speed up inference and reduce latency.
The method is Not required to call before predict_with_openvino.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<p>if you want to evaluate on a single node(which is common practice), please call
.to_local().evaluate(data, …)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.evaluate_with_onnx">
<span class="sig-name descname"><span class="pre">evaluate_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.evaluate_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.export_onnx_file">
<span class="sig-name descname"><span class="pre">export_onnx_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model.onnx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantized_dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'qmodel.onnx'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.export_onnx_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the onnx model file to the disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dirname</strong> – The dir location you want to save the onnx file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'output'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">earlystop_patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line"><br /></div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>validation_data</strong> – <p>Validation sample for validation loop. Defaults to ‘None’.
If you do not input data for ‘validation_data’, the validation_step will be skipped.
The validation_data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 1.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.
If you input a pytorch dataloader for <cite>data</cite>, the batch_size will follow the
batch_size setted in <cite>data</cite>.if the forecaster is distributed, the batch_size will be
evenly distributed to all workers.</p></li>
<li><p><strong>validation_mode</strong> – <p>A str represent the operation mode while having ‘validation_data’.
Defaults to ‘output’. The validation_mode includes the following types:</p>
<div class="line-block">
<div class="line">1. output:</div>
<div class="line">If you choose ‘output’ for validation_mode, it will return a dict that records the</div>
<div class="line">average validation loss of each epoch.</div>
<div class="line"><br /></div>
<div class="line">2. earlystop:</div>
<div class="line">Monitor the val_loss and stop training when it stops improving.</div>
</div>
</p></li>
<li><p><strong>earlystop_patience</strong> – Number of checks with no improvement after which training will
be stopped. It takes effect when ‘validation_mode’ is ‘earlystop’. Under the default
configuration, one check happens after every training epoch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Validation loss if ‘validation_data’ is not None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.get_model">
<span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.get_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learned PyTorch model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a pytorch model instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>restore the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The checkpoint file location you want to
load the quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<p>if you want to predict on a single node(which is common practice), please call
.to_local().predict(x, …)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x}, where x’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim)
if data is a numpy ndarray or a dataloader.
A xshard item with format {‘prediction’: result},
where result is a numpy array with shape (num_samples, horizon, target_dim)
if data is a xshard item.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict_with_onnx">
<span class="sig-name descname"><span class="pre">predict_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict_with_openvino">
<span class="sig-name descname"><span class="pre">predict_with_openvino</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.predict_with_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with openvino. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_openvino is valid and Forecaster will
automatically build an openvino session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">calib_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">framework</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pytorch_fx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approach</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relative_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>calib_data</strong> – A torch.utils.data.dataloader.DataLoader object for calibration.
Required for static quantization.</p></li>
<li><p><strong>val_data</strong> – A torch.utils.data.dataloader.DataLoader object for evaluation.</p></li>
<li><p><strong>metric</strong> – A str represent the metrics for tunning the quality of
quantization. You may choose from “mse”, “mae”, “rmse”, “r2”, “mape”, “smape”.</p></li>
<li><p><strong>conf</strong> – A path to conf yaml file for quantization. Default to None,
using default config.</p></li>
<li><p><strong>framework</strong> – string or list, [{‘pytorch’|’pytorch_fx’|’pytorch_ipex’},
{‘onnxrt_integerops’|’onnxrt_qlinearops’}]. Default: ‘pytorch_fx’.
Consistent with Intel Neural Compressor.</p></li>
<li><p><strong>approach</strong> – str, ‘static’ or ‘dynamic’. Default to ‘static’.</p></li>
<li><p><strong>tuning_strategy</strong> – str, ‘bayesian’, ‘basic’, ‘mse’ or ‘sigopt’. Default to ‘bayesian’.</p></li>
<li><p><strong>relative_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 0.1 means that we accept a 10% increase in the metrics error.</p></li>
<li><p><strong>absolute_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 5 means that we can only accept metrics smaller than 5.</p></li>
<li><p><strong>timeout</strong> – Tuning timeout (seconds). Default to 0, which means early stop.
Combine with max_trials field to decide when to exit.</p></li>
<li><p><strong>max_trials</strong> – Max tune times. Default to 1. Combine with timeout field to
decide when to exit. “timeout=0, max_trials=1” means it will try quantization
only once and return satisfying best model.</p></li>
<li><p><strong>sess_options</strong> – The session option for onnxruntime, only valid when
framework contains ‘onnxrt_integerops’ or ‘onnxrt_qlinearops’,
otherwise will be ignored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<p>Please note that if you only want the pytorch model or onnx model
file, you can call .get_model() or .export_onnx_file(). The checkpoint
file generated by .save() method can only be used by .load().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The location you want to save the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The location you want to save quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.to_local">
<span class="sig-name descname"><span class="pre">to_local</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.to_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a distributed forecaster to a local (non-distributed) one.</p>
<p>Common practice is to use distributed training (fit) and predict/
evaluate with onnx or other frameworks on a single node. To do so,
you need to call .to_local() and transform the forecaster to a non-
distributed one.</p>
<p>The optimizer is refreshed, incremental training after to_local
might have some problem.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a forecaster instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Search the hyper parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – train data, as numpy ndarray tuple (x, y)</p></li>
<li><p><strong>validation_data</strong> – validation data, as numpy ndarray tuple (x,y)</p></li>
<li><p><strong>target_metric</strong> – the target metric to optimize,
a string or an instance of torchmetrics.metric.Metric</p></li>
<li><p><strong>direction</strong> – in which direction to optimize the target metric,
“maximize” - larger the better
“minimize” - smaller the better</p></li>
<li><p><strong>n_trials</strong> – number of trials to run</p></li>
<li><p><strong>n_parallels</strong> – number of parallel processes used to run trials.
to use parallel tuning you need to use a RDB url for storage and specify study_name.
For more information, refer to Nano AutoML user guide.</p></li>
<li><p><strong>epochs</strong> – the number of epochs to run in each trial fit, defaults to 1</p></li>
<li><p><strong>batch_size</strong> – number of batch size for each trial fit, defaults to 32</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.from_tsdataset">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_tsdataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tsdataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/lstm_forecaster.html#LSTMForecaster.from_tsdataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.lstm_forecaster.LSTMForecaster.from_tsdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a LSTM Forecaster Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tsdataset</strong> – A bigdl.chronos.data.tsdataset.TSDataset instance.</p></li>
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).
Do not specify the ‘past_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.</p></li>
<li><p><strong>kwargs</strong> – Specify parameters of Forecaster,
e.g. loss and optimizer, etc. More info, please refer to
LSTMForecaster.__init__ methods.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A LSTM Forecaster Model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tf.lstm_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tf.lstm_forecaster.</span></span><span class="sig-name descname"><span class="pre">LSTMForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/lstm_forecaster.html#LSTMForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.tf.base_forecaster.BaseTF2Forecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">LSTMForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                                input_feature_num=2,</span>
<span class="go">                                output_feature_num=2,</span>
<span class="go">                                ...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_dir_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_dir_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a LSTM Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>hidden_dim</strong> – int or list, Specify the hidden dim of each lstm layer.
The value defaults to 32.</p></li>
<li><p><strong>layer_num</strong> – Specify the number of lstm layer to be used. The value
defaults to 1.</p></li>
<li><p><strong>dropout</strong> – int or list, Specify the dropout close possibility
(i.e. the close possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – Str or a tf.keras.losses.Loss instance, specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae” and “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a Keras model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.chronos.metric.forecaster_metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p>
</dd>
<dt class="field-odd">Params multioutput_value</dt>
<dd class="field-odd"><p>Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a tf.data.Dataset:</div>
<div class="line">A TFDataset instance which contains x and y with same shape as the tuple.</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim),</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim).</div>
</div>
</p>
</dd>
<dt class="field-even">Params epochs</dt>
<dd class="field-even"><p>Number of epochs you want to train. The value defaults to 1.</p>
</dd>
<dt class="field-odd">Params batch_size</dt>
<dd class="field-odd"><p>Number of batch size you want to train. The value defaults to 32.
Do not specify the batch_size, if your data in the form of tf.data datasets.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>predict batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).
The value default to 32. If set to None,
the model will be used directly for inference.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.lstm_forecaster.LSTMForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The location you want to save the forecaster.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="seq2seqforecaster">
<h2>Seq2SeqForecaster<a class="headerlink" href="#seq2seqforecaster" title="Permalink to this headline">¶</a></h2>
<p>Seq2SeqForecaster wraps a sequence to sequence model based on LSTM, and is suitable for multivariant &amp; multistep time series forecasting.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-1-1-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-1-1-0" name="1-0" role="tab" tabindex="0">PyTorch</button><button aria-controls="panel-1-1-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-1-1-1" name="1-1" role="tab" tabindex="-1">Tensorflow</button></div><div aria-labelledby="tab-1-1-0" class="sphinx-tabs-panel" id="panel-1-1-0" name="1-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.seq2seq_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.seq2seq_forecaster.</span></span><span class="sig-name descname"><span class="pre">Seq2SeqForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lstm_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lstm_layer_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_forcing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/seq2seq_forecaster.html#Seq2SeqForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.base_forecaster.BasePytorchForecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1. Initialize Forecaster directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">Seq2SeqForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                                   future_seq_len=2,</span>
<span class="go">                                   input_feature_num=1,</span>
<span class="go">                                   output_feature_num=1,</span>
<span class="go">                                   ...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2. Initialize Forecaster from from_tsdataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">Seq2SeqForecaster</span><span class="o">.</span><span class="n">from_tsdataset</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">to_local</span><span class="p">()</span>  <span class="c1"># if you set distributed=True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a Seq2Seq Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>lstm_hidden_dim</strong> – LSTM hidden channel for decoder and encoder.
The value defaults to 64.</p></li>
<li><p><strong>lstm_layer_num</strong> – LSTM layer number for decoder and encoder.
The value defaults to 2.</p></li>
<li><p><strong>teacher_forcing</strong> – If use teacher forcing in training. The value
defaults to False.</p></li>
<li><p><strong>dropout</strong> – Specify the dropout close possibility (i.e. the close
possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – str or pytorch loss instance, Specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae”, “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.build_onnx">
<span class="sig-name descname"><span class="pre">build_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">thread_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.build_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Build onnx model to speed up inference and reduce latency.
The method is Not required to call before predict_with_onnx,
evaluate_with_onnx or export_onnx_file.
It is recommended to use when you want to:</p>
<div class="line-block">
<div class="line">1. Strictly control the thread to be used during inferencing.</div>
<div class="line">2. Alleviate the cold start problem when you call predict_with_onnx
for the first time.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>thread_num</strong> – int, the num of thread limit. The value is set to None by
default where no limit is set.</p></li>
<li><p><strong>sess_options</strong> – an onnxruntime.SessionOptions instance, if you set this
other than None, a new onnxruntime session will be built on this setting
and ignore other settings you assigned(e.g. thread_num…).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># to pre build onnx sess</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">build_onnx</span><span class="p">(</span><span class="n">thread_num</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># build onnx runtime sess for single thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ------------------------------------------------------</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># directly call onnx related method is also supported</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.build_openvino">
<span class="sig-name descname"><span class="pre">build_openvino</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.build_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Build openvino model to speed up inference and reduce latency.
The method is Not required to call before predict_with_openvino.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<p>if you want to evaluate on a single node(which is common practice), please call
.to_local().evaluate(data, …)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.evaluate_with_onnx">
<span class="sig-name descname"><span class="pre">evaluate_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.evaluate_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.export_onnx_file">
<span class="sig-name descname"><span class="pre">export_onnx_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model.onnx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantized_dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'qmodel.onnx'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.export_onnx_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the onnx model file to the disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dirname</strong> – The dir location you want to save the onnx file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'output'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">earlystop_patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line"><br /></div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>validation_data</strong> – <p>Validation sample for validation loop. Defaults to ‘None’.
If you do not input data for ‘validation_data’, the validation_step will be skipped.
The validation_data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 1.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.
If you input a pytorch dataloader for <cite>data</cite>, the batch_size will follow the
batch_size setted in <cite>data</cite>.if the forecaster is distributed, the batch_size will be
evenly distributed to all workers.</p></li>
<li><p><strong>validation_mode</strong> – <p>A str represent the operation mode while having ‘validation_data’.
Defaults to ‘output’. The validation_mode includes the following types:</p>
<div class="line-block">
<div class="line">1. output:</div>
<div class="line">If you choose ‘output’ for validation_mode, it will return a dict that records the</div>
<div class="line">average validation loss of each epoch.</div>
<div class="line"><br /></div>
<div class="line">2. earlystop:</div>
<div class="line">Monitor the val_loss and stop training when it stops improving.</div>
</div>
</p></li>
<li><p><strong>earlystop_patience</strong> – Number of checks with no improvement after which training will
be stopped. It takes effect when ‘validation_mode’ is ‘earlystop’. Under the default
configuration, one check happens after every training epoch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Validation loss if ‘validation_data’ is not None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.from_tsdataset">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_tsdataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tsdataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.from_tsdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a Forecaster Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tsdataset</strong> – A bigdl.chronos.data.tsdataset.TSDataset instance.</p></li>
<li><p><strong>past_seq_len</strong> – int or “auto”, Specify the history time steps (i.e. lookback).
Do not specify the ‘past_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.
If “auto”, the mode of time series’ cycle length will be taken as the past_seq_len.</p></li>
<li><p><strong>future_seq_len</strong> – int or list, Specify the output time steps (i.e. horizon).
Do not specify the ‘future_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.</p></li>
<li><p><strong>kwargs</strong> – Specify parameters of Forecaster,
e.g. loss and optimizer, etc.
More info, please refer to Forecaster.__init__ methods.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Forecaster Model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.get_model">
<span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.get_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learned PyTorch model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a pytorch model instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>restore the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The checkpoint file location you want to
load the quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<p>if you want to predict on a single node(which is common practice), please call
.to_local().predict(x, …)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x}, where x’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim)
if data is a numpy ndarray or a dataloader.
A xshard item with format {‘prediction’: result},
where result is a numpy array with shape (num_samples, horizon, target_dim)
if data is a xshard item.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict_with_onnx">
<span class="sig-name descname"><span class="pre">predict_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict_with_openvino">
<span class="sig-name descname"><span class="pre">predict_with_openvino</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.predict_with_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with openvino. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_openvino is valid and Forecaster will
automatically build an openvino session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">calib_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">framework</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pytorch_fx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approach</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relative_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>calib_data</strong> – A torch.utils.data.dataloader.DataLoader object for calibration.
Required for static quantization.</p></li>
<li><p><strong>val_data</strong> – A torch.utils.data.dataloader.DataLoader object for evaluation.</p></li>
<li><p><strong>metric</strong> – A str represent the metrics for tunning the quality of
quantization. You may choose from “mse”, “mae”, “rmse”, “r2”, “mape”, “smape”.</p></li>
<li><p><strong>conf</strong> – A path to conf yaml file for quantization. Default to None,
using default config.</p></li>
<li><p><strong>framework</strong> – string or list, [{‘pytorch’|’pytorch_fx’|’pytorch_ipex’},
{‘onnxrt_integerops’|’onnxrt_qlinearops’}]. Default: ‘pytorch_fx’.
Consistent with Intel Neural Compressor.</p></li>
<li><p><strong>approach</strong> – str, ‘static’ or ‘dynamic’. Default to ‘static’.</p></li>
<li><p><strong>tuning_strategy</strong> – str, ‘bayesian’, ‘basic’, ‘mse’ or ‘sigopt’. Default to ‘bayesian’.</p></li>
<li><p><strong>relative_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 0.1 means that we accept a 10% increase in the metrics error.</p></li>
<li><p><strong>absolute_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 5 means that we can only accept metrics smaller than 5.</p></li>
<li><p><strong>timeout</strong> – Tuning timeout (seconds). Default to 0, which means early stop.
Combine with max_trials field to decide when to exit.</p></li>
<li><p><strong>max_trials</strong> – Max tune times. Default to 1. Combine with timeout field to
decide when to exit. “timeout=0, max_trials=1” means it will try quantization
only once and return satisfying best model.</p></li>
<li><p><strong>sess_options</strong> – The session option for onnxruntime, only valid when
framework contains ‘onnxrt_integerops’ or ‘onnxrt_qlinearops’,
otherwise will be ignored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<p>Please note that if you only want the pytorch model or onnx model
file, you can call .get_model() or .export_onnx_file(). The checkpoint
file generated by .save() method can only be used by .load().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The location you want to save the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The location you want to save quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.to_local">
<span class="sig-name descname"><span class="pre">to_local</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.to_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a distributed forecaster to a local (non-distributed) one.</p>
<p>Common practice is to use distributed training (fit) and predict/
evaluate with onnx or other frameworks on a single node. To do so,
you need to call .to_local() and transform the forecaster to a non-
distributed one.</p>
<p>The optimizer is refreshed, incremental training after to_local
might have some problem.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a forecaster instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.seq2seq_forecaster.Seq2SeqForecaster.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Search the hyper parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – train data, as numpy ndarray tuple (x, y)</p></li>
<li><p><strong>validation_data</strong> – validation data, as numpy ndarray tuple (x,y)</p></li>
<li><p><strong>target_metric</strong> – the target metric to optimize,
a string or an instance of torchmetrics.metric.Metric</p></li>
<li><p><strong>direction</strong> – in which direction to optimize the target metric,
“maximize” - larger the better
“minimize” - smaller the better</p></li>
<li><p><strong>n_trials</strong> – number of trials to run</p></li>
<li><p><strong>n_parallels</strong> – number of parallel processes used to run trials.
to use parallel tuning you need to use a RDB url for storage and specify study_name.
For more information, refer to Nano AutoML user guide.</p></li>
<li><p><strong>epochs</strong> – the number of epochs to run in each trial fit, defaults to 1</p></li>
<li><p><strong>batch_size</strong> – number of batch size for each trial fit, defaults to 32</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div><div aria-labelledby="tab-1-1-1" class="sphinx-tabs-panel" hidden="true" id="panel-1-1-1" name="1-1" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tf.seq2seq_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tf.seq2seq_forecaster.</span></span><span class="sig-name descname"><span class="pre">Seq2SeqForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lstm_hidden_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lstm_layer_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">teacher_forcing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/seq2seq_forecaster.html#Seq2SeqForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.tf.base_forecaster.BaseTF2Forecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">Seq2SeqForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                                   future_seq_len=2,</span>
<span class="go">                                   input_feature_num=1,</span>
<span class="go">                                   output_feature_num=1,</span>
<span class="go">                                   ...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_dir_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_dir_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a Seq2Seq Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>lstm_hidden_dim</strong> – LSTM hidden channel for decoder and encoder.
The value defaults to 64.</p></li>
<li><p><strong>lstm_layer_num</strong> – LSTM layer number for decoder and encoder.
The value defaults to 2.</p></li>
<li><p><strong>teacher_forcing</strong> – If use teacher forcing in training. The value
defaults to False.</p></li>
<li><p><strong>dropout</strong> – Specify the dropout close possibility (i.e. the close
possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – Str or a tf.keras.losses.Loss instance, specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae” and “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a Keras model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.chronos.metric.forecaster_metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p>
</dd>
<dt class="field-odd">Params multioutput_value</dt>
<dd class="field-odd"><p>Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a tf.data.Dataset:</div>
<div class="line">A TFDataset instance which contains x and y with same shape as the tuple.</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim),</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim).</div>
</div>
</p>
</dd>
<dt class="field-even">Params epochs</dt>
<dd class="field-even"><p>Number of epochs you want to train. The value defaults to 1.</p>
</dd>
<dt class="field-odd">Params batch_size</dt>
<dd class="field-odd"><p>Number of batch size you want to train. The value defaults to 32.
Do not specify the batch_size, if your data in the form of tf.data datasets.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>predict batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).
The value default to 32. If set to None,
the model will be used directly for inference.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.seq2seq_forecaster.Seq2SeqForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The location you want to save the forecaster.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="tcnforecaster">
<h2>TCNForecaster<a class="headerlink" href="#tcnforecaster" title="Permalink to this headline">¶</a></h2>
<p>Temporal Convolutional Networks (TCN) is a neural network that use convolutional architecture rather than recurrent networks. It supports multi-step and multi-variant cases. Causal Convolutions enables large scale parallel computing which makes TCN has less inference time than RNN based model such as LSTM.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-2-2-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-2-2-0" name="2-0" role="tab" tabindex="0">PyTorch</button><button aria-controls="panel-2-2-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-2-2-1" name="2-1" role="tab" tabindex="-1">Tensorflow</button></div><div aria-labelledby="tab-2-2-0" class="sphinx-tabs-panel" id="panel-2-2-0" name="2-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tcn_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tcn_forecaster.</span></span><span class="sig-name descname"><span class="pre">TCNForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repo_initialization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcn_forecaster.html#TCNForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.base_forecaster.BasePytorchForecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1. Initialize Forecaster directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">TCNForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                               future_seq_len=5,</span>
<span class="go">                               input_feature_num=1,</span>
<span class="go">                               output_feature_num=1,</span>
<span class="go">                               ...)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2. Initialize Forecaster from from_tsdataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">TCNForecaster</span><span class="o">.</span><span class="n">from_tsdataset</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">to_local</span><span class="p">()</span>  <span class="c1"># if you set distributed=True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">test_eval</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">evaluate</span><span class="p">((</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a TCN Forecast Model.</p>
<p>TCN Forecast may fall into local optima. Please set repo_initialization
to False to alleviate the issue. You can also change a random seed to
work around.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>num_channels</strong> – Specify the convolutional layer filter number in
TCN’s encoder. This value defaults to [30]*7.</p></li>
<li><p><strong>kernel_size</strong> – Specify convolutional layer filter height in TCN’s
encoder. This value defaults to 3.</p></li>
<li><p><strong>repo_initialization</strong> – if to use framework default initialization,
True to use paper author’s initialization and False to use the
framework’s default initialization. The value defaults to True.</p></li>
<li><p><strong>dropout</strong> – Specify the dropout close possibility (i.e. the close
possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – str or pytorch loss instance, Specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae”, “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.build_onnx">
<span class="sig-name descname"><span class="pre">build_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">thread_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.build_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Build onnx model to speed up inference and reduce latency.
The method is Not required to call before predict_with_onnx,
evaluate_with_onnx or export_onnx_file.
It is recommended to use when you want to:</p>
<div class="line-block">
<div class="line">1. Strictly control the thread to be used during inferencing.</div>
<div class="line">2. Alleviate the cold start problem when you call predict_with_onnx
for the first time.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>thread_num</strong> – int, the num of thread limit. The value is set to None by
default where no limit is set.</p></li>
<li><p><strong>sess_options</strong> – an onnxruntime.SessionOptions instance, if you set this
other than None, a new onnxruntime session will be built on this setting
and ignore other settings you assigned(e.g. thread_num…).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># to pre build onnx sess</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">build_onnx</span><span class="p">(</span><span class="n">thread_num</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># build onnx runtime sess for single thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ------------------------------------------------------</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># directly call onnx related method is also supported</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.build_openvino">
<span class="sig-name descname"><span class="pre">build_openvino</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.build_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Build openvino model to speed up inference and reduce latency.
The method is Not required to call before predict_with_openvino.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<p>if you want to evaluate on a single node(which is common practice), please call
.to_local().evaluate(data, …)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.evaluate_with_onnx">
<span class="sig-name descname"><span class="pre">evaluate_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.evaluate_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.export_onnx_file">
<span class="sig-name descname"><span class="pre">export_onnx_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model.onnx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantized_dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'qmodel.onnx'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.export_onnx_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the onnx model file to the disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dirname</strong> – The dir location you want to save the onnx file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'output'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">earlystop_patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line"><br /></div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>validation_data</strong> – <p>Validation sample for validation loop. Defaults to ‘None’.
If you do not input data for ‘validation_data’, the validation_step will be skipped.
The validation_data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 1.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.
If you input a pytorch dataloader for <cite>data</cite>, the batch_size will follow the
batch_size setted in <cite>data</cite>.if the forecaster is distributed, the batch_size will be
evenly distributed to all workers.</p></li>
<li><p><strong>validation_mode</strong> – <p>A str represent the operation mode while having ‘validation_data’.
Defaults to ‘output’. The validation_mode includes the following types:</p>
<div class="line-block">
<div class="line">1. output:</div>
<div class="line">If you choose ‘output’ for validation_mode, it will return a dict that records the</div>
<div class="line">average validation loss of each epoch.</div>
<div class="line"><br /></div>
<div class="line">2. earlystop:</div>
<div class="line">Monitor the val_loss and stop training when it stops improving.</div>
</div>
</p></li>
<li><p><strong>earlystop_patience</strong> – Number of checks with no improvement after which training will
be stopped. It takes effect when ‘validation_mode’ is ‘earlystop’. Under the default
configuration, one check happens after every training epoch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Validation loss if ‘validation_data’ is not None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.from_tsdataset">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_tsdataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tsdataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.from_tsdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a Forecaster Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tsdataset</strong> – A bigdl.chronos.data.tsdataset.TSDataset instance.</p></li>
<li><p><strong>past_seq_len</strong> – int or “auto”, Specify the history time steps (i.e. lookback).
Do not specify the ‘past_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.
If “auto”, the mode of time series’ cycle length will be taken as the past_seq_len.</p></li>
<li><p><strong>future_seq_len</strong> – int or list, Specify the output time steps (i.e. horizon).
Do not specify the ‘future_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.</p></li>
<li><p><strong>kwargs</strong> – Specify parameters of Forecaster,
e.g. loss and optimizer, etc.
More info, please refer to Forecaster.__init__ methods.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A Forecaster Model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.get_model">
<span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.get_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learned PyTorch model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a pytorch model instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>restore the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The checkpoint file location you want to
load the quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<p>if you want to predict on a single node(which is common practice), please call
.to_local().predict(x, …)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x}, where x’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim)
if data is a numpy ndarray or a dataloader.
A xshard item with format {‘prediction’: result},
where result is a numpy array with shape (num_samples, horizon, target_dim)
if data is a xshard item.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict_with_onnx">
<span class="sig-name descname"><span class="pre">predict_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict_with_openvino">
<span class="sig-name descname"><span class="pre">predict_with_openvino</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.predict_with_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with openvino. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_openvino is valid and Forecaster will
automatically build an openvino session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">calib_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">framework</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pytorch_fx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approach</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relative_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>calib_data</strong> – A torch.utils.data.dataloader.DataLoader object for calibration.
Required for static quantization.</p></li>
<li><p><strong>val_data</strong> – A torch.utils.data.dataloader.DataLoader object for evaluation.</p></li>
<li><p><strong>metric</strong> – A str represent the metrics for tunning the quality of
quantization. You may choose from “mse”, “mae”, “rmse”, “r2”, “mape”, “smape”.</p></li>
<li><p><strong>conf</strong> – A path to conf yaml file for quantization. Default to None,
using default config.</p></li>
<li><p><strong>framework</strong> – string or list, [{‘pytorch’|’pytorch_fx’|’pytorch_ipex’},
{‘onnxrt_integerops’|’onnxrt_qlinearops’}]. Default: ‘pytorch_fx’.
Consistent with Intel Neural Compressor.</p></li>
<li><p><strong>approach</strong> – str, ‘static’ or ‘dynamic’. Default to ‘static’.</p></li>
<li><p><strong>tuning_strategy</strong> – str, ‘bayesian’, ‘basic’, ‘mse’ or ‘sigopt’. Default to ‘bayesian’.</p></li>
<li><p><strong>relative_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 0.1 means that we accept a 10% increase in the metrics error.</p></li>
<li><p><strong>absolute_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 5 means that we can only accept metrics smaller than 5.</p></li>
<li><p><strong>timeout</strong> – Tuning timeout (seconds). Default to 0, which means early stop.
Combine with max_trials field to decide when to exit.</p></li>
<li><p><strong>max_trials</strong> – Max tune times. Default to 1. Combine with timeout field to
decide when to exit. “timeout=0, max_trials=1” means it will try quantization
only once and return satisfying best model.</p></li>
<li><p><strong>sess_options</strong> – The session option for onnxruntime, only valid when
framework contains ‘onnxrt_integerops’ or ‘onnxrt_qlinearops’,
otherwise will be ignored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<p>Please note that if you only want the pytorch model or onnx model
file, you can call .get_model() or .export_onnx_file(). The checkpoint
file generated by .save() method can only be used by .load().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The location you want to save the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The location you want to save quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.to_local">
<span class="sig-name descname"><span class="pre">to_local</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.to_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a distributed forecaster to a local (non-distributed) one.</p>
<p>Common practice is to use distributed training (fit) and predict/
evaluate with onnx or other frameworks on a single node. To do so,
you need to call .to_local() and transform the forecaster to a non-
distributed one.</p>
<p>The optimizer is refreshed, incremental training after to_local
might have some problem.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a forecaster instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tcn_forecaster.TCNForecaster.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Search the hyper parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – train data, as numpy ndarray tuple (x, y)</p></li>
<li><p><strong>validation_data</strong> – validation data, as numpy ndarray tuple (x,y)</p></li>
<li><p><strong>target_metric</strong> – the target metric to optimize,
a string or an instance of torchmetrics.metric.Metric</p></li>
<li><p><strong>direction</strong> – in which direction to optimize the target metric,
“maximize” - larger the better
“minimize” - smaller the better</p></li>
<li><p><strong>n_trials</strong> – number of trials to run</p></li>
<li><p><strong>n_parallels</strong> – number of parallel processes used to run trials.
to use parallel tuning you need to use a RDB url for storage and specify study_name.
For more information, refer to Nano AutoML user guide.</p></li>
<li><p><strong>epochs</strong> – the number of epochs to run in each trial fit, defaults to 1</p></li>
<li><p><strong>batch_size</strong> – number of batch size for each trial fit, defaults to 32</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div><div aria-labelledby="tab-2-2-1" class="sphinx-tabs-panel" hidden="true" id="panel-2-2-1" name="2-1" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tf.tcn_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tf.tcn_forecaster.</span></span><span class="sig-name descname"><span class="pre">TCNForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30,</span> <span class="pre">30]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repo_initialization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/tcn_forecaster.html#TCNForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.tf.base_forecaster.BaseTF2Forecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">TCNForecaster</span><span class="p">(</span><span class="n">past_seq_len</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
<span class="go">                               future_seq_len=5,</span>
<span class="go">                               input_feature_num=1,</span>
<span class="go">                               output_feature_num=1,</span>
<span class="go">                               ...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">((</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">ckpt_name</span><span class="p">})</span>
</pre></div>
</div>
<p>Build a TCN Forecast Model.</p>
<p>TCN Forecast may fall into local optima. Please set repo_initialization
to False to alleviate the issue. You can also change a random seed to
work around.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>num_channels</strong> – Specify the convolutional layer filter number in
TCN’s encoder. This value defaults to [30]*7.</p></li>
<li><p><strong>kernel_size</strong> – Specify convolutional layer filter height in TCN’s
encoder. This value defaults to 3.</p></li>
<li><p><strong>repo_initialization</strong> – if to use framework default initialization,
True to use paper author’s initialization and False to use the
framework’s default initialization. The value defaults to True.</p></li>
<li><p><strong>dropout</strong> – Specify the dropout close possibility (i.e. the close
possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – Str or a tf.keras.losses.Loss instance, specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae” and “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.chronos.metric.forecaster_metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p>
</dd>
<dt class="field-odd">Params multioutput_value</dt>
<dd class="field-odd"><p>Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a tf.data.Dataset:</div>
<div class="line">A TFDataset instance which contains x and y with same shape as the tuple.</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim),</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim).</div>
</div>
</p>
</dd>
<dt class="field-even">Params epochs</dt>
<dd class="field-even"><p>Number of epochs you want to train. The value defaults to 1.</p>
</dd>
<dt class="field-odd">Params batch_size</dt>
<dd class="field-odd"><p>Number of batch size you want to train. The value defaults to 32.
Do not specify the batch_size, if your data in the form of tf.data datasets.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list">
<dt class="field-odd">Params data</dt>
<dd class="field-odd"><p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</dd>
<dt class="field-even">Params batch_size</dt>
<dd class="field-even"><p>predict batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).
The value default to 32. If set to None,
the model will be used directly for inference.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.tf.tcn_forecaster.TCNForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Params checkpoint_file</dt>
<dd class="field-odd"><p>The location you want to save the forecaster.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="autoformerforecaster">
<h2>AutoformerForecaster<a class="headerlink" href="#autoformerforecaster" title="Permalink to this headline">¶</a></h2>
<p>Autoformer is a neural network that use transformer architecture with autocorrelation. It supports multi-step and multi-variant cases. It shows significant accuracy improvement while longer training/inference time than TCN.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-3-3-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-3-3-0" name="3-0" role="tab" tabindex="0">PyTorch</button></div><div aria-labelledby="tab-3-3-0" class="sphinx-tabs-panel" id="panel-3-3-0" name="3-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.autoformer_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.autoformer_forecaster.</span></span><span class="sig-name descname"><span class="pre">AutoformerForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_feature_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attention</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moving_avg</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'timeF'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">factor</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_head</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gelu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">e_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_scheduler_milestones</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">9,</span> <span class="pre">10]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.abstract.Forecaster</span></code></p>
<p>Build a AutoformerForecaster Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>input_feature_num</strong> – Specify the feature dimension.</p></li>
<li><p><strong>output_feature_num</strong> – Specify the output dimension.</p></li>
<li><p><strong>label_len</strong> – Start token length of AutoFormer decoder.</p></li>
<li><p><strong>freq</strong> – Freq for time features encoding. You may choose from “s”,
“t”,”h”,”d”,”w”,”m” for second, minute, hour, day, week or month.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – str or pytorch loss instance, Specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae”, “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>lr_scheduler_milestones</strong> – Specify the milestones parameters in
torch.optim.lr_scheduler.MultiStepLR.This value defaults to
[3, 4, 5, 6, 7, 8, 9, 10]. If you don’t want to use scheduler,
set this parameter to None to disbale lr_scheduler.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
<li><p><strong>kwargs</strong> – other hyperparameter please refer to
<a class="reference external" href="https://github.com/zhouhaoyi/Informer2020#usage">https://github.com/zhouhaoyi/Informer2020#usage</a></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minimize'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.tune"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Search the hyper parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – train data, as numpy ndarray tuple (x, y, x_enc, y_enc)</p></li>
<li><p><strong>validation_data</strong> – validation data, as numpy ndarray tuple (x, y, x_enc, y_enc)</p></li>
<li><p><strong>target_metric</strong> – the target metric to optimize,
a string or an instance of torchmetrics.metric.Metric, default to ‘mse’.</p></li>
<li><p><strong>direction</strong> – in which direction to optimize the target metric,
“maximize” - larger the better
“minimize” - smaller the better
default to “minimize”.</p></li>
<li><p><strong>n_trials</strong> – number of trials to run</p></li>
<li><p><strong>n_parallels</strong> – number of parallel processes used to run trials.
to use parallel tuning you need to use a RDB url for storage and specify study_name.
For more information, refer to Nano AutoML user guide.</p></li>
<li><p><strong>epochs</strong> – the number of epochs to run in each trial fit, defaults to 1</p></li>
<li><p><strong>batch_size</strong> – number of batch size for each trial fit, defaults to 32</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.search_summary">
<span class="sig-name descname"><span class="pre">search_summary</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.search_summary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.search_summary" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. numpy ndarrays: generate from <cite>TSDataset.roll</cite>,
be sure to set label_len &gt; 0 and time_enc = True</div>
<div class="line">2. pytorch dataloader: generate from <cite>TSDataset.to_torch_data_loader</cite>,
be sure to set label_len &gt; 0 and time_enc = True</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 1.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.
if you input a pytorch dataloader for <cite>data</cite>, the batch_size will follow the
batch_size setted in <cite>data</cite>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. numpy ndarrays: generate from <cite>TSDataset.roll</cite>,
be sure to set label_len &gt; 0 and time_enc = True</div>
<div class="line">2. pytorch dataloader: generate from <cite>TSDataset.to_torch_data_loader</cite>,
be sure to set label_len &gt; 0, time_enc = True and is_predict = True</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of numpy ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. numpy ndarrays: generate from <cite>TSDataset.roll</cite>,
be sure to set label_len &gt; 0 and time_enc = True</div>
<div class="line">2. pytorch dataloader: generate from <cite>TSDataset.to_torch_data_loader</cite>,
be sure to set label_len &gt; 0 and time_enc = True</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A dict, currently returns the loss rather than metrics</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>restore the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/autoformer_forecaster.html#AutoformerForecaster.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.autoformer_forecaster.AutoformerForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="nbeatsforecaster">
<h2>NBeatsForecaster<a class="headerlink" href="#nbeatsforecaster" title="Permalink to this headline">¶</a></h2>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-4-4-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-4-4-0" name="4-0" role="tab" tabindex="0">PyTorch</button></div><div aria-labelledby="tab-4-4-0" class="sphinx-tabs-panel" id="panel-4-4-0" name="4-0" role="tabpanel" tabindex="0"><p>Neural basis expansion analysis for interpretable time series forecasting (<a class="reference external" href="https://arxiv.org/abs/1905.10437">N-BEATS</a>) is a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. Nbeats can solve univariate time series point forecasting problems, being interpretable, and fast to train.</p>
<span class="target" id="module-bigdl.chronos.forecaster.nbeats_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.nbeats_forecaster.</span></span><span class="sig-name descname"><span class="pre">NBeatsForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stack_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('generic',</span> <span class="pre">'generic')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_blocks_per_stack</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">thetas_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(4,</span> <span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_weights_in_stack</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_layer_units</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nb_harmonics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mse']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">workers_per_node</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distributed_backend</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ray'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/nbeats_forecaster.html#NBeatsForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.base_forecaster.BasePytorchForecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># 1. Initialize Forecaster directly</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span> <span class="o">=</span> <span class="n">NBeatForecaster</span><span class="p">(</span><span class="n">paste_seq_len</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="go">                                 future_seq_len=1,</span>
<span class="go">                                 stack_types=(&quot;generic&quot;, &quot;generic&quot;),</span>
<span class="go">                                 ...)</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># 2. The from_tsdataset method can also initialize a NBeatForecaster.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">from_tsdataset</span><span class="p">(</span><span class="n">tsdata</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">tsdata</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">to_local</span><span class="p">()</span> <span class="c1"># if you set distributed=True</span>
</pre></div>
</div>
<p>Build a NBeats Forecaster Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).</p></li>
<li><p><strong>stack_types</strong> – Specifies the type of stack,
including “generic”, “trend”, “seasnoality”.
This value defaults to (“generic”, “generic”).
If set distributed=True, the second type should not be “generic”,
use “seasonality” or “trend”, e.g. (“generic”, “trend”).</p></li>
<li><p><strong>nb_blocks_per_stack</strong> – Specify the number of blocks
contained in each stack, This value defaults to 3.</p></li>
<li><p><strong>thetas_dim</strong> – Expansion Coefficients of Multilayer FC Networks.
if type is “generic”, Extended length factor, if type is “trend”
then polynomial coefficients, if type is “seasonality”
expressed as a change within each step.</p></li>
<li><p><strong>share_weights_in_stack</strong> – Share block weights for each stack.,
This value defaults to False.</p></li>
<li><p><strong>hidden_layer_units</strong> – Number of fully connected layers with per block.
This values defaults to 256.</p></li>
<li><p><strong>nb_harmonics</strong> – Only available in “seasonality” type,
specifies the time step of backward, This value defaults is None.</p></li>
<li><p><strong>dropout</strong> – Specify the dropout close possibility
(i.e. the close possibility to a neuron). This value defaults to 0.1.</p></li>
<li><p><strong>optimizer</strong> – Specify the optimizer used for training. This value
defaults to “Adam”.</p></li>
<li><p><strong>loss</strong> – str or pytorch loss instance, Specify the loss function
used for training. This value defaults to “mse”. You can choose
from “mse”, “mae”, “huber_loss” or any customized loss instance
you want to use.</p></li>
<li><p><strong>lr</strong> – Specify the learning rate. This value defaults to 0.001.</p></li>
<li><p><strong>metrics</strong> – A list contains metrics for evaluating the quality of
forecasting. You may only choose from “mse” and “mae” for a
distributed forecaster. You may choose from “mse”, “mae”,
“rmse”, “r2”, “mape”, “smape” or a callable function for a
non-distributed forecaster. If callable function, it signature
should be func(y_true, y_pred), where y_true and y_pred are numpy
ndarray.</p></li>
<li><p><strong>seed</strong> – int, random seed for training. This value defaults to None.</p></li>
<li><p><strong>distributed</strong> – bool, if init the forecaster in a distributed
fashion. If True, the internal model will use an Orca Estimator.
If False, the internal model will use a pytorch model. The value
defaults to False.</p></li>
<li><p><strong>workers_per_node</strong> – int, the number of worker you want to use.
The value defaults to 1. The param is only effective when
distributed is set to True.</p></li>
<li><p><strong>distributed_backend</strong> – str, select from “ray” or
“horovod”. The value defaults to “ray”.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.build_onnx">
<span class="sig-name descname"><span class="pre">build_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">thread_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.build_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Build onnx model to speed up inference and reduce latency.
The method is Not required to call before predict_with_onnx,
evaluate_with_onnx or export_onnx_file.
It is recommended to use when you want to:</p>
<div class="line-block">
<div class="line">1. Strictly control the thread to be used during inferencing.</div>
<div class="line">2. Alleviate the cold start problem when you call predict_with_onnx
for the first time.</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>thread_num</strong> – int, the num of thread limit. The value is set to None by
default where no limit is set.</p></li>
<li><p><strong>sess_options</strong> – an onnxruntime.SessionOptions instance, if you set this
other than None, a new onnxruntime session will be built on this setting
and ignore other settings you assigned(e.g. thread_num…).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># to pre build onnx sess</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">forecaster</span><span class="o">.</span><span class="n">build_onnx</span><span class="p">(</span><span class="n">thread_num</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># build onnx runtime sess for single thread</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ------------------------------------------------------</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># directly call onnx related method is also supported</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict_with_onnx</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.build_openvino">
<span class="sig-name descname"><span class="pre">build_openvino</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.build_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Build openvino model to speed up inference and reduce latency.
The method is Not required to call before predict_with_openvino.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<p>if you want to evaluate on a single node(which is common practice), please call
.to_local().evaluate(data, …)</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.The param is only effective when the forecaster is a
non-distribtued version.</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.evaluate_with_onnx">
<span class="sig-name descname"><span class="pre">evaluate_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.evaluate_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<p>Please note that evaluate result is calculated by scaled y and yhat. If you scaled
your data (e.g. use .scale() on the TSDataset) please follow the following code
snap to evaluate your result if you need to evaluate on unscaled data.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">bigdl.orca.automl.metrics</span> <span class="k">import</span> <span class="n">Evaluator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat</span> <span class="o">=</span> <span class="n">forecaster</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_hat_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_unscaled</span> <span class="o">=</span> <span class="n">tsdata</span><span class="o">.</span><span class="n">unscale_numpy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># or other customized unscale methods</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">metric</span><span class="o">=...</span><span class="p">,</span> <span class="n">y_unscaled</span><span class="p">,</span> <span class="n">y_hat_unscaled</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.export_onnx_file">
<span class="sig-name descname"><span class="pre">export_onnx_file</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'model.onnx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantized_dirname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'qmodel.onnx'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.export_onnx_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the onnx model file to the disk.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dirname</strong> – The dir location you want to save the onnx file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'output'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">earlystop_patience</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit(Train) the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x, ‘y’: y}, where x and y’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line"><br /></div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>validation_data</strong> – <p>Validation sample for validation loop. Defaults to ‘None’.
If you do not input data for ‘validation_data’, the validation_step will be skipped.
The validation_data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
<div class="line"><br /></div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader should return x, y in each iteration with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">y’s shape is (num_samples, horizon, target_dim), where horizon and target_dim</div>
<div class="line">should be the same as future_seq_len and output_feature_num.</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 1.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.
If you input a pytorch dataloader for <cite>data</cite>, the batch_size will follow the
batch_size setted in <cite>data</cite>.if the forecaster is distributed, the batch_size will be
evenly distributed to all workers.</p></li>
<li><p><strong>validation_mode</strong> – <p>A str represent the operation mode while having ‘validation_data’.
Defaults to ‘output’. The validation_mode includes the following types:</p>
<div class="line-block">
<div class="line">1. output:</div>
<div class="line">If you choose ‘output’ for validation_mode, it will return a dict that records the</div>
<div class="line">average validation loss of each epoch.</div>
<div class="line"><br /></div>
<div class="line">2. earlystop:</div>
<div class="line">Monitor the val_loss and stop training when it stops improving.</div>
</div>
</p></li>
<li><p><strong>earlystop_patience</strong> – Number of checks with no improvement after which training will
be stopped. It takes effect when ‘validation_mode’ is ‘earlystop’. Under the default
configuration, one check happens after every training epoch.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Validation loss if ‘validation_data’ is not None.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.get_model">
<span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.get_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the learned PyTorch model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a pytorch model instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>restore the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The checkpoint file location you want to
load the quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<p>if you want to predict on a single node(which is common practice), please call
.to_local().predict(x, …)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. a xshard item:</div>
<div class="line">each partition can be a dictionary of {‘x’: x}, where x’s shape</div>
<div class="line">should follow the shape stated before.</div>
<div class="line">3. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">4. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>quantize</strong> – if use the quantized model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim)
if data is a numpy ndarray or a dataloader.
A xshard item with format {‘prediction’: result},
where result is a numpy array with shape (num_samples, horizon, target_dim)
if data is a xshard item.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict_with_onnx">
<span class="sig-name descname"><span class="pre">predict_with_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict_with_onnx" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with onnxruntime. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_onnx is valid and Forecaster will
automatically build an onnxruntime session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">2. pytorch dataloader:</div>
<div class="line">the dataloader needs to return at least x in each iteration</div>
<div class="line">with the shape as following:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
<div class="line">If returns x and y only get x.</div>
<div class="line">3. A bigdl.chronos.data.tsdataset.TSDataset instance:</div>
<div class="line">Forecaster will automatically process the TSDataset.</div>
<div class="line">By default, TSDataset will be transformed to a pytorch dataloader,</div>
<div class="line">which is memory-friendly while a little bit slower.</div>
<div class="line">Users may call <cite>roll</cite> on the TSDataset before calling <cite>fit</cite></div>
<div class="line">Then the training speed will be faster but will consume more memory.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
<li><p><strong>quantize</strong> – if use the quantized onnx model to predict.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict_with_openvino">
<span class="sig-name descname"><span class="pre">predict_with_openvino</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.predict_with_openvino" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster with openvino. The method can only be
used when forecaster is a non-distributed version.</p>
<p>Directly call this method without calling build_openvino is valid and Forecaster will
automatically build an openvino session with default settings.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray x:</div>
<div class="line">x’s shape is (num_samples, lookback, feature_dim) where lookback and feature_dim</div>
<div class="line">should be the same as past_seq_len and input_feature_num.</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect predict
result but will affect resources cost(e.g. memory and time). Defaults
to 32. None for all-data-single-time inference.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy array with shape (num_samples, horizon, target_dim).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">calib_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">framework</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'pytorch_fx'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">approach</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'static'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tuning_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'bayesian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">relative_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">absolute_drop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timeout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sess_options</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>calib_data</strong> – A torch.utils.data.dataloader.DataLoader object for calibration.
Required for static quantization.</p></li>
<li><p><strong>val_data</strong> – A torch.utils.data.dataloader.DataLoader object for evaluation.</p></li>
<li><p><strong>metric</strong> – A str represent the metrics for tunning the quality of
quantization. You may choose from “mse”, “mae”, “rmse”, “r2”, “mape”, “smape”.</p></li>
<li><p><strong>conf</strong> – A path to conf yaml file for quantization. Default to None,
using default config.</p></li>
<li><p><strong>framework</strong> – string or list, [{‘pytorch’|’pytorch_fx’|’pytorch_ipex’},
{‘onnxrt_integerops’|’onnxrt_qlinearops’}]. Default: ‘pytorch_fx’.
Consistent with Intel Neural Compressor.</p></li>
<li><p><strong>approach</strong> – str, ‘static’ or ‘dynamic’. Default to ‘static’.</p></li>
<li><p><strong>tuning_strategy</strong> – str, ‘bayesian’, ‘basic’, ‘mse’ or ‘sigopt’. Default to ‘bayesian’.</p></li>
<li><p><strong>relative_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 0.1 means that we accept a 10% increase in the metrics error.</p></li>
<li><p><strong>absolute_drop</strong> – Float, tolerable ralative accuracy drop. Default to None,
e.g. set to 5 means that we can only accept metrics smaller than 5.</p></li>
<li><p><strong>timeout</strong> – Tuning timeout (seconds). Default to 0, which means early stop.
Combine with max_trials field to decide when to exit.</p></li>
<li><p><strong>max_trials</strong> – Max tune times. Default to 1. Combine with timeout field to
decide when to exit. “timeout=0, max_trials=1” means it will try quantization
only once and return satisfying best model.</p></li>
<li><p><strong>sess_options</strong> – The session option for onnxruntime, only valid when
framework contains ‘onnxrt_integerops’ or ‘onnxrt_qlinearops’,
otherwise will be ignored.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantize_checkpoint_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<p>Please note that if you only want the pytorch model or onnx model
file, you can call .get_model() or .export_onnx_file(). The checkpoint
file generated by .save() method can only be used by .load().</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_file</strong> – The location you want to save the forecaster.</p></li>
<li><p><strong>quantize_checkpoint_file</strong> – The location you want to save quantized forecaster.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.to_local">
<span class="sig-name descname"><span class="pre">to_local</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.to_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a distributed forecaster to a local (non-distributed) one.</p>
<p>Common practice is to use distributed training (fit) and predict/
evaluate with onnx or other frameworks on a single node. To do so,
you need to call .to_local() and transform the forecaster to a non-
distributed one.</p>
<p>The optimizer is refreshed, incremental training after to_local
might have some problem.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a forecaster instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.tune">
<span class="sig-name descname"><span class="pre">tune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">direction</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_parallels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.tune" title="Permalink to this definition">¶</a></dt>
<dd><p>Search the hyper parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – train data, as numpy ndarray tuple (x, y)</p></li>
<li><p><strong>validation_data</strong> – validation data, as numpy ndarray tuple (x,y)</p></li>
<li><p><strong>target_metric</strong> – the target metric to optimize,
a string or an instance of torchmetrics.metric.Metric</p></li>
<li><p><strong>direction</strong> – in which direction to optimize the target metric,
“maximize” - larger the better
“minimize” - smaller the better</p></li>
<li><p><strong>n_trials</strong> – number of trials to run</p></li>
<li><p><strong>n_parallels</strong> – number of parallel processes used to run trials.
to use parallel tuning you need to use a RDB url for storage and specify study_name.
For more information, refer to Nano AutoML user guide.</p></li>
<li><p><strong>epochs</strong> – the number of epochs to run in each trial fit, defaults to 1</p></li>
<li><p><strong>batch_size</strong> – number of batch size for each trial fit, defaults to 32</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.from_tsdataset">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_tsdataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tsdataset</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">past_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_seq_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/nbeats_forecaster.html#NBeatsForecaster.from_tsdataset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.nbeats_forecaster.NBeatsForecaster.from_tsdataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a NBeats Forecaster Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tsdataset</strong> – A bigdl.chronos.data.tsdataset.TSDataset instance.</p></li>
<li><p><strong>past_seq_len</strong> – Specify the history time steps (i.e. lookback).
Do not specify the ‘past_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.</p></li>
<li><p><strong>future_seq_len</strong> – Specify the output time steps (i.e. horizon).
Do not specify the ‘future_seq_len’ if your tsdataset has called
the ‘TSDataset.roll’ method or ‘TSDataset.to_torch_data_loader’.</p></li>
<li><p><strong>kwargs</strong> – Specify parameters of Forecaster,
e.g. loss and optimizer, etc. More info,
please refer to NBeatsForecaster.__init__ methods.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A NBeats Forecaster Model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="tcmfforecaster">
<h2>TCMFForecaster<a class="headerlink" href="#tcmfforecaster" title="Permalink to this headline">¶</a></h2>
<p>Chronos TCMFForecaster provides an efficient way to forecast high dimensional time series.</p>
<p>TCMFForecaster is based on DeepGLO algorithm, which is a deep forecasting model which thinks globally and acts locally.
You can refer to <a class="reference external" href="https://arxiv.org/abs/1905.03806">the deepglo paper</a> for more details.</p>
<p>TCMFForecaster supports distributed training and inference. It is based on Orca PyTorch Estimator, which is an estimator to do PyTorch training/evaluation/prediction on Spark in a distributed fashion. Also you can choose to enable distributed training and inference or not.</p>
<p><strong>Remarks</strong>:</p>
<ul class="simple">
<li><p>You can refer to <a class="reference external" href="https://github.com/intel-analytics/BigDL/blob/main/docs/docs/Chronos/tutorials/TCMFForecaster.md#step-0-prepare-environment">TCMFForecaster installation</a> to install required packages.</p></li>
<li><p>Your operating system (OS) is required to be one of the following 64-bit systems: <strong>Ubuntu 16.04 or later</strong> and <strong>macOS 10.12.6 or later</strong>.</p></li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-5-5-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-5-5-0" name="5-0" role="tab" tabindex="0">PyTorch</button></div><div aria-labelledby="tab-5-5-0" class="sphinx-tabs-panel" id="panel-5-5-0" name="5-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tcmf_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tcmf_forecaster.</span></span><span class="sig-name descname"><span class="pre">TCMFForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vbsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hbsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[32,</span> <span class="pre">32,</span> <span class="pre">32,</span> <span class="pre">32,</span> <span class="pre">32,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_channels_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">16,</span> <span class="pre">1]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size_Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0005</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_time</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">svd</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.abstract.Forecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">TCMFForecaster</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fit_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">val_len</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
<span class="go">                   start_date=&quot;2020-1-1&quot;,</span>
<span class="go">                   freq=&quot;5min&quot;,</span>
<span class="go">                   y_iters=1,</span>
<span class="go">                   init_FX_epoch=1,</span>
<span class="go">                   max_FX_epoch=1,</span>
<span class="go">                   max_TCN_epoch=1,</span>
<span class="go">                   alt_iters=2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndarray_input</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;id&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">300</span><span class="p">),</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">480</span><span class="p">)}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ndarray_input</span><span class="p">,</span> <span class="n">fit_params</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">horizon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">horizon</span><span class="o">=</span><span class="n">horizon</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="n">tempdirname</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loaded_model</span> <span class="o">=</span> <span class="n">TCMFForecaster</span><span class="o">.</span><span class="n">load</span><span class="p">({</span><span class="n">tempdirname</span><span class="p">},</span> <span class="n">is_xshards_distributed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">horizon</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">target_value</span><span class="o">=</span><span class="nb">dict</span><span class="p">({</span><span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">data_new</span><span class="p">}),</span> <span class="n">metric</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mse&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit_incremental</span><span class="p">({</span><span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">data_new</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">yhat_incr</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">horizon</span><span class="o">=</span><span class="n">horizon</span><span class="p">)</span>
</pre></div>
</div>
<p>Build a TCMF Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>vbsize</strong> – int, default is 128.
Vertical batch size, which is the number of cells per batch.</p></li>
<li><p><strong>hbsize</strong> – int, default is 256.
Horizontal batch size, which is the number of time series per batch.</p></li>
<li><p><strong>num_channels_X</strong> – list, default=[32, 32, 32, 32, 32, 1].
List containing channel progression of temporal convolution network for local model</p></li>
<li><p><strong>num_channels_Y</strong> – list, default=[16, 16, 16, 16, 16, 1]
List containing channel progression of temporal convolution network for hybrid model.</p></li>
<li><p><strong>kernel_size</strong> – int, default is 7.
Kernel size for local models</p></li>
<li><p><strong>dropout</strong> – float, default is 0.1.
Dropout rate during training</p></li>
<li><p><strong>rank</strong> – int, default is 64.
The rank in matrix factorization of global model.</p></li>
<li><p><strong>kernel_size_Y</strong> – int, default is 7.
Kernel size of hybrid model</p></li>
<li><p><strong>learning_rate</strong> – float, default is 0.0005</p></li>
<li><p><strong>normalize</strong> – boolean, false by default.
Whether to normalize input data for training.</p></li>
<li><p><strong>use_time</strong> – boolean, default is True.
Whether to use time coveriates.</p></li>
<li><p><strong>svd</strong> – boolean, default is False.
Whether factor matrices are initialized by NMF</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">val_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">24</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_date</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'2020-4-1'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'1H'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dti</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">24</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_FX_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_FX_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_TCN_epoch</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alt_iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model on x from scratch</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – the input for fit. Only dict of ndarray and SparkXShards of dict of ndarray
are supported. Example: {‘id’: id_arr, ‘y’: data_ndarray}, and data_ndarray
is of shape (n, T), where n is the number f target time series and T is the
number of time steps.</p></li>
<li><p><strong>val_len</strong> – int, default is 24.
Validation length. We will use the last val_len time points as validation data.</p></li>
<li><p><strong>start_date</strong> – str or datetime-like.
Start date time for the time-series. e.g. “2020-01-01”</p></li>
<li><p><strong>freq</strong> – str or DateOffset, default is ‘H’
Frequency of data</p></li>
<li><p><strong>covariates</strong> – 2-D ndarray or None. The shape of ndarray should be (r, T), where r is
the number of covariates and T is the number of time points.
Global covariates for all time series. If None, only default time coveriates will be
used while use_time is True. If not, the time coveriates used is the stack of input
covariates and default time coveriates.</p></li>
<li><p><strong>dti</strong> – DatetimeIndex or None.
If None, use default fixed frequency DatetimeIndex generated with start_date and freq.</p></li>
<li><p><strong>period</strong> – int, default is 24.
Periodicity of input time series, leave it out if not known</p></li>
<li><p><strong>y_iters</strong> – int, default is 10.
Number of iterations while training the hybrid model.</p></li>
<li><p><strong>init_FX_epoch</strong> – int, default is 100.
Number of iterations while initializing factors</p></li>
<li><p><strong>max_FX_epoch</strong> – int, default is 300.
Max number of iterations while training factors.</p></li>
<li><p><strong>max_TCN_epoch</strong> – int, default is 300.
Max number of iterations while training the local model.</p></li>
<li><p><strong>alt_iters</strong> – int, default is 10.
Number of iterations while alternate training.</p></li>
<li><p><strong>num_workers</strong> – the number of workers you want to use for fit. If None, it defaults to
num_ray_nodes in the created OrcaRayContext or 1 if there is no active OrcaRayContext.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.fit_incremental">
<span class="sig-name descname"><span class="pre">fit_incremental</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x_incr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">covariates_incr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dti_incr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.fit_incremental"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.fit_incremental" title="Permalink to this definition">¶</a></dt>
<dd><p>Incrementally fit the model. Note that we only incrementally fit X_seq (TCN in global model)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x_incr</strong> – incremental data to be fitted. It should be of the same format as input x in
fit, which is a dict of ndarray or SparkXShards of dict of ndarray.
Example: {‘id’: id_arr, ‘y’: incr_ndarray}, and incr_ndarray is of shape (n, T_incr)
, where n is the number of target time series, T_incr is the number of time steps
incremented. You can choose not to input ‘id’ in x_incr, but if you do, the elements
of id in x_incr should be the same as id in x of fit.</p></li>
<li><p><strong>covariates_incr</strong> – covariates corresponding to x_incr. 2-D ndarray or None.
The shape of ndarray should be (r, T_incr), where r is the number of covariates.
Global covariates for all time series. If None, only default time coveriates will be
used while use_time is True. If not, the time coveriates used is the stack of input
covariates and default time coveriates.</p></li>
<li><p><strong>dti_incr</strong> – dti corresponding to the x_incr. DatetimeIndex or None.
If None, use default fixed frequency DatetimeIndex generated with the last date of x in
fit and freq.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_value</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mae']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_covariates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_dti</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_value</strong> – target value for evaluation. We interpret its second dimension of
as the horizon length for evaluation.</p></li>
<li><p><strong>metric</strong> – the metrics. A list of metric names.</p></li>
<li><p><strong>target_covariates</strong> – covariates corresponding to target_value.
2-D ndarray or None.
The shape of ndarray should be (r, horizon), where r is the number of covariates.
Global covariates for all time series. If None, only default time coveriates will be
used while use_time is True. If not, the time coveriates used is the stack of input
covariates and default time coveriates.</p></li>
<li><p><strong>target_dti</strong> – dti corresponding to target_value.
DatetimeIndex or None.
If None, use default fixed frequency DatetimeIndex generated with the last date of x in
fit and freq.</p></li>
<li><p><strong>num_workers</strong> – the number of workers to use in evaluate. If None, it defaults to
num_ray_nodes in the created OrcaRayContext or 1 if there is no active OrcaRayContext.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Each item represents a metric.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">horizon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">24</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_covariates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">future_dti</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using a trained forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>horizon</strong> – horizon length to look forward.</p></li>
<li><p><strong>future_covariates</strong> – covariates corresponding to future horizon steps data to predict.
2-D ndarray or None.
The shape of ndarray should be (r, horizon), where r is the number of covariates.
Global covariates for all time series. If None, only default time coveriates will be
used while use_time is True. If not, the time coveriates used is the stack of input
covariates and default time coveriates.</p></li>
<li><p><strong>future_dti</strong> – dti corresponding to future horizon steps data to predict.
DatetimeIndex or None.
If None, use default fixed frequency DatetimeIndex generated with the last date of x in
fit and freq.</p></li>
<li><p><strong>num_workers</strong> – the number of workers to use in predict. If None, it defaults to
num_ray_nodes in the created OrcaRayContext or 1 if there is no active OrcaRayContext.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy ndarray with shape of (nd, horizon), where nd is the same number
of time series as input x in fit_eval.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>path</strong> – Path to target saved file.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.is_xshards_distributed">
<span class="sig-name descname"><span class="pre">is_xshards_distributed</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.is_xshards_distributed"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.is_xshards_distributed" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether model is distributed by input xshards.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>True if the model is distributed by input xshards</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.load">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_xshards_distributed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minPartitions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tcmf_forecaster.html#TCMFForecaster.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tcmf_forecaster.TCMFForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load a saved model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>path</strong> – The location you want to save the forecaster.</p></li>
<li><p><strong>is_xshards_distributed</strong> – Whether the model is distributed trained with
input of dict of SparkXshards.</p></li>
<li><p><strong>minPartitions</strong> – The minimum partitions for the XShards.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the model loaded</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="mtnetforecaster">
<h2>MTNetForecaster<a class="headerlink" href="#mtnetforecaster" title="Permalink to this headline">¶</a></h2>
<p>MTNet is a memory-network based solution for multivariate time-series forecasting. In a specific task of multivariate time-series forecasting, we have several variables observed in time series and we want to forecast some or all of the variables’ value in a future time stamp.</p>
<p>MTNet is proposed by paper <a class="reference external" href="https://arxiv.org/abs/1809.02105">A Memory-Network Based Solution for Multivariate Time-Series Forecasting</a>.</p>
<p>For the detailed algorithm description, please refer to <a class="reference external" href="https://github.com/intel-analytics/BigDL/blob/main/docs/docs/Chronos/Algorithm/MTNetAlgorithm.md">here</a>.</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-6-6-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-6-6-0" name="6-0" role="tab" tabindex="0">Tensorflow</button></div><div aria-labelledby="tab-6-6-0" class="sphinx-tabs-panel" id="panel-6-6-0" name="6-0" role="tabpanel" tabindex="0"><span class="target" id="module-bigdl.chronos.forecaster.tf.mtnet_forecaster"></span><dl class="py class">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">bigdl.chronos.forecaster.tf.mtnet_forecaster.</span></span><span class="sig-name descname"><span class="pre">MTNetForecaster</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">long_series_num</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">series_length</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ar_window_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn_height</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn_hid_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnn_hid_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[16,</span> <span class="pre">32]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mae'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rnn_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean_squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">uncertainty</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">bigdl.chronos.forecaster.abstract.Forecaster</span></code></p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1">#The dataset is split into x_train, x_val, x_test, y_train, y_val, y_test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MTNetForecaster</span><span class="p">(</span><span class="n">target_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="go">                            feature_dim=x_train.shape[-1],</span>
<span class="go">                            long_series_num=6,</span>
<span class="go">                            series_length=2</span>
<span class="go">                            )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span>
<span class="go">              validation_data=(x_val, y_val),</span>
<span class="go">              epochs=2,</span>
<span class="go">              batch_size=32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<p>Build a MTNet Forecast Model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_dim</strong> – the dimension of model output</p></li>
<li><p><strong>feature_dim</strong> – the dimension of input feature</p></li>
<li><p><strong>long_series_num</strong> – the number of series for the long-term memory series</p></li>
<li><p><strong>series_length</strong> – the series size for long-term and short-term memory series</p></li>
<li><p><strong>ar_window_size</strong> – the auto regression window size in MTNet</p></li>
<li><p><strong>cnn_hid_size</strong> – the hidden layer unit for cnn in encoder</p></li>
<li><p><strong>rnn_hid_sizes</strong> – the hidden layers unit for rnn in encoder</p></li>
<li><p><strong>cnn_height</strong> – cnn filter height in MTNet</p></li>
<li><p><strong>metric</strong> – the metric for validation and evaluation</p></li>
<li><p><strong>uncertainty</strong> – whether to enable calculation of uncertainty</p></li>
<li><p><strong>lr</strong> – learning rate</p></li>
<li><p><strong>loss</strong> – the target function you want to optimize on</p></li>
<li><p><strong>cnn_dropout</strong> – the dropout possibility for cnn in encoder</p></li>
<li><p><strong>rnn_dropout</strong> – the dropout possibility for rnn in encoder</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster.fit"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.fit" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, (long_series_num+1)*series_length, feature_dim)</div>
<div class="line">y’s shape is (num_samples, target_dim)</div>
</div>
</p></li>
<li><p><strong>epochs</strong> – Number of epochs you want to train. The value defaults to 2.</p></li>
<li><p><strong>batch_size</strong> – Number of batch size you want to train. The value defaults to 32.</p></li>
<li><p><strong>validation_data</strong> – Data on which to evaluate the loss
and any model metrics at the end of each epoch.
The model will not be trained on this data.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.predict" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. data’s shape is (num_samples, (long_series_num+1)*series_length, feature_dim)</div>
</div>
</p></li>
<li><p><strong>batch_size</strong> – predict batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A numpy.ndarray with shape of (num_samples, feature_dum).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.evaluate">
<span class="sig-name descname"><span class="pre">evaluate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['mae']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raw_values'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster.evaluate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong> – <p>The data support following formats:</p>
<div class="line-block">
<div class="line">1. a numpy ndarray tuple (x, y):</div>
<div class="line">x’s shape is (num_samples, (long_series_num+1)*series_length, feature_dim)</div>
<div class="line">y’s shape is (num_samples, target_dim)</div>
</div>
</p></li>
<li><p><strong>metric</strong> – metric is the evaluation metric name to optimize, e.g. [“mae”].</p></li>
<li><p><strong>batch_size</strong> – evaluate batch size. The value will not affect evaluate
result but will affect resources cost(e.g. memory and time).</p></li>
<li><p><strong>multioutput</strong> – Defines aggregating of multiple output values.
String in [‘raw_values’, ‘uniform_average’]. The value defaults to
‘raw_values’.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of evaluation results. Calculation results for each metrics.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster.save"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint_file</strong> – The location you want to save the forecaster.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_file</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/bigdl/chronos/forecaster/tf/mtnet_forecaster.html#MTNetForecaster.load"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#bigdl.chronos.forecaster.tf.mtnet_forecaster.MTNetForecaster.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Load the forecaster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint_file</strong> – The checkpoint file location you want to load the forecaster.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div></div>
</section>
<section id="arimaforecaster">
<h2>ARIMAForecaster<a class="headerlink" href="#arimaforecaster" title="Permalink to this headline">¶</a></h2>
<p>AutoRegressive Integrated Moving Average (ARIMA) is a class of statistical models for analyzing and forecasting time series data. It consists of 3 components: AR (AutoRegressive), I (Integrated) and MA (Moving Average). In ARIMAForecaster we use the SARIMA model (Seasonal ARIMA), which is an extension of ARIMA that additionally supports the direct modeling of the seasonal component of the time series.</p>
</section>
<section id="prophetforecaster">
<h2>ProphetForecaster<a class="headerlink" href="#prophetforecaster" title="Permalink to this headline">¶</a></h2>
<p>Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.</p>
<p>For the detailed algorithm description, please refer to <a class="reference external" href="https://github.com/facebook/prophet">here</a>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="automodels.html" class="btn btn-neutral float-left" title="Auto Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="anomaly_detectors.html" class="btn btn-neutral float-right" title="Anomaly Detectors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, BigDL Authors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>