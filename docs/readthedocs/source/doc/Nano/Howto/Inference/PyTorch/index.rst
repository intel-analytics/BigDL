Inference Optimization: For PyTorch Users
=============================================

* `How to find accelerated method with minimal latency using InferenceOptimizer <inference_optimizer_optimize.html>`_
* `How to accelerate a PyTorch inference pipeline through ONNXRuntime <accelerate_pytorch_inference_onnx.html>`_
* `How to accelerate a PyTorch inference pipeline through OpenVINO <accelerate_pytorch_inference_openvino.html>`_
* `How to accelerate a PyTorch inference pipeline through JIT/IPEX <accelerate_pytorch_inference_jit_ipex.html>`_
* `How to quantize your PyTorch model in INT8 for inference using Intel Neural Compressor <quantize_pytorch_inference_inc.html>`_
* `How to quantize your PyTorch model in INT8 for inference using OpenVINO Post-training Optimization Tools <quantize_pytorch_inference_pot.html>`_
* `How to enable automatic context management for PyTorch inference on Nano optimized models <pytorch_context_manager.html>`_
* `How to save and load optimized ONNXRuntime model <pytorch_save_and_load_onnx.html>`_
* `How to save and load optimized OpenVINO model <pytorch_save_and_load_openvino.html>`_
* `How to save and load optimized JIT model <pytorch_save_and_load_jit.html>`_
* `How to save and load optimized IPEX model <pytorch_save_and_load_ipex.html>`_
* `How to accelerate a PyTorch inference pipeline through multiple instances <multi_instance_pytorch_inference.html>`_
* `How to accelerate a PyTorch inference pipeline using Intel ARC series dGPU <accelerate_pytorch_inference_gpu.html>`_
* `How to accelerate PyTorch inference using async multi-stage pipeline <accelerate_pytorch_inference_async_pipeline.html>`_