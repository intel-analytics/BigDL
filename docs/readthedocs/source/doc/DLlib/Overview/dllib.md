# DLlib User Guide

## 1. Overview

DLlib is a distributed deep learning library for Apache Spark; with DLlib, users can write their deep learning applications as standard Spark programs (using either Scala or Python APIs).

It includes the functionalities of the [original BigDL](https://github.com/intel-analytics/BigDL/tree/branch-0.14) project, and provides following high-level APIs for distributed deep learning on Spark:

* [Keras-like API](keras-api.md) 
* [Spark ML pipeline support](nnframes.md)

## 2. Scala user guide

### 2.1 Install

#### **Download a pre-built library**
You can download the bigdl-dllib build from the [Release Page](../release.md).

## **Link with a release version**

Currently, dllib releases are hosted on maven central; here's an example to add the dllib dependency to your own project:
```xml
<dependency>
    <groupId>com.intel.analytics.bigdl</groupId>
    <artifactId>bigdl-dllib-[spark_2.4.6|spark_3.1.1]</artifactId>
    <version>${BIGD_DLLIB_VERSION}</version>
</dependency>
```
Please choose the suffix according to your Spark platform.

SBT developers can use
```sbt
libraryDependencies += "com.intel.analytics.bigdl" % "bigdl-[spark_2.4.6|spark_3.1.1]" % "${BIGDL_DLLIB_VERSION}"
```

### 2.2 Run
#### **Set Environment Variables**
Set **BIGDL_HOME** and **SPARK_HOME**:

* If you download bigdl-dllib from the [Release Page](../release-download.md)
```bash
export SPARK_HOME=folder path where you extract the spark package
export BIGDL_HOME=folder path where you extract the bigdl package
```

* If you build bigdl-dllib by yourself
```bash
export SPARK_HOME=folder path where you extract the spark package
export BIGDL_HOME=the dist folder generated by the build process, which is under the top level of the source folder
```

---
#### **Use Interactive Spark Shell**
You can try bigdl-dllib easily using the Spark interactive shell. Run below command to start spark shell with BigDL support:
```bash
${SPARK_HOME}/bin/spark-shell \
  --properties-file ${BIGDL_HOME}/conf/spark-bigdl.conf \
  --jars ${BIGDL_HOME}/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --conf spark.driver.extraClassPath=${BIGDL_HOME}/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --conf spark.executor.extraClassPath=${BIGDL_HOME}/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --master local[*]
```
You will see a welcome message looking like below:
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Spark context available as sc.
scala>
```

To use BigDL, you should first initialize the engine as below.
```scala
scala> import com.intel.analytics.bigdl.dllib.utils.Engine
scala> Engine.init
```

Once the engine is successfully initiated, you'll be able to play with dllib API's.
For instance, to experiment with the ````Tensor```` APIs in dllib, you may try below code:
```scala
scala> import com.intel.analytics.bigdl.dllib.tensor.Tensor
import com.intel.analytics.bigdl.dllib.tensor.Tensor

scala> Tensor[Double](2,2).fill(1.0)
res9: com.intel.analytics.bigdl.dllib.tensor.Tensor[Double] =
1.0     1.0
1.0     1.0
[com.intel.analytics.bigdl.dllib.tensor.DenseTensor of size 2x2]
```

---

#### **Run as a Spark Program**
You can run a bigdl-dllib program, e.g., the [Image Inference](https://github.com/intel-analytics/BigDL/blob/branch-2.0/scala/dllib/src/main/scala/com/intel/analytics/bigdl/dllib/example/nnframes/imageInference), as a standard Spark program (running in either local mode or cluster mode) as follows:

1. Download the pretrained caffe model and prepare the images

2. Run the following command:
```bash
# Spark local mode
${SPARK_HOME}/bin/spark-submit.sh \
  --master local[2] \
  --driver-class-path dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --properties-file dist/conf/spark-bigdl.conf \
  --class com.intel.analytics.bigdl.dllib.examples.nnframes.imageInference.ImageInferenceExample \
  dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \   #change to your jar file if your download is not spark_2.4.3-0.14.0
  --caffeDefPath PROTOTXT_PATH \
  --caffeWeightsPath CAFFE_MODEL_PATH \
  --batchSize 32 \
  --imagePath IMAGE_PATH

# Spark standalone mode
## ${SPARK_HOME}/sbin/start-master.sh
## check master URL from http://localhost:8080
${SPARK_HOME}/bin/spark-submit.sh \
  --master spark://... \
  --driver-class-path dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --properties-file dist/conf/spark-bigdl.conf \
  --conf spark.driver.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --conf spark.executor.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
  --executor-cores cores_per_executor \
  --total-executor-cores total_cores_for_the_job \
  --class com.intel.analytics.bigdl.dllib.examples.nnframes.imageInference.ImageInferenceExample \
  dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \   #change to your jar file if your download is not spark_2.4.3-0.14.0
  --caffeDefPath PROTOTXT_PATH \
  --caffeWeightsPath CAFFE_MODEL_PATH \
  --batchSize 32 \
  --imagePath IMAGE_PATH

# Spark yarn client mode
${SPARK_HOME}/bin/spark-submit.sh \
 --master yarn \
 --deploy-mode client \
 --driver-class-path dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --properties-file dist/conf/spark-bigdl.conf \
 --conf spark.driver.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --conf spark.executor.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --executor-cores cores_per_executor \
 --num-executors executors_number \
 --class com.intel.analytics.bigdl.dllib.examples.nnframes.imageInference.ImageInferenceExample \
 dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --caffeDefPath PROTOTXT_PATH \
 --caffeWeightsPath CAFFE_MODEL_PATH \
 --batchSize 32 \
 --imagePath IMAGE_PATH

# Spark yarn cluster mode
${SPARK_HOME}/bin/spark-submit.sh \
 --master yarn \
 --deploy-mode cluster \
 --driver-class-path dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --properties-file dist/conf/spark-bigdl.conf \
 --conf spark.driver.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --conf spark.executor.extraClassPath=dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --executor-cores cores_per_executor \
 --num-executors executors_number \
 --class com.intel.analytics.bigdl.dllib.examples.nnframes.imageInference.ImageInferenceExample \
 dist/lib/bigdl-dllib-0.14.0-SNAPSHOT-jar-with-dependencies.jar \
 --caffeDefPath PROTOTXT_PATH \
 --caffeWeightsPath CAFFE_MODEL_PATH \
 --batchSize 32 \
 --imagePath IMAGE_PATH
```

  The parameters used in the above command are:

  * --caffeDefPath: The path where your put the pretrained caffe prototxt.

  * --caffeWeightsPath: The path where your put the pretrained caffe model.

  * -b: The mini-batch size. The mini-batch size is expected to be a multiple of *total cores* used in the job. In this example, the mini-batch size is suggested to be set to *total cores * 4*

  * --imagePath: The folder where you put the image files.

If you are to run your own program, do remember to create SparkContext and initialize the engine before call other bigdl-dllib API's, as shown below.
```scala
 // Scala code example
 val conf = Engine.createSparkConf()
 val sc = new SparkContext(conf)
 Engine.init
```
---

### 2.3 Get started (example)
---

This section is a short introduction of some classic examples/tutorials. They can give you a clear idea of how to build simple deep learning programs using BigDL. Besides these examples, BigDL also provides plenty of models ready for re-use and examples in both Scala and Python.

---
#### **Training LeNet on MNIST - The "hello world" for deep learning**
This tutorial is an explanation of what is happening in the [lenet](https://github.com/intel-analytics/BigDL/tree/branch-2.0/scala/dllib/src/main/scala/com/intel/analytics/bigdl/dllib/models/lenet) for details of how to run the example**

A bigdl-dllib program starts with `import com.intel.analytics.bigdl.dllib._`; it then _**creates the `SparkContext`**_ using the `SparkConf` returned by the `Engine`; after that, it _**initializes the `Engine`**_.
````scala
  val conf = Engine.createSparkConf()
      .setAppName("Train Lenet on MNIST")
      .set("spark.task.maxFailures", "1")
  val sc = new SparkContext(conf)
  Engine.init
````
````Engine.createSparkConf```` will return a ````SparkConf```` populated with some appropriate configuration. And ````Engine.init```` will verify and read some environment information(e.g. executor numbers and executor cores) from the ````SparkContext````.

After the initialization, we need to:

1._**Create the LeNet model**_ by calling the [````LeNet5()````](https://github.com/intel-analytics/BigDL/tree/branch-2.0/scala/dllib/src/main/scala/com/intel/analytics/bigdl/dllib/models/lenet/LeNet5.scala), which creates the LeNet-5 convolutional network model as follows:

````scala
    val model = Sequential()
    model.add(Reshape(Array(1, 28, 28)))
      .add(SpatialConvolution(1, 6, 5, 5))
      .add(Tanh())
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Tanh())
      .add(SpatialConvolution(6, 12, 5, 5))
      .add(SpatialMaxPooling(2, 2, 2, 2))
      .add(Reshape(Array(12 * 4 * 4)))
      .add(Linear(12 * 4 * 4, 100))
      .add(Tanh())
      .add(Linear(100, classNum))
      .add(LogSoftMax())
````
2.Load the data by _**creating the [```DataSet```](https://github.com/intel-analytics/BigDL/tree/branch-2.0/scala/dllib/src/main/scala/com/intel/analytics/bigdl/dllib/feature/dataset)**_ (e.g., ````SampleToGreyImg````, ````GreyImgNormalizer```` and ````GreyImgToBatch````):

````scala
    val trainSet = (if (sc.isDefined) {
        DataSet.array(load(trainData, trainLabel), sc.get, param.nodeNumber)
      } else {
        DataSet.array(load(trainData, trainLabel))
      }) -> SampleToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(
        param.batchSize)
````

After that, we _**create the [```Optimizer```](https://github.com/intel-analytics/BigDL/tree/branch-2.0/scala/dllib/src/main/scala/com/intel/analytics/bigdl/dllib/optim)**_ (either a distributed or local one depending on whether it runs on Spark or not) by specifying the ````DataSet````, the model and the ````Criterion```` (which, given input and target, computes gradient per given loss function):
````scala
  val optimizer = Optimizer(
    model = model,
    dataset = trainSet,
    criterion = ClassNLLCriterion[Float]())
````

Finally (after optionally specifying the validation data and methods for the ````Optimizer````), we _**train the model by calling ````Optimizer.optimize()````**_:
````scala
  optimizer
    .setValidation(
      trigger = Trigger.everyEpoch,
      dataset = validationSet,
      vMethods = Array(new Top1Accuracy))
    .setOptimMethod(new Adagrad(learningRate=0.01, learningRateDecay=0.0002))
    .setEndWhen(Trigger.maxEpoch(param.maxEpoch))
    .optimize()
````

---

## 3. Python user guide

### 3.1 Install

Run below command to install _bigdl-dllib_.

```bash
conda create -n my_env python=3.7
conda activate my_env
pip install bigdl-dllib
```

### 3.2 Run

#### **3.2.1 Interactive Shell**

You may test if the installation is successful using the interactive Python shell as follows:

* Type `python` in the command line to start a REPL.
* Try to run the example code below to verify the installation:

  ```python
  from bigdl.dllib.utils.nncontext import *

  sc = init_nncontext()  # Initiation of bigdl-dllib on the underlying cluster.
  ```

#### **3.2.2 Jupyter Notebook**

You can start the Jupyter notebook as you normally do using the following command and run Analytics Zoo programs directly in a Jupyter notebook:

```bash
jupyter notebook --notebook-dir=./ --ip=* --no-browser
```

#### **3.2.3 Python Script**

You can directly write bigdl-dlllib programs in a Python file (e.g. script.py) and run in the command line as a normal Python program:

```bash
python script.py
```

---
### 3.3 Get started (example)
---

#### **Text Classification using BigDL Python API**

This tutorial describes the [textclassifier]( https://github.com/intel-analytics/BigDL/tree/master/pyspark/bigdl/models/textclassifier) example written using BigDL Python API, which builds a text classifier using a CNN (convolutional neural network) or LSTM or GRU model (as specified by the user). (It was first described by [this Keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html))

The example first creates the `SparkContext` using the SparkConf` return by the `create_spark_conf()` method, and then initialize the engine:
```python
  sc = SparkContext(appName="text_classifier",
                    conf=create_spark_conf())
  init_engine()
```

It then loads the [20 Newsgroup dataset](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) into RDD, and transforms the input data into an RDD of `Sample`. (Each `Sample` in essence contains a tuple of two NumPy ndarray representing the feature and label).

```python
  texts = news20.get_news20()
  data_rdd = sc.parallelize(texts, 2)
  ...
  sample_rdd = vector_rdd.map(
      lambda (vectors, label): to_sample(vectors, label, embedding_dim))
  train_rdd, val_rdd = sample_rdd.randomSplit(
      [training_split, 1-training_split])
```

After that, the example creates the neural network model as follows:
```python
def build_model(class_num):
    model = Sequential()

    if model_type.lower() == "cnn":
        model.add(Reshape([embedding_dim, 1, sequence_len]))
        model.add(SpatialConvolution(embedding_dim, 128, 5, 1))
        model.add(ReLU())
        model.add(SpatialMaxPooling(5, 1, 5, 1))
        model.add(SpatialConvolution(128, 128, 5, 1))
        model.add(ReLU())
        model.add(SpatialMaxPooling(5, 1, 5, 1))
        model.add(Reshape([128]))
    elif model_type.lower() == "lstm":
        model.add(Recurrent()
                  .add(LSTM(embedding_dim, 128)))
        model.add(Select(2, -1))
    elif model_type.lower() == "gru":
        model.add(Recurrent()
                  .add(GRU(embedding_dim, 128)))
        model.add(Select(2, -1))
    else:
        raise ValueError('model can only be cnn, lstm, or gru')

    model.add(Linear(128, 100))
    model.add(Linear(100, class_num))
    model.add(LogSoftMax())
    return model
```
Finally the example creates the `Optimizer` (which accepts both the model and the training Sample RDD) and trains the model by calling `Optimizer.optimize()`:

```python
optimizer = Optimizer(
    model=build_model(news20.CLASS_NUM),
    training_rdd=train_rdd,
    criterion=ClassNLLCriterion(),
    end_trigger=MaxEpoch(max_epoch),
    batch_size=batch_size,
    optim_method=Adagrad())
...
train_model = optimizer.optimize()
```