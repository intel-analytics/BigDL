Abs.scala
object Abs {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Abs[T] = {
    new Abs[T]()
  }
}

()
Abs [] []
AbsCriterion.scala
object AbsCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
    sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : AbsCriterion[T] = {
    new AbsCriterion[T](sizeAverage)
  }
}

(
    sizeAverage: Boolean = true)
AbsCriterion ['size_average'] ['True']
Add.scala
object Add {
  def apply[@specialized(Float, Double) T: ClassTag](
    inputSize: Int)(implicit ev: TensorNumeric[T]) : Add[T] = {
    new Add[T](inputSize)
  }
}

(
    inputSize: Int)
Add ['input_size'] ['NoD']
AddConstant.scala
object AddConstant {
  def apply[@specialized(Float, Double) T: ClassTag](
    constant_scalar: T,
    inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : AddConstant[T] = {
    new AddConstant[T](constant_scalar, inplace)
  }
}

(
    constant_scalar: T,
    inplace: Boolean = false)
AddConstant ['constant_scalar', 'inplace'] ['NoD', 'False']
BCECriterion.scala
object BCECriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
    weights: Tensor[T] = null,
    sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : BCECriterion[T] = {
    new BCECriterion[T](weights, sizeAverage)
  }
}

(
    weights: Tensor[T] = null,
    sizeAverage: Boolean = true)
BCECriterion ['weights', 'size_average'] ['NoD', 'True']
BatchNormalization.scala
object BatchNormalization {
  def apply[@specialized(Float, Double) T: ClassTag](
    nOutput: Int,
    eps: Double = 1e-5,
    momentum: Double = 0.1,
    affine: Boolean = true)(implicit ev: TensorNumeric[T]): BatchNormalization[T] = {
    new BatchNormalization[T](nOutput, eps, momentum, affine)
  }
}

(
    nOutput: Int,
    eps: Double = 1e-5,
    momentum: Double = 0.1,
    affine: Boolean = true)
BatchNormalization ['n_output', 'eps', 'momentum', 'affine'] ['NoD', '1e-5', '0.1', 'True']
Bilinear.scala
object Bilinear {
  def apply[@specialized(Float, Double) T: ClassTag](
    inputSize1: Int,
    inputSize2: Int,
    outputSize: Int,
    biasRes: Boolean = true)(implicit ev: TensorNumeric[T]) : Bilinear[T] = {
    new Bilinear[T](inputSize1, inputSize2, outputSize, biasRes)
  }
}

(
    inputSize1: Int,
    inputSize2: Int,
    outputSize: Int,
    biasRes: Boolean = true)
Bilinear ['input_size1', 'input_size2', 'output_size', 'bias_res'] ['NoD', 'NoD', 'NoD', 'True']
Bottle.scala
object Bottle {
  def apply[@specialized(Float, Double) T: ClassTag](
    module: Module[T],
    nInputDim: Int = 2,
    nOutputDim1: Int = Int.MaxValue)(implicit ev: TensorNumeric[T]) : Bottle[T] = {
    new Bottle[T](module, nInputDim, nOutputDim1)
  }
}

(
    module: Module[T],
    nInputDim: Int = 2,
    nOutputDim1: Int = Int.MaxValue)
Bottle ['module', 'n_input_dim', 'n_output_dim1'] ['NoD', '2', 'Int.MaxValue']
CAdd.scala
object CAdd {
  def apply[@specialized(Float, Double) T: ClassTag](
    size: Array[Int]
  )(implicit ev: TensorNumeric[T]) : CAdd[T] = {
    new CAdd[T](size)
  }
}

(
    size: Array[Int]
  )
CAdd ['size'] ['NoD']
CAddTable.scala
object CAddTable {
  def apply[@specialized(Float, Double) T: ClassTag](
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : CAddTable[T] = {
    new CAddTable[T](inplace)
  }
}

(
      inplace: Boolean = false)
CAddTable ['inplace'] ['False']
CDivTable.scala
object CDivTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CDivTable[T] = {
    new CDivTable[T]()
  }
}

()
CDivTable [] []
CMaxTable.scala
object CMaxTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CMaxTable[T] = {
    new CMaxTable[T]()
  }
}

()
CMaxTable [] []
CMinTable.scala
object CMinTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CMinTable[T] = {
    new CMinTable[T]()
  }
}

()
CMinTable [] []
CMul.scala
object CMul {
  def apply[@specialized(Float, Double) T: ClassTag](
      size: Array[Int])(implicit ev: TensorNumeric[T]) : CMul[T] = {
    new CMul[T](size)
  }
}

(
      size: Array[Int])
CMul ['size'] ['NoD']
CMulTable.scala
object CMulTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CMulTable[T] = {
    new CMulTable[T]()
  }
}

()
CMulTable [] []
CSubTable.scala
object CSubTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CSubTable[T] = {
    new CSubTable[T]()
  }
}

()
CSubTable [] []
Clamp.scala
object Clamp {
  def apply[@specialized(Float, Double) T: ClassTag](
      min: Int,
      max: Int)(implicit ev: TensorNumeric[T]) : Clamp[T] = {
    new Clamp[T](min, max)
  }
}

(
      min: Int,
      max: Int)
Clamp ['min', 'max'] ['NoD', 'NoD']
ClassNLLCriterion.scala
object ClassNLLCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      weights: Tensor[T] = null,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : ClassNLLCriterion[T] = {
    new ClassNLLCriterion[T](weights, sizeAverage)
  }
}

(
      weights: Tensor[T] = null,
      sizeAverage: Boolean = true)
ClassNLLCriterion ['weights', 'size_average'] ['NoD', 'True']
ClassSimplexCriterion.scala
object ClassSimplexCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      nClasses: Int)(implicit ev: TensorNumeric[T]) : ClassSimplexCriterion[T] = {
    new ClassSimplexCriterion[T](nClasses)
  }
}

(
      nClasses: Int)
ClassSimplexCriterion ['n_classes'] ['NoD']
Concat.scala
object Concat {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int)(implicit ev: TensorNumeric[T]) : Concat[T] = {
    new Concat[T](dimension)
  }
}

(
      dimension: Int)
Concat ['dimension'] ['NoD']
ConcatTable.scala
object ConcatTable {
  def apply[A <: Activity : ClassTag, @specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : ConcatTable[T] = {
    new ConcatTable[T]()
  }
}

()
ConcatTable [] []
Container.scala
end of file scala/com/intel/analytics/bigdl/nn/Container.scala
Contiguous.scala
object Contiguous {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Contiguous[T] = {
    new Contiguous[T]()
  }
}

()
Contiguous [] []
Copy.scala
object Copy {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Copy[T] = {
    new Copy[T]()
  }
}

()
Copy [] []
Cosine.scala
object Cosine {
  def apply[@specialized(Float, Double) T: ClassTag](
      inputSize : Int,
      outputSize : Int)(implicit ev: TensorNumeric[T]) : Cosine[T] = {
    new Cosine[T](inputSize, outputSize)
  }
}

(
      inputSize : Int,
      outputSize : Int)
Cosine [] []
CosineDistance.scala
object CosineDistance {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : CosineDistance[T] = {
    new CosineDistance[T]()
  }
}

()
CosineDistance [] []
CosineEmbeddingCriterion.scala
object CosineEmbeddingCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      margin: Double = 0.0,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : CosineEmbeddingCriterion[T] = {
    new CosineEmbeddingCriterion[T](margin, sizeAverage)
  }
}

(
      margin: Double = 0.0,
      sizeAverage: Boolean = true)
CosineEmbeddingCriterion ['margin', 'size_average'] ['0.0', 'True']
CriterionTable.scala
object CriterionTable {
  def apply[@specialized(Float, Double) T: ClassTag](
      criterion: TensorCriterion[T])(implicit ev: TensorNumeric[T]) : CriterionTable[T] = {
    new CriterionTable[T](criterion)
  }
}

(
      criterion: TensorCriterion[T])
CriterionTable ['criterion'] ['NoD']
CrossEntropyCriterion.scala
object CrossEntropyCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      weights: Tensor[T] = null)
      (implicit ev: TensorNumeric[T]) : CrossEntropyCriterion[T] = {
    new CrossEntropyCriterion[T](weights)
  }
}

(
      weights: Tensor[T] = null)
CrossEntropyCriterion ['weights'] ['NoD']
DistKLDivCriterion.scala
object DistKLDivCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
    sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : DistKLDivCriterion[T] = {
    new DistKLDivCriterion[T](sizeAverage)
  }
}

(
    sizeAverage: Boolean = true)
DistKLDivCriterion ['size_average'] ['True']
DotProduct.scala
object DotProduct {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : DotProduct[T] = {
    new DotProduct[T]()
  }
}

()
DotProduct [] []
Dropout.scala
object Dropout {
  def apply[@specialized(Float, Double) T: ClassTag](
      initP: Double = 0.5,
      inplace: Boolean = false,
      scale: Boolean = true)(implicit ev: TensorNumeric[T]) : Dropout[T] = {
    new Dropout[T](initP, inplace, scale)
  }
}

(
      initP: Double = 0.5,
      inplace: Boolean = false,
      scale: Boolean = true)
Dropout ['init_p', 'inplace', 'scale'] ['0.5', 'False', 'True']
ELU.scala
object ELU {
  def apply[@specialized(Float, Double) T: ClassTag](
      alpha: Double = 1.0,
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : ELU[T] = {
    new ELU[T](alpha, inplace)
  }
}

(
      alpha: Double = 1.0,
      inplace: Boolean = false)
ELU ['alpha', 'inplace'] ['1.0', 'False']
Echo.scala
object Echo {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Echo[T] = {
    new Echo[T]()
  }
}

()
Echo [] []
ErrorInfo.scala
object ErrorInfo {
  final val constrainInputAsVectorOrBatch =
    """
      | The input to the layer needs to be a vector(or a mini-batch of vectors);
      | please use the Reshape module to convert multi-dimensional input into vectors
      | if appropriate"
    """.stripMargin

  final val constrainInputAs3DOrBatch =
    """
      | The input to the layer needs to be a 3D tensor(or a mini-batch of 3D tensors);
      | please use the Reshape module to convert multi-dimensional input into 3D tensors
      | if appropriate"
    """.stripMargin

  final val constrainEachInputAsVectorOrBatch =
    """
      | Each tensor contained in the input to the layer needs to be a vector
      | (or a mini-batch of vectors);\n please use the Reshape module to convert
      | multi-dimensional input into vectors if appropriate""".stripMargin

  final val constrainInputDimSameAsTarget =
    """
      | The dimensions of input and target to the criterion layer need to be the same;
      | please use the Reshape module to convert if appropriate""".stripMargin

  final val constrainInputSizeSameAsTarget =
    """
      | The size of input and target to the criterion layer need to be the same;
      | please use the Reshape module to convert if appropriate""".stripMargin

}

(or a mini-batch of 3D tensors)
ErrorInfo [] []
Euclidean.scala
object Euclidean {
  def apply[@specialized(Float, Double) T: ClassTag](
    inputSize: Int,
    outputSize: Int,
    fastBackward: Boolean = true)(implicit ev: TensorNumeric[T]) : Euclidean[T] = {
    new Euclidean[T](inputSize, outputSize, fastBackward)
  }
}

(
    inputSize: Int,
    outputSize: Int,
    fastBackward: Boolean = true)
Euclidean ['input_size', 'output_size', 'fast_backward'] ['NoD', 'NoD', 'True']
Exp.scala
object Exp {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Exp[T] = {
    new Exp[T]()
  }
}

()
Exp [] []
FlattenTable.scala
object FlattenTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : FlattenTable[T] = {
    new FlattenTable[T]()
  }
}

()
FlattenTable [] []
GradientReversal.scala
object GradientReversal {
  def apply[@specialized(Float, Double) T: ClassTag](
      lambda: Double = 1)(implicit ev: TensorNumeric[T]) : GradientReversal[T] = {
    new GradientReversal[T](lambda)
  }
}

(
      lambda: Double = 1)
GradientReversal ['lambda'] ['1']
HardShrink.scala
object HardShrink {
  def apply[@specialized(Float, Double) T: ClassTag](
      lambda: Double = 0.5)(implicit ev: TensorNumeric[T]) : HardShrink[T] = {
    new HardShrink[T]()
  }
}

(
      lambda: Double = 0.5)
HardShrink ['lambda'] ['0.5']
HardTanh.scala
object HardTanh {
  def apply[@specialized(Float, Double) T: ClassTag](
      minValue: Double = -1,
      maxValue: Double = 1,
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : HardTanh[T] = {
    new HardTanh[T](minValue, maxValue, inplace)
  }
}

(
      minValue: Double = -1,
      maxValue: Double = 1,
      inplace: Boolean = false)
HardTanh ['min_value', 'max_value', 'inplace'] ['-1', '1', 'False']
HingeEmbeddingCriterion.scala
object HingeEmbeddingCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      margin: Double = 1,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : HingeEmbeddingCriterion[T] = {
    new HingeEmbeddingCriterion[T](margin, sizeAverage)
  }
}

(
      margin: Double = 1,
      sizeAverage: Boolean = true)
HingeEmbeddingCriterion ['margin', 'size_average'] ['1', 'True']
Identity.scala
object Identity {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Identity[T] = {
    new Identity[T]()
  }
}

()
Identity [] []
Index.scala
object Index {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int)(implicit ev: TensorNumeric[T]) : Index[T] = {
    new Index[T](dimension)
  }
}

(
      dimension: Int)
Index ['dimension'] ['NoD']
InferReshape.scala
object InferReshape {
  def apply[@specialized(Float, Double) T: ClassTag](size: Array[Int], batchMode: Boolean = false)
    (implicit ev: TensorNumeric[T]): InferReshape[T] =
    new InferReshape(size, batchMode)
}

(size: Array[Int], batchMode: Boolean = false)
InferReshape ['size', 'batch_mode'] ['NoD', 'False']
InitializationMethod.scala
end of file scala/com/intel/analytics/bigdl/nn/InitializationMethod.scala
JoinTable.scala
object JoinTable {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int,
      nInputDims: Int)(implicit ev: TensorNumeric[T]) : JoinTable[T] = {
    new JoinTable[T](dimension, nInputDims)
  }
}

(
      dimension: Int,
      nInputDims: Int)
JoinTable ['dimension', 'n_input_dims'] ['NoD', 'NoD']
L1Cost.scala
object L1Cost {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : L1Cost[T] = {
    new L1Cost[T]()
  }
}

()
L1Cost [] []
L1HingeEmbeddingCriterion.scala
object L1HingeEmbeddingCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      margin: Double = 1)(implicit ev: TensorNumeric[T]) : L1HingeEmbeddingCriterion[T] = {
    new L1HingeEmbeddingCriterion[T](margin)
  }
}

(
      margin: Double = 1)
L1HingeEmbeddingCriterion ['margin'] ['1']
L1Penalty.scala
object L1Penalty {
  def apply[@specialized(Float, Double) T: ClassTag](
      l1weight: Int,
      sizeAverage: Boolean = false,
      provideOutput: Boolean = true)(implicit ev: TensorNumeric[T]) : L1Penalty[T] = {
    new L1Penalty[T](l1weight, sizeAverage, provideOutput)
  }
}

(
      l1weight: Int,
      sizeAverage: Boolean = false,
      provideOutput: Boolean = true)
L1Penalty ['l1weight', 'size_average', 'provide_output'] ['NoD', 'False', 'True']
LeakyReLU.scala
object LeakyReLU {
  def apply[@specialized(Float, Double) T: ClassTag](
      negval: Double = 0.01,
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : LeakyReLU[T] = {
    new LeakyReLU[T](negval, inplace)
  }
}

(
      negval: Double = 0.01,
      inplace: Boolean = false)
LeakyReLU ['negval', 'inplace'] ['0.01', 'False']
Linear.scala
object Linear {
  def apply[@specialized(Float, Double) T: ClassTag](
      inputSize: Int,
      outputSize: Int,
      initMethod: InitializationMethod = Default,
      withBias: Boolean = true
  )(implicit ev: TensorNumeric[T]) : Linear[T] = {
    new Linear[T](inputSize, outputSize, initMethod, withBias)
  }
}

(
      inputSize: Int,
      outputSize: Int,
      initMethod: InitializationMethod = Default,
      withBias: Boolean = true
  )
Linear ['input_size', 'output_size', 'init_method', 'with_bias'] ['NoD', 'NoD', 'default', 'True']
Log.scala
object Log {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Log[T] = {
    new Log[T]()
  }
}

()
Log [] []
LogSigmoid.scala
object LogSigmoid {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : LogSigmoid[T] = {
    new LogSigmoid[T]()
  }
}

()
LogSigmoid [] []
LogSoftMax.scala
object LogSoftMax {

  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : LogSoftMax[T] = {
    new LogSoftMax[T]()
  }
  private val A0 = 1.0
  private val A1 = 0.125
  private val A2 = 0.0078125
  private val A3 = 0.00032552083
  private val A4 = 1.0172526e-5

  def expMinusApprox(x: Double): Double = {
    if (x < 0) {
      return exp(-x)
    } else {
      var y = 0.0
      if (x < 13.0) {
        y = A0 + x * (A1 + x * (A2 + x * (A3 + x * A4)))
        y *= y
        y *= y
        y *= y
        y = 1 / y
        return y
      }
    }

    return 0.0
  }
}


()
LogSoftMax [] []
LookupTable.scala
object LookupTable {
  def apply[@specialized(Float, Double)T: ClassTag](
    nIndex: Int, nOutput: Int,
    paddingValue: Double = 0, maxNorm: Double = Double.MaxValue,
    normType: Double = 2.0, shouldScaleGradByFreq: Boolean = false)
   (implicit ev: TensorNumeric[T]): LookupTable[T] =
    new LookupTable[T](nIndex, nOutput, paddingValue, maxNorm, normType, shouldScaleGradByFreq)
}



(
    nIndex: Int, nOutput: Int,
    paddingValue: Double = 0, maxNorm: Double = Double.MaxValue,
    normType: Double = 2.0, shouldScaleGradByFreq: Boolean = false)
LookupTable ['n_index', 'n_output', 'padding_value', 'max_norm', 'norm_type', 'should_scale_grad_by_freq'] ['NoD', 'NoD', '0', 'Double.MaxValue', '2.0', 'False']
MM.scala
object MM {
  def apply[@specialized(Float, Double) T: ClassTag](
      transA: Boolean = false,
      transB: Boolean = false)(implicit ev: TensorNumeric[T]) : MM[T] = {
    new MM[T](transA, transB)
  }
}

(
      transA: Boolean = false,
      transB: Boolean = false)
MM ['trans_a', 'trans_b'] ['False', 'False']
MSECriterion.scala
object MSECriterion {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : MSECriterion[T] = {
    new MSECriterion[T]()
  }
}

()
MSECriterion [] []
MV.scala
object MV {
  def apply[@specialized(Float, Double) T: ClassTag](
      trans: Boolean = false)(implicit ev: TensorNumeric[T]) : MV[T] = {
    new MV[T](trans)
  }
}

(
      trans: Boolean = false)
MV ['trans'] ['False']
MapTable.scala
object MapTable {
  def apply[@specialized(Float, Double) T: ClassTag](
      module: AbstractModule[_ <: Activity, _ <: Activity, T] = null
  )(implicit ev: TensorNumeric[T]) : MapTable[T] = {
    new MapTable[T](module)
  }
}

(
      module: AbstractModule[_ <: Activity, _ <: Activity, T] = null
  )
MapTable ['module'] ['NoD']
MarginCriterion.scala
object MarginCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      margin: Double = 1.0,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : MarginCriterion[T] = {
    new MarginCriterion[T](margin, sizeAverage)
  }
}

(
      margin: Double = 1.0,
      sizeAverage: Boolean = true)
MarginCriterion ['margin', 'size_average'] ['1.0', 'True']
MarginRankingCriterion.scala
object MarginRankingCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      margin: Double = 1.0,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : MarginRankingCriterion[T] = {
    new MarginRankingCriterion[T](margin, sizeAverage)
  }
}

(
      margin: Double = 1.0,
      sizeAverage: Boolean = true)
MarginRankingCriterion ['margin', 'size_average'] ['1.0', 'True']
MaskedSelect.scala
object MaskedSelect {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : MaskedSelect[T] = {
    new MaskedSelect[T]()
  }
}

()
MaskedSelect [] []
Max.scala
object Max {
  def apply[T: ClassTag](
      dim : Int = 1,
      numInputDims: Int = Int.MinValue)(implicit ev: TensorNumeric[T]) : Max[T] = {
    new Max[T](dim, numInputDims)
  }
}

(implicit ev: TensorNumeric[T])
Max ['ev'] ['NoD']
Mean.scala
object Mean {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int = 1,
      nInputDims: Int = -1)(implicit ev: TensorNumeric[T]) : Mean[T] = {
    new Mean[T](dimension, nInputDims)
  }
}

(
      dimension: Int = 1,
      nInputDims: Int = -1)
Mean ['dimension', 'n_input_dims'] ['1', '-1']
Min.scala
object Min {
  def apply[@specialized(Float, Double) T: ClassTag](
      dim : Int = 1,
      numInputDims: Int = Int.MinValue)(implicit ev: TensorNumeric[T]) : Min[T] = {
    new Min[T](dim, numInputDims)
  }
}

(
      dim : Int = 1,
      numInputDims: Int = Int.MinValue)
Min ['num_input_dims'] ['Int.MinValue']
MixtureTable.scala
object MixtureTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : MixtureTable[T] = {
    new MixtureTable[T]()
  }
}

()
MixtureTable [] []
Module.scala
object Module {
  def load[T: ClassTag](path : String) : AbstractModule[Activity, Activity, T] = {
    File.load[AbstractModule[Activity, Activity, T]](path)
  }

  def loadTorch[T: ClassTag](path : String) : AbstractModule[Activity, Activity, T] = {
    File.loadTorch[AbstractModule[Activity, Activity, T]](path)
  }

  def loadCaffe[T: ClassTag](model: AbstractModule[Activity, Activity, T],
    defPath: String, modelPath: String, matchAll: Boolean = true)(
    implicit ev: TensorNumeric[T]): AbstractModule[Activity, Activity, T] = {
    CaffeLoader.load[T](model, defPath, modelPath, matchAll)
  }

  def flatten[@specialized(Float, Double) T: ClassTag](parameters: Array[Tensor[T]])(
    implicit ev: TensorNumeric[T]): Tensor[T] = {
    val compactedTensor = isCompact(parameters)
    if (compactedTensor != null) {
      return compactedTensor
    }
    var i = 0
    var length = 0
    while (i < parameters.length) {
      require(parameters(i).isContiguous())
      length += parameters(i).nElement()
      i += 1
    }

    val result = Tensor[T](length)
    val resultStorage = result.storage()

    i = 0
    var offset = 0
    while (i < parameters.length) {
      System.arraycopy(parameters(i).storage().array(), parameters(i).storageOffset() - 1,
        resultStorage.array(), offset, parameters(i).nElement())
      parameters(i).set(resultStorage, offset + 1, parameters(i).size(), parameters(i).stride())
      offset += parameters(i).nElement()
      i += 1
    }

    result
  }

  def isCompact[@specialized(Float, Double) T: ClassTag](paramters: Array[Tensor[T]])(
    implicit ev: TensorNumeric[T]): Tensor[T] = {
    require(paramters.length > 0)
    var i = 1
    val storage = paramters(0).storage()
    var length = paramters(0).nElement()
    while (i < paramters.length) {
      if (!storage.eq(paramters(i).storage())) {
        return null
      }
      length += paramters(i).nElement()
      i += 1
    }

    if (length != storage.array().length) {
      return null
    }

    return Tensor(storage)
  }
}

(path)
Module [] []
Mul.scala
object Mul {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Mul[T] = {
    new Mul[T]()
  }
}

()
Mul [] []
MulConstant.scala
object MulConstant {
  def apply[@specialized(Float, Double) T: ClassTag](
      scalar : T,
      inplace : Boolean = false)(implicit ev: TensorNumeric[T]) : MulConstant[T] = {
    new MulConstant[T](scalar, inplace)
  }
}

(
      scalar : T,
      inplace : Boolean = false)
MulConstant [] []
MultiCriterion.scala
object MultiCriterion {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : MultiCriterion[T] = {
    new MultiCriterion[T]()
  }
}

()
MultiCriterion [] []
MultiLabelMarginCriterion.scala
object MultiLabelMarginCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : MultiLabelMarginCriterion[T] = {
    new MultiLabelMarginCriterion[T](sizeAverage)
  }
}

(
      sizeAverage: Boolean = true)
MultiLabelMarginCriterion ['size_average'] ['True']
MultiLabelSoftMarginCriterion.scala
object MultiLabelSoftMarginCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      weights: Tensor[T] = null,
      sizeAverage: Boolean = true
  )(implicit ev: TensorNumeric[T]): MultiLabelSoftMarginCriterion[T] = {
    new MultiLabelSoftMarginCriterion[T](weights, sizeAverage)
  }
}

(
      weights: Tensor[T] = null,
      sizeAverage: Boolean = true
  )
MultiLabelSoftMarginCriterion ['weights', 'size_average'] ['NoD', 'True']
MultiMarginCriterion.scala
object MultiMarginCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      p: Int = 1,
      weights: Tensor[T] = null,
      margin: Double = 1.0,
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : MultiMarginCriterion[T] = {
    new MultiMarginCriterion[T](p, weights, margin, sizeAverage)
  }
}

(
      p: Int = 1,
      weights: Tensor[T] = null,
      margin: Double = 1.0,
      sizeAverage: Boolean = true)
MultiMarginCriterion ['p', 'weights', 'margin', 'size_average'] ['NoD', '1', '1.0', 'True']
NNPrimitive.scala
object NNPrimitive {
  def im2colDouble(
    fInput: Tensor[Double], input: Tensor[Double],
    kW: Int, kH: Int,
    dW: Int, dH: Int,
    padW: Int, padH: Int,
    nInputPlane: Int, inputWidth: Int, inputHeight: Int,
    outputWidth: Int, outputHeight: Int): Unit = {

    val inputData = input.storage().array()
    val fInputData = fInput.storage().array()

    var k = 0
    while (k < nInputPlane * kH * kW) {
      val nip = k / (kH * kW)
      val rest = k % (kH * kW)
      val kh = rest / kW
      val kw = rest % kW
      val dstOffset = k * outputHeight * outputWidth + fInput.storageOffset() - 1
      val srcOffset = nip * inputWidth * inputHeight + input.storageOffset() - 1
      if (padW > 0 || padH > 0) {
        var y = 0
        while (y < outputHeight) {
          val iy = y * dH - padH + kh
          if (iy < 0 || iy >= inputHeight) {
            util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
              dstOffset + (y + 1) * outputWidth, 0)
          } else {
            if (dW == 1) {
              val ix = 0 - padW + kw
              val lpad = Math.max(0, padW - kw)
              val rpad = Math.max(0, padW - (kW - kw - 1))
              if (outputWidth - rpad - lpad <= 0) {
                util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
                  dstOffset + (y + 1) * outputWidth, 0)
              } else {
                if (lpad > 0) util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
                  dstOffset + y * outputWidth + lpad, 0)
                System.arraycopy(inputData, srcOffset + iy * inputWidth + ix + lpad, fInputData,
                  dstOffset + y * outputWidth + lpad, outputWidth - rpad - lpad)
                if (rpad > 0) util.Arrays.fill(fInputData, dstOffset + (y + 1) * outputWidth - rpad,
                  dstOffset + (y + 1) * outputWidth, 0)
              }
            } else {
              var x = 0
              while (x < outputWidth) {
                val ix = x * dW - padW + kw
                if (ix < 0 || ix >= inputWidth) {
                  fInputData(dstOffset + y * outputWidth + x) = 0
                } else {
                  fInputData(dstOffset + y * outputWidth + x) =
                    inputData(srcOffset + iy * inputWidth + ix)
                }
                x += 1
              }
            }
          }
          y += 1
        }
      } else {
        var y = 0
        while (y < outputHeight) {
          val iy = y * dH + kh
          val ix = 0 + kw
          if (dW == 1) {
            System.arraycopy(inputData, srcOffset + iy * inputWidth + ix,
              fInputData, dstOffset + y * outputWidth, outputWidth)
          } else {
            var x = 0
            while (x < outputWidth) {
              fInputData(dstOffset + y * outputWidth + x) =
                inputData(srcOffset + iy * inputWidth + ix + x * dW)
              x += 1
            }
          }
          y += 1
        }
      }
      k += 1
    }
  }

  def im2colFloat(
    fInput: Tensor[Float], input: Tensor[Float],
    kW: Int, kH: Int,
    dW: Int, dH: Int,
    padW: Int, padH: Int,
    nInputPlane: Int, inputWidth: Int, inputHeight: Int,
    outputWidth: Int, outputHeight: Int): Unit = {

    val inputData = input.storage().array()
    val fInputData = fInput.storage().array()

    var k = 0
    while (k < nInputPlane * kH * kW) {
      val nip = k / (kH * kW)
      val rest = k % (kH * kW)
      val kh = rest / kW
      val kw = rest % kW
      val dstOffset = k * outputHeight * outputWidth + fInput.storageOffset() - 1
      val srcOffset = nip * inputWidth * inputHeight + input.storageOffset() - 1
      if (padW > 0 || padH > 0) {
        var y = 0
        while (y < outputHeight) {
          val iy = y * dH - padH + kh
          if (iy < 0 || iy >= inputHeight) {
            util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
              dstOffset + (y + 1) * outputWidth, 0)
          } else {
            if (dW == 1) {
              val ix = 0 - padW + kw
              val lpad = Math.max(0, padW - kw)
              val rpad = Math.max(0, padW - (kW - kw - 1))
              if (outputWidth - rpad - lpad <= 0) {
                util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
                  dstOffset + (y + 1) * outputWidth, 0)
              } else {
                if (lpad > 0) util.Arrays.fill(fInputData, dstOffset + y * outputWidth,
                  dstOffset + y * outputWidth + lpad, 0)
                System.arraycopy(inputData, srcOffset + iy * inputWidth + ix + lpad, fInputData,
                  dstOffset + y * outputWidth + lpad, outputWidth - rpad - lpad)
                if (rpad > 0) util.Arrays.fill(fInputData, dstOffset + (y + 1) * outputWidth - rpad,
                  dstOffset + (y + 1) * outputWidth, 0)
              }
            } else {
              var x = 0
              while (x < outputWidth) {
                val ix = x * dW - padW + kw
                if (ix < 0 || ix >= inputWidth) {
                  fInputData(dstOffset + y * outputWidth + x) = 0
                } else {
                  fInputData(dstOffset + y * outputWidth + x) =
                    inputData(srcOffset + iy * inputWidth + ix)
                }
                x += 1
              }
            }
          }
          y += 1
        }
      } else {
        var y = 0
        while (y < outputHeight) {
          val iy = y * dH + kh
          val ix = 0 + kw
          if (dW == 1) {
            System.arraycopy(inputData, srcOffset + iy * inputWidth + ix,
              fInputData, dstOffset + y * outputWidth, outputWidth)
          } else {
            var x = 0
            while (x < outputWidth) {
              fInputData(dstOffset + y * outputWidth + x) =
                inputData(srcOffset + iy * inputWidth + ix + x * dW)
              x += 1
            }
          }
          y += 1
        }
      }
      k += 1
    }
  }

  def col2imDouble(
    fInput: Tensor[Double], input: Tensor[Double],
    kW: Int, kH: Int,
    dW: Int, dH: Int,
    padW: Int, padH: Int,
    nInputPlane: Int,
    inputWidth: Int, inputHeight: Int,
    outputWidth: Int, outputHeight: Int
  ): Unit = {

    val inputData = input.storage().array()
    val fInputData = fInput.storage().array()
    var nPlane = 0
    while (nPlane < nInputPlane) {
      var kh = 0
      while (kh < kH) {
        var kw = 0
        while (kw < kW) {
          val srcOffset = nPlane * (kH * kW * outputHeight * outputWidth) +
            kh * (kW * outputHeight * outputWidth) +
            kw * (outputHeight * outputWidth) + fInput.storageOffset() - 1
          val dstOffset = nPlane * (inputHeight * inputWidth) + input.storageOffset() - 1
          if (padW > 0 || padH > 0) {
            var y = 0
            while (y < outputHeight) {
              val iy = y * dH - padH + kh
              if (iy >= 0 && iy < inputHeight) {
                if (dW == 1) {
                  val ix = 0 - padW + kw
                  val lPad = Math.max(0, padW - kw)
                  val rPad = Math.max(0, padW - (kW - kw - 1))
                  val inputDataOffset = dstOffset + iy * inputWidth + ix + lPad
                  val fInputDataOffset = srcOffset + y * outputWidth + lPad
                  val n = outputWidth - lPad - rPad
                  var i = 0
                  while (i < n) {
                    inputData(inputDataOffset + i) += fInputData(fInputDataOffset + i)
                    i += 1
                  }
                } else {
                  var x = 0
                  while (x < outputWidth) {
                    val ix = x * dW - padW + kw
                    if (ix >= 0 && ix < inputWidth) {
                      inputData(dstOffset + iy * inputWidth + ix) +=
                        fInputData(srcOffset + y * outputWidth + x)
                    }
                    x += 1
                  }
                }
              }
              y += 1
            }
          } else {
            var y = 0
            while (y < outputHeight) {
              val iy = y * dH + kh
              val ix = 0 + kw
              if (dW == 1) {
                var i = 0
                val inputDataOffset = dstOffset + iy * inputWidth + ix
                val fInputDataOffset = srcOffset + y * outputWidth
                while (i < outputWidth) {
                  inputData(inputDataOffset + i) += fInputData(fInputDataOffset + i)
                  i += 1
                }
              } else {
                var x = 0
                while (x < outputWidth) {
                  inputData(dstOffset + iy * inputWidth + ix + x * dW) +=
                    fInputData(srcOffset + y * outputWidth + x)
                  x += 1
                }
              }
              y += 1
            }
          }
          kw += 1
        }
        kh += 1
      }
      nPlane += 1
    }
  }

  def col2imFloat(
    fInput: Tensor[Float], input: Tensor[Float],
    kW: Int, kH: Int,
    dW: Int, dH: Int,
    padW: Int, padH: Int,
    nInputPlane: Int,
    inputWidth: Int, inputHeight: Int,
    outputWidth: Int, outputHeight: Int
  ): Unit = {

    val inputData = input.storage().array()
    val fInputData = fInput.storage().array()
    var nPlane = 0
    while (nPlane < nInputPlane) {
      var kh = 0
      while (kh < kH) {
        var kw = 0
        while (kw < kW) {
          val srcOffset = nPlane * (kH * kW * outputHeight * outputWidth) + kh *
            (kW * outputHeight * outputWidth) +
            kw * (outputHeight * outputWidth) + fInput.storageOffset() - 1
          val dstOffset = nPlane * (inputHeight * inputWidth) + input.storageOffset() - 1
          if (padW > 0 || padH > 0) {
            var y = 0
            while (y < outputHeight) {
              val iy = y * dH - padH + kh
              if (iy >= 0 && iy < inputHeight) {
                if (dW == 1) {
                  val ix = 0 - padW + kw
                  val lPad = Math.max(0, padW - kw)
                  val rPad = Math.max(0, padW - (kW - kw - 1))
                  val inputDataOffset = dstOffset + iy * inputWidth + ix + lPad
                  val fInputDataOffset = srcOffset + y * outputWidth + lPad
                  val n = outputWidth - lPad - rPad
                  var i = 0
                  while (i < n) {
                    inputData(inputDataOffset + i) += fInputData(fInputDataOffset + i)
                    i += 1
                  }
                } else {
                  var x = 0
                  while (x < outputWidth) {
                    val ix = x * dW - padW + kw
                    if (ix >= 0 && ix < inputWidth) {
                      inputData(dstOffset + iy * inputWidth + ix) +=
                        fInputData(srcOffset + y * outputWidth + x)
                    }
                    x += 1
                  }
                }
              }
              y += 1
            }
          } else {
            var y = 0
            while (y < outputHeight) {
              val iy = y * dH + kh
              val ix = 0 + kw
              if (dW == 1) {
                var i = 0
                val inputDataOffset = dstOffset + iy * inputWidth + ix
                val fInputDataOffset = srcOffset + y * outputWidth
                while (i < outputWidth) {
                  inputData(inputDataOffset + i) += fInputData(fInputDataOffset + i)
                  i += 1
                }
              } else {
                var x = 0
                while (x < outputWidth) {
                  inputData(dstOffset + iy * inputWidth + ix + x * dW) +=
                    fInputData(srcOffset + y * outputWidth + x)
                  x += 1
                }
              }
              y += 1
            }
          }
          kw += 1
        }
        kh += 1
      }
      nPlane += 1
    }
  }

  def maxPoolingForwardDouble(
    input: Array[Double], inputOffset: Int,
    output: Array[Double], outputOffset: Int,
    indices: Array[Double], indicesOffset: Int,
    nSlices: Int, iWidth: Int, iHeight: Int, oWidth: Int, oHeight: Int,
    kW: Int, kH: Int, dW: Int, dH: Int, padW: Int, padH: Int) {

    val slices = Range(0, nSlices).iterator
    while (slices.hasNext) {
      val k = slices.next()
      var i = 0
      while (i < oHeight) {
        var j = 0
        while (j < oWidth) {
          // k, i, j output indexers
          var hstart = i * dH - padH
          var wstart = j * dW - padW
          val hend = math.min(hstart + kH, iHeight)
          val wend = math.min(wstart + kW, iWidth)
          hstart = math.max(hstart, 0)
          wstart = math.max(wstart, 0)

          var maxindex = 0  // default is 0
          var maxval = Double.MinValue
          var tcntr = 0
          var y = hstart
          while (y < hend) {
            var x = wstart
            while (x < wend) {
              // k, y, x input indexers
              tcntr = y * iWidth + x
              val value = input(tcntr + inputOffset + k * iWidth * iHeight)
              if (value > maxval) {
                maxval = value
                maxindex = tcntr
              }
              x += 1
            }
            y += 1
          }
          output(outputOffset + k * oWidth * oHeight + i * oWidth + j) = maxval
          indices(indicesOffset + k * oWidth * oHeight + i * oWidth + j) = maxindex + 1
          j += 1
        }
        i += 1
      }
    }
  }

  def maxPoolingForwardFloat(
    input: Array[Float], inputOffset: Int,
    output: Array[Float], outputOffset: Int,
    indices: Array[Float], indicesOffset: Int,
    nSlices: Int, iWidth: Int, iHeight: Int, oWidth: Int, oHeight: Int,
    kW: Int, kH: Int, dW: Int, dH: Int, padW: Int, padH: Int) {

    val slices = Range(0, nSlices).iterator
    while (slices.hasNext) {
      val k = slices.next()
      var i = 0
      while (i < oHeight) {
        var j = 0
        while (j < oWidth) {
          // k, i, j output indexers
          var hstart = i * dH - padH
          var wstart = j * dW - padW
          val hend = math.min(hstart + kH, iHeight)
          val wend = math.min(wstart + kW, iWidth)
          hstart = math.max(hstart, 0)
          wstart = math.max(wstart, 0)

          var maxindex = 0  // default is 0
          var maxval = Float.MinValue
          var tcntr = 0
          var y = hstart
          while (y < hend) {
            var x = wstart
            while (x < wend) {
              // k, y, x input indexers
              tcntr = y * iWidth + x
              val value = input(tcntr + inputOffset + k * iWidth * iHeight)
              if (value > maxval) {
                maxval = value
                maxindex = tcntr
              }
              x += 1
            }
            y += 1
          }
          output(outputOffset + k * oWidth * oHeight + i * oWidth + j) = maxval
          indices(indicesOffset + k * oWidth * oHeight + i * oWidth + j) = maxindex + 1
          j += 1
        }
        i += 1
      }
    }
  }

  def maxPoolingBackwardFloat(
    gradInput: Array[Float], gradInputOffset: Int,
    gradOutput: Array[Float], gradOutputOffset: Int,
    indices: Array[Float], indicesOffset: Int,
    nSlices: Int, iwidth: Int, iheight: Int, owidth: Int, oheight: Int): Unit = {
    val slices = Range(0, nSlices).iterator
    while (slices.hasNext) {
      val k = slices.next()
      var i = 0
      while (i < oheight) {
        var j = 0
        while (j < owidth) {
          val maxp = indices(i * owidth + j + indicesOffset + k * owidth * oheight).toInt - 1
          gradInput(maxp + k * iwidth * iheight + gradInputOffset) +=
            gradOutput(gradOutputOffset + k * owidth * oheight + i * owidth + j)
          j += 1
        }
        i += 1
      }
    }
  }

  def maxPoolingBackwardDouble(
    gradInput: Array[Double], gradInputOffset: Int,
    gradOutput: Array[Double], gradOutputOffset: Int,
    indices: Array[Double], indicesOffset: Int,
    nSlices: Int, iwidth: Int, iheight: Int, owidth: Int, oheight: Int): Unit = {
    val slices = Range(0, nSlices).iterator
    while (slices.hasNext) {
      val k = slices.next()
      var i = 0
      while (i < oheight) {
        var j = 0
        while (j < owidth) {
          val maxp = indices(i * owidth + j + indicesOffset + k * owidth * oheight).toInt - 1
          gradInput(maxp + k * iwidth * iheight + gradInputOffset) += gradOutput(gradOutputOffset
            + k * owidth * oheight + i * owidth + j)
          j += 1
        }
        i += 1
      }
    }
  }

  // For SpatialFullConvolution
  def col2imWithDilationDouble(columns : Tensor[Double], image : Tensor[Double],
    channels : Int, height : Int, width : Int,
    kernelH : Int, kernelW : Int,
    padH : Int, padW : Int,
    strideH : Int, strideW : Int,
    dilationH : Int, dilationW : Int) {

    val dataIm = image.storage().array()
    val dataImOffset = image.storageOffset() - 1
    val dataCol = columns.storage().array()
    val dataColOffset = columns.storageOffset() - 1

    val heightCol = (height + 2 * padH -
      (dilationH * (kernelH - 1) + 1)) / strideH + 1
    val widthCol = (width + 2 * padW -
      (dilationW * (kernelW - 1) + 1)) / strideW + 1
    val channelsCol = channels * kernelH * kernelW
    var cCol = 0
    while (cCol < channelsCol) {
      val wOffset = cCol % kernelW
      val hOffset = (cCol / kernelW) % kernelH
      val cIm = cCol / kernelH / kernelW
      var hCol = 0
      while (hCol < heightCol) {
        var wCol = 0
        while (wCol < widthCol) {
          val hIm = hCol * strideH - padH + hOffset * dilationH
          val wIm = wCol * strideW - padW + wOffset * dilationW
          if (hIm >= 0 && hIm < height && wIm >= 0 && wIm < width) {
            dataIm((cIm * height + hIm) * width + wIm + dataImOffset) +=
              dataCol((cCol * heightCol + hCol) * widthCol + wCol + dataColOffset)
          }
          wCol += 1
        }
        hCol += 1
      }
      cCol += 1
    }
  }

  def col2imWithDilationFloat(columns : Tensor[Float], image : Tensor[Float],
    channels : Int, height : Int, width : Int,
    kernelH : Int, kernelW : Int,
    padH : Int, padW : Int,
    strideH : Int, strideW : Int,
    dilationH : Int, dilationW : Int) {

    val dataIm = image.storage().array()
    val dataImOffset = image.storageOffset() - 1
    val dataCol = columns.storage().array()
    val dataColOffset = columns.storageOffset() - 1

    val heightCol = (height + 2 * padH -
      (dilationH * (kernelH - 1) + 1)) / strideH + 1
    val widthCol = (width + 2 * padW -
      (dilationW * (kernelW - 1) + 1)) / strideW + 1
    val channelsCol = channels * kernelH * kernelW
    var cCol = 0
    while (cCol < channelsCol) {
      val wOffset = cCol % kernelW
      val hOffset = (cCol / kernelW) % kernelH
      val cIm = cCol / kernelH / kernelW
      var hCol = 0
      while (hCol < heightCol) {
        var wCol = 0
        while (wCol < widthCol) {
          val hIm = hCol * strideH - padH + hOffset * dilationH
          val wIm = wCol * strideW - padW + wOffset * dilationW
          if (hIm >= 0 && hIm < height && wIm >= 0 && wIm < width) {
            dataIm((cIm * height + hIm) * width + wIm + dataImOffset) +=
              dataCol((cCol * heightCol + hCol) * widthCol + wCol + dataColOffset)
          }
          wCol += 1
        }
        hCol += 1
      }
      cCol += 1
    }
  }

  def im2colWithDilationDouble(image: Tensor[Double], columns: Tensor[Double],
    channels : Int, height : Int, width : Int,
    kernelH : Int, kernelW : Int,
    padH : Int, padW : Int,
    strideH : Int, strideW : Int,
    dilationH : Int, dilationW : Int): Unit = {

    val dataIm = image.storage().array()
    val dataImOffset = image.storageOffset() - 1
    val dataCol = columns.storage().array()
    val dataColOffset = columns.storageOffset() - 1

    val heightCol = (height + 2 * padH -
      (dilationH * (kernelH - 1) + 1)) / strideH + 1
    val widthCol = (width + 2 * padW -
      (dilationW * (kernelW - 1) + 1)) / strideW + 1
    val channelsCol = channels * kernelH * kernelW
    var cCol = 0
    while (cCol < channelsCol) {
      val wOffset = cCol % kernelW
      val hOffset = (cCol / kernelW) % kernelH
      val cIm = cCol / kernelH / kernelW
      var hCol = 0
      while (hCol < heightCol) {
        var wCol = 0
        while (wCol < widthCol) {
          val hIm = hCol * strideH - padH + hOffset * dilationH
          val wIm = wCol * strideW - padW + wOffset * dilationW
          dataCol((cCol * heightCol + hCol) * widthCol + wCol + dataColOffset) =
            if (hIm >= 0 && wIm >= 0 && hIm < height && wIm < width) {
              dataIm((cIm * height + hIm) * width + wIm + dataImOffset)
            }
            else {
              0
            }
          wCol += 1
        }
        hCol += 1
      }
      cCol += 1
    }
  }

  def im2colWithDilationFloat(image: Tensor[Float], columns: Tensor[Float],
    channels : Int, height : Int, width : Int,
    kernelH : Int, kernelW : Int,
    padH : Int, padW : Int,
    strideH : Int, strideW : Int,
    dilationH : Int, dilationW : Int): Unit = {

    val dataIm = image.storage().array()
    val dataImOffset = image.storageOffset() - 1
    val dataCol = columns.storage().array()
    val dataColOffset = columns.storageOffset() - 1

    val heightCol = (height + 2 * padH -
      (dilationH * (kernelH - 1) + 1)) / strideH + 1
    val widthCol = (width + 2 * padW -
      (dilationW * (kernelW - 1) + 1)) / strideW + 1
    val channelsCol = channels * kernelH * kernelW
    var cCol = 0
    while (cCol < channelsCol) {
      val wOffset = cCol % kernelW
      val hOffset = (cCol / kernelW) % kernelH
      val cIm = cCol / kernelH / kernelW
      var hCol = 0
      while (hCol < heightCol) {
        var wCol = 0
        while (wCol < widthCol) {
          val hIm = hCol * strideH - padH + hOffset * dilationH
          val wIm = wCol * strideW - padW + wOffset * dilationW
          dataCol((cCol * heightCol + hCol) * widthCol + wCol + dataColOffset) =
            if (hIm >= 0 && wIm >= 0 && hIm < height && wIm < width) {
              dataIm((cIm * height + hIm) * width + wIm + dataImOffset)
            }
            else {
              0
            }
          wCol += 1
        }
        hCol += 1
      }
      cCol += 1
    }
  }
}

()
NNPrimitive [] []
Narrow.scala
object Narrow {
  def apply[@specialized(Float, Double) T: ClassTag](
    dimension: Int,
    offset: Int,
    length: Int = 1)(implicit ev: TensorNumeric[T]) : Narrow[T] = {
    new Narrow[T](dimension, offset, length)
  }
}

(
    dimension: Int,
    offset: Int,
    length: Int = 1)
Narrow ['dimension', 'offset', 'length'] ['NoD', 'NoD', '1']
NarrowTable.scala
object NarrowTable {
  def apply[@specialized(Float, Double) T: ClassTag](
      offset: Int,
      length: Int = 1)(implicit ev: TensorNumeric[T]) : NarrowTable[T] = {
    new NarrowTable[T](offset, length)
  }
}

(
      offset: Int,
      length: Int = 1)
NarrowTable ['offset', 'length'] ['NoD', '1']
Nms.scala
end of file scala/com/intel/analytics/bigdl/nn/Nms.scala
Normalize.scala
object Normalize {
  def apply[@specialized(Float, Double) T: ClassTag](
    p: Double,
    eps: Double = 1e-10)(implicit ev: TensorNumeric[T]) : Normalize[T] = {
    new Normalize(p, eps)
  }
}

(
    p: Double,
    eps: Double = 1e-10)
Normalize ['p', 'eps'] ['NoD', '1e-10']
PReLU.scala
object PReLU {
  def apply[@specialized(Float, Double) T: ClassTag](
      nOutputPlane: Int = 0)(implicit ev: TensorNumeric[T]) : PReLU[T] = {
    new PReLU[T](nOutputPlane)
  }
}

(
      nOutputPlane: Int = 0)
PReLU ['n_output_plane'] ['0']
Padding.scala
object Padding{
  def apply[@specialized(Float, Double) T: ClassTag](
    dim: Int,
    pad: Int,
    nInputDim: Int,
    value: Double = 0.0,
    nIndex: Int = 1)(implicit ev: TensorNumeric[T]) : Padding[T] =
    new Padding[T](dim, pad, nInputDim, value, nIndex)
}

(
    dim: Int,
    pad: Int,
    nInputDim: Int,
    value: Double = 0.0,
    nIndex: Int = 1)
Padding{ ['dim', 'pad', 'n_input_dim', 'value', 'n_index'] ['NoD', 'NoD', 'NoD', '0.0', '1']
PairwiseDistance.scala
object PairwiseDistance {
  def apply[@specialized(Float, Double) T: ClassTag](
      norm : Int = 2)(implicit ev: TensorNumeric[T]) : PairwiseDistance[T] = {
    new PairwiseDistance[T](norm)
  }
}

(
      norm : Int = 2)
PairwiseDistance [] []
ParallelCriterion.scala
object ParallelCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      repeatTarget: Boolean = false)(implicit ev: TensorNumeric[T]) : ParallelCriterion[T] = {
    new ParallelCriterion[T](repeatTarget)
  }
}

(
      repeatTarget: Boolean = false)
ParallelCriterion ['repeat_target'] ['False']
ParallelTable.scala
object ParallelTable {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : ParallelTable[T] = {
    new ParallelTable[T]()
  }
}

()
ParallelTable [] []
Power.scala
object Power {
  def apply[@specialized(Float, Double) T: ClassTag](
      power: Double,
      scale : Double = 1,
      shift : Double = 0)(implicit ev: TensorNumeric[T]) : Power[T] = {
    new Power[T](power, scale, shift)
  }
}

(
      power: Double,
      scale : Double = 1,
      shift : Double = 0)
Power ['power'] ['NoD']
RNN.scala
object RnnCell {
  def apply[@specialized(Float, Double) T: ClassTag](
    inputSize: Int = 4,
    hiddenSize: Int = 3)
   (implicit ev: TensorNumeric[T]) : RnnCell[T] = {
    new RnnCell[T](inputSize, hiddenSize)
  }
}

(
    inputSize: Int = 4,
    hiddenSize: Int = 3)
RnnCell ['input_size', 'hidden_size'] ['4', '3']
RReLU.scala
object RReLU {
  def apply[@specialized(Float, Double) T: ClassTag](
      lower: Double = 1.0/8,
      upper: Double = 1.0/3,
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : RReLU[T] = {
    new RReLU[T](lower, upper, inplace)
  }
}

(
      lower: Double = 1.0/8,
      upper: Double = 1.0/3,
      inplace: Boolean = false)
RReLU ['lower', 'upper', 'inplace'] ['1.0/8', '1.0/3', 'False']
ReLU.scala
object ReLU {
  def apply[@specialized(Float, Double) T: ClassTag](
      ip: Boolean = false)(implicit ev: TensorNumeric[T]) : ReLU[T] = {
    new ReLU[T](ip)
  }
}

(
      ip: Boolean = false)
ReLU ['ip'] ['False']
ReLU6.scala
object ReLU6 {
  def apply[@specialized(Float, Double) T: ClassTag](
      inplace: Boolean = false)(implicit ev: TensorNumeric[T]) : ReLU6[T] = {
    new ReLU6[T]()
  }
}

(
      inplace: Boolean = false)
ReLU6 ['inplace'] ['False']
Recurrent.scala
object Recurrent {
  def apply[@specialized(Float, Double) T: ClassTag](
    hiddenSize: Int = 3,
    bpttTruncate: Int = 2)
    (implicit ev: TensorNumeric[T]) : Recurrent[T] = {
    new Recurrent[T](hiddenSize, bpttTruncate)
  }
}

(
    hiddenSize: Int = 3,
    bpttTruncate: Int = 2)
Recurrent ['hidden_size', 'bptt_truncate'] ['3', '2']
Replicate.scala
object Replicate {
  def apply[@specialized(Float, Double) T: ClassTag](
      nFeatures : Int,
      dim : Int = 1,
      nDim : Int = Int.MaxValue)(implicit ev: TensorNumeric[T]) : Replicate[T] = {
    new Replicate[T](nFeatures, dim, nDim)
  }
}

(
      nFeatures : Int,
      dim : Int = 1,
      nDim : Int = Int.MaxValue)
Replicate [] []
Reshape.scala
object Reshape {
  def apply[@specialized(Float, Double) T: ClassTag](
      size: Array[Int],
      batchMode: Option[Boolean] = None)(implicit ev: TensorNumeric[T]) : Reshape[T] = {
    new Reshape[T](size, batchMode)
  }
}

(
      size: Array[Int],
      batchMode: Option[Boolean] = None)
Reshape ['size', 'batch_mode'] ['NoD', 'NoD']
RoiPooling.scala
object RoiPooling {
  def apply[@specialized(Float, Double) T: ClassTag](
    pooled_w: Int, pooled_h: Int, spatial_scale: T)(implicit ev: TensorNumeric[T]): RoiPooling[T] =
    new RoiPooling[T](pooled_w, pooled_h, spatial_scale)
}

(
    pooled_w: Int, pooled_h: Int, spatial_scale: T)
RoiPooling ['pooled_w', 'pooled_h', 'spatial_scale'] ['NoD', 'NoD', 'NoD']
Scale.scala
object Scale {
  def apply[@specialized(Float, Double) T: ClassTag](size: Array[Int])
    (implicit ev: TensorNumeric[T]): Scale[T] = new Scale[T](size)
}

(size: Array[Int])
Scale ['size'] ['NoD']
Select.scala
object Select {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int,
      index: Int)(implicit ev: TensorNumeric[T]) : Select[T] = {
    new Select[T](dimension, index)
  }
}

(
      dimension: Int,
      index: Int)
Select ['dimension', 'index'] ['NoD', 'NoD']
SelectTable.scala
end of file scala/com/intel/analytics/bigdl/nn/SelectTable.scala
Sequential.scala
object Sequential {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Sequential[T] = {
    new Sequential[T]()
  }
}

()
Sequential [] []
Sigmoid.scala
object Sigmoid {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Sigmoid[T] = {
    new Sigmoid[T]()
  }
}

()
Sigmoid [] []
SmoothL1Criterion.scala
object SmoothL1Criterion {
  def apply[@specialized(Float, Double) T: ClassTag](
      sizeAverage: Boolean = true)(implicit ev: TensorNumeric[T]) : SmoothL1Criterion[T] = {
    new SmoothL1Criterion[T](sizeAverage)
  }
}

(
      sizeAverage: Boolean = true)
SmoothL1Criterion ['size_average'] ['True']
SmoothL1CriterionWithWeights.scala
object SmoothL1CriterionWithWeights {
  def apply[@specialized(Float, Double) T: ClassTag](sigma: Double, num: Int = 0)
    (implicit ev: TensorNumeric[T]): SmoothL1CriterionWithWeights[T] =
    new SmoothL1CriterionWithWeights[T](sigma, num)
}

(sigma: Double, num: Int = 0)
SmoothL1CriterionWithWeights ['sigma', 'num'] ['NoD', '0']
SoftMarginCriterion.scala
end of file scala/com/intel/analytics/bigdl/nn/SoftMarginCriterion.scala
SoftMax.scala
object SoftMax{

  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : SoftMax[T] = {
    new SoftMax[T]()
  }
  // Notice: SoftMin will call this function
  private[nn] def updateOutput[T: ClassTag](input: Tensor[T], output: Tensor[T],
    results: Array[Future[Unit]]) (implicit ev: TensorNumeric[T]): Tensor[T] = {

    val (nFrame, dim, stride) = if (input.nDimension() == 1) {
      (1, input.size(1), 1)
    } else if (input.nDimension() == 2) {
      (input.size(1), input.size(2), 1)
    } else if (input.nDimension() == 3) {
      (1, input.size(1), input.size(2) * input.size(3))
    } else {
      (input.size(1), input.size(2), input.size(3) * input.size(4))
    }

    val outputArray = output.storage().array()
    val inputArray = if (input.isContiguous()) {
      input.storage().array()
    } else {
      input.contiguous().storage().array()
    }

    var t = 0
    while (t < stride * nFrame) {
      val _t = t
      results(_t) = Engine.model.invoke(() => {
        val inputOffset = (_t / stride) * dim * stride + _t % stride
        val outputOffset = (_t / stride) * dim * stride + _t % stride

        var inputMax = ev.fromType[Float](Float.MinValue)

        var d = 0
        while (d < dim) {
          if (ev.isGreater(inputArray(d * stride + inputOffset), inputMax)) {
            inputMax = inputArray(d * stride + inputOffset)
          }
          d += 1
        }

        var sum = ev.fromType[Int](0)
        d = 0
        while (d < dim) {
          val z = ev.exp(ev.minus(inputArray(d * stride + inputOffset), inputMax))
          outputArray(d * stride + outputOffset) = z
          sum = ev.plus(sum, z)
          d += 1
        }

        d = 0
        while (d < dim) {
          outputArray(d * stride + outputOffset) =
            ev.times(outputArray(d * stride + outputOffset), ev.divide(ev.fromType[Int](1), sum))
          d += 1
        }
      })

      t += 1
    }
    Engine.model.sync(results)

    output
  }

  private[nn] def updateGradInput[T: ClassTag](input: Tensor[T], gradOutput: Tensor[T],
    gradInput: Tensor[T], output: Tensor[T],
    results: Array[Future[Unit]])(implicit ev: TensorNumeric[T]): Tensor[T] = {

    require(input.size().deep == gradOutput.size().deep,
      "input should have the same size with gradOutput")
    val (nFrame, dim, stride) = if (output.nDimension() == 1) {
      (1, output.size(1), 1)
    } else if (output.nDimension() == 2) {
      (output.size(1), output.size(2), 1)
    } else if (output.nDimension() == 3) {
      (1, output.size(1), output.size(2) * output.size(3))
    } else {
      (output.size(1), output.size(2), output.size(3) * output.size(4))
    }

    val gradInputArray = gradInput.storage().array()
    val outputArray = if (output.isContiguous()) {
      output.storage().array()
    } else {
      output.contiguous().storage().array()
    }
    val gradOutputArray = if (gradOutput.isContiguous()) {
      gradOutput.storage().array()
    } else {
      gradOutput.contiguous().storage().array()
    }

    var t = 0
    while (t < stride * nFrame) {
      val _t = t
      results(_t) = Engine.model.invoke(() => {
        val gradInputOffset = (_t / stride) * dim * stride + _t % stride
        val outputOffset = (_t / stride) * dim * stride + _t % stride
        val gradOutputOffset = (_t / stride) * dim * stride + _t % stride

        var sum = ev.fromType[Int](0)
        var d = 0
        while (d < dim) {
          sum = ev.plus(sum, ev.times(gradOutputArray(d * stride + gradOutputOffset),
            outputArray(d * stride + outputOffset)))
          d += 1
        }

        d = 0
        while (d < dim) {
          gradInputArray(d * stride + gradInputOffset) =
            ev.times(outputArray(d * stride + outputOffset),
              ev.minus(gradOutputArray(d * stride + gradOutputOffset), sum))
          d += 1
        }
      })

      t += 1
    }

    Engine.model.sync(results)

    gradInput
  }
}

()
SoftMax{ [] []
SoftMin.scala
object SoftMin {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : SoftMin[T] = {
    new SoftMin[T]()
  }
}



()
SoftMin [] []
SoftPlus.scala
object SoftPlus {
  def apply[@specialized(Float, Double) T: ClassTag](
      beta: Double = 1.0)(implicit ev: TensorNumeric[T]) : SoftPlus[T] = {
    new SoftPlus[T](beta)
  }
}

(
      beta: Double = 1.0)
SoftPlus ['beta'] ['1.0']
SoftShrink.scala
object SoftShrink {
  def apply[@specialized(Float, Double) T: ClassTag](
      lambda: Double = 0.5)(implicit ev: TensorNumeric[T]) : SoftShrink[T] = {
    new SoftShrink[T](lambda)
  }
}

(
      lambda: Double = 0.5)
SoftShrink ['lambda'] ['0.5']
SoftSign.scala
object SoftSign {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : SoftSign[T] = {
    new SoftSign[T]()
  }
}

()
SoftSign [] []
SoftmaxWithCriterion.scala
object NormMode extends Enumeration {
  type NormMode = Value
  val FULL, VALID, BATCH_SIZE, NONE = Value
}

object SoftmaxWithCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](ignoreLabel: Option[Int] = None,
    normalizeMode: NormMode = NormMode.VALID)
    (implicit ev: TensorNumeric[T]): SoftmaxWithCriterion[T] =
    new SoftmaxWithCriterion[T](ignoreLabel, normalizeMode)
}

(ignoreLabel: Option[Int] = None,
    normalizeMode: NormMode = NormMode.VALID)
NormMode ['ignore_label', 'normalize_mode'] ['NoD', 'NormMode.VALID']
SpatialAveragePooling.scala
object SpatialAveragePooling {
  def apply[@specialized(Float, Double) T: ClassTag](
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      ceilMode: Boolean = false,
      countIncludePad: Boolean = true,
      divide: Boolean = true)(implicit ev: TensorNumeric[T]) : SpatialAveragePooling[T] = {
    new SpatialAveragePooling[T](kW, kH, dW, dH, padW, padH, ceilMode, countIncludePad, divide)
  }
}

(
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      ceilMode: Boolean = false,
      countIncludePad: Boolean = true,
      divide: Boolean = true)
SpatialAveragePooling ['kw', 'kh', 'dw', 'dh', 'pad_w', 'pad_h', 'ceil_mode', 'count_include_pad', 'divide'] ['NoD', 'NoD', '1', '1', '0', '0', 'False', 'True', 'True']
SpatialBatchNormalization.scala
object SpatialBatchNormalization {
  def apply[@specialized(Float, Double) T: ClassTag](
      nOutput: Int,
      eps: Double = 1e-5,
      momentum: Double = 0.1,
      affine: Boolean = true)(implicit ev: TensorNumeric[T]) : SpatialBatchNormalization[T] = {
    new SpatialBatchNormalization[T](nOutput, eps, momentum, affine)
  }
}

(
      nOutput: Int,
      eps: Double = 1e-5,
      momentum: Double = 0.1,
      affine: Boolean = true)
SpatialBatchNormalization ['n_output', 'eps', 'momentum', 'affine'] ['NoD', '1e-5', '0.1', 'True']
SpatialContrastiveNormalization.scala
object SpatialContrastiveNormalization {
  def apply[@specialized(Float, Double) T: ClassTag](
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null,
      threshold: Double = 1e-4,
      thresval: Double = 1e-4)(
      implicit ev: TensorNumeric[T]) : SpatialContrastiveNormalization[T] = {
    new SpatialContrastiveNormalization[T](nInputPlane, kernel, threshold, thresval)
  }
}

(
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null,
      threshold: Double = 1e-4,
      thresval: Double = 1e-4)
SpatialContrastiveNormalization ['n_input_plane', 'kernel', 'threshold', 'thresval'] ['NoD', '1', '1e-4', '1e-4']
SpatialConvolution.scala
object SpatialConvolution {
  def apply[@specialized(Float, Double) T: ClassTag](
      nInputPlane: Int,
      nOutputPlane: Int,
      kernelW: Int,
      kernelH: Int,
      strideW: Int = 1,
      strideH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      nGroup: Int = 1,
      propagateBack: Boolean = true,
      initMethod: InitializationMethod = Default
  )(implicit ev: TensorNumeric[T]): SpatialConvolution[T] = {
    new SpatialConvolution[T](nInputPlane, nOutputPlane, kernelW, kernelH,
      strideW, strideH, padW, padH, nGroup, propagateBack, initMethod)
  }
}

(
      nInputPlane: Int,
      nOutputPlane: Int,
      kernelW: Int,
      kernelH: Int,
      strideW: Int = 1,
      strideH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      nGroup: Int = 1,
      propagateBack: Boolean = true,
      initMethod: InitializationMethod = Default
  )
SpatialConvolution ['n_input_plane', 'n_output_plane', 'kernel_w', 'kernel_h', 'stride_w', 'stride_h', 'pad_w', 'pad_h', 'n_group', 'propagate_back', 'init_method'] ['NoD', 'NoD', 'NoD', 'NoD', '1', '1', '0', '0', '1', 'True', 'default']
SpatialConvolutionMap.scala
object SpatialConvolutionMap {

  def apply[@specialized(Float, Double) T: ClassTag](
      connTable: Tensor[T],
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0)(implicit ev: TensorNumeric[T]) : SpatialConvolutionMap[T] = {
    new SpatialConvolutionMap[T](connTable, kW, kH, dW, dH, padW, padH)
  }

  def full[@specialized(Float, Double) T: ClassTag](nin: Int, nout: Int)(
    implicit ev: TensorNumeric[T]): Tensor[T] = {
    val ft = Tensor[T](nin * nout, 2)
    var p = 1
    var i = 1
    var j = 1
    while (j <= nout) {
      while (i <= nin) {
        ft(p)(1) = ev.fromType[Int](i)
        ft(p)(2) = ev.fromType[Int](j)
        p = p + 1
        i = i + 1
      }
      j = j + 1
    }
    ft
  }

  def oneToOne[@specialized(Float, Double) T: ClassTag](nfeat: Int)(
    implicit ev: TensorNumeric[T]): Tensor[T] = {
    val ft = Tensor[T](nfeat, 2)
    var i = 1
    while (i <= nfeat) {
      ft(i)(1) = ev.fromType[Int](i)
      ft(i)(2) = ev.fromType[Int](i)
      i = i + 1
    }
    ft
  }

  def random[@specialized(Float, Double) T: ClassTag](nin: Int, nout: Int, nto: Int)(
    implicit ev: TensorNumeric[T]): Tensor[T] = {
    val nker = nto * nout
    val tbl = Tensor[T](nker, 2)
    val fi = Tensor.randperm[T](nin)
    var frcntr = 1
    val nfi = Math.floor(nin / nto).toInt // number of distinct nto chunks
    val totbl = tbl.select(2, 2)
    val frtbl = tbl.select(2, 1)
    val fitbl = fi.narrow(1, 1, nfi * nto) // part of fi that covers distinct chunks
    val ufrtbl = frtbl.unfold(1, nto, nto)
    val utotbl = totbl.unfold(1, nto, nto)
    val ufitbl = fitbl.unfold(1, nto, nto)

    // start filling frtbl
    var i = 1
    while (i <= nout) {
      // fro each unit in target map
      ufrtbl.select(1, i).copy(ufitbl.select(1, frcntr))
      frcntr = frcntr + 1
      if (frcntr - 1 == nfi) {
        fi.copy(Tensor.randperm[T](nin))
        frcntr = 1
      }
      i = i + 1
    }
    var tocntr = 1
    while (tocntr <= utotbl.size(1)) {
      utotbl.select(1, tocntr).fill(ev.fromType[Int](tocntr))
      tocntr = tocntr + 1
    }

    tbl
  }
}

(
      connTable: Tensor[T],
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0)
SpatialConvolutionMap ['conn_table', 'kw', 'kh', 'dw', 'dh', 'pad_w', 'pad_h'] ['NoD', 'NoD', 'NoD', '1', '1', '0', '0']
SpatialCrossMapLRN.scala
object SpatialCrossMapLRN {

  def apply[@specialized(Float, Double) T: ClassTag](
      size: Int = 5,
      alpha: Double = 1.0,
      beta: Double = 0.75,
      k: Double = 1.0)(implicit ev: TensorNumeric[T]) : SpatialCrossMapLRN[T] = {
    new SpatialCrossMapLRN[T](size, alpha, beta, k)
  }

  private def forwardFrame[T](input: Tensor[T], output: Tensor[T],
    scale: Tensor[T], alpha: Double, size: Int, beta: Double, k: Double)
    (implicit ev: TensorNumeric[T]): Unit = {
    val channels = input.size(1)

    val inputSquare = output
    inputSquare.pow(input, ev.fromType(2))
    val prePad = (size - 1) / 2 + 1
    val prePadCrop = if (prePad > channels) channels else prePad
    val scaleFirst = scale.select(1, 1).zero()

    var c = 1
    while (c <= prePadCrop) {
      scaleFirst.add(inputSquare.select(1, c))
      c += 1
    }

    c = 2
    while (c <= channels) {
      val scalePrevious = scale.select(1, c - 1)
      val scaleCurrent = scale.select(1, c)
      scaleCurrent.copy(scalePrevious)
      if (c < channels - prePad + 2) {
        val squareNext = inputSquare.select(1, c + prePad - 1)
        scaleCurrent.add(ev.fromType(1), squareNext)
      }
      if (c > prePad) {
        val squarePrevious = inputSquare.select(1, c - prePad)
        scaleCurrent.add(ev.fromType(-1), squarePrevious)
      }
      c += 1
    }

    scale.mul(ev.fromType(alpha / size)).add(ev.fromType(k))
    output.pow(scale, ev.fromType(-beta))
    output.cmul(input)
  }

  private def backwardFrame[T](
    input: Tensor[T], output: Tensor[T], scale: Tensor[T],
    gradOutput: Tensor[T], gradInput: Tensor[T], paddedRatio: Tensor[T],
    accumRatio: Tensor[T], alpha: Double, size: Int, beta: Double)
    (implicit ev: TensorNumeric[T]): Unit = {

    val channels = input.size(1)
    val inversePrePad = size - (size - 1) / 2
    val cacheRatioValue = ev.fromType(-2 * alpha * beta / size)

    gradInput.pow(scale, ev.fromType(-beta)).cmul(gradOutput)
    paddedRatio.zero()
    val paddedRatioCenter = paddedRatio.narrow(1, inversePrePad, channels)
    paddedRatioCenter.cmul(gradOutput, output).cdiv(scale)
    accumRatio.sum(paddedRatio.narrow(1, 1, size - 1), 1)
    var c = 1
    while (c <= channels) {
      accumRatio.add(paddedRatio.select(1, c + size - 1))
      gradInput.select(1, c).addcmul(cacheRatioValue, input.select(1, c), accumRatio)
      accumRatio.add(ev.fromType(-1), paddedRatio.select(1, c))
      c += 1
    }
  }
}

(
      size: Int = 5,
      alpha: Double = 1.0,
      beta: Double = 0.75,
      k: Double = 1.0)
SpatialCrossMapLRN ['size', 'alpha', 'beta', 'k'] ['5', '1.0', '0.75', '1.0']
SpatialDilatedConvolution.scala
object SpatialDilatedConvolution {
  def apply[@specialized(Float, Double) T: ClassTag](
      nInputPlane: Int,
      nOutputPlane: Int,
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      dilationW: Int = 1,
      dilationH: Int = 1,
      initMethod: InitializationMethod = Default
  )(implicit ev: TensorNumeric[T]) : SpatialDilatedConvolution[T] = {
    new SpatialDilatedConvolution[T](nInputPlane, nOutputPlane, kW, kH, dW, dH,
      padW, padH, dilationW, dilationH, initMethod)
  }
}

(
      nInputPlane: Int,
      nOutputPlane: Int,
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      dilationW: Int = 1,
      dilationH: Int = 1,
      initMethod: InitializationMethod = Default
  )
SpatialDilatedConvolution ['n_input_plane', 'n_output_plane', 'kw', 'kh', 'dw', 'dh', 'pad_w', 'pad_h', 'dilation_w', 'dilation_h', 'init_method'] ['NoD', 'NoD', 'NoD', 'NoD', '1', '1', '0', '0', '1', '1', 'default']
SpatialDivisiveNormalization.scala
object SpatialDivisiveNormalization {
  def apply[@specialized(Float, Double) T: ClassTag](
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null,
      threshold: Double = 1e-4,
      thresval: Double = 1e-4)(
      implicit ev: TensorNumeric[T]) : SpatialDivisiveNormalization[T] = {
    new SpatialDivisiveNormalization[T](nInputPlane, kernel, threshold, thresval)
  }
}

(
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null,
      threshold: Double = 1e-4,
      thresval: Double = 1e-4)
SpatialDivisiveNormalization ['n_input_plane', 'kernel', 'threshold', 'thresval'] ['NoD', '1', '1e-4', '1e-4']
SpatialFullConvolution.scala
object SpatialFullConvolution {
  def apply[A <: Activity : ClassTag, @specialized(Float, Double) T: ClassTag](
      nInputPlane: Int,
      nOutputPlane: Int,
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      adjW: Int = 0,
      adjH: Int = 0,
      nGroup: Int = 1,
      noBias: Boolean = false,
      initMethod: InitializationMethod = Default
  )(implicit ev: TensorNumeric[T]) : SpatialFullConvolution[A, T] = {
    new SpatialFullConvolution[A, T](nInputPlane, nOutputPlane, kW, kH, dW, dH,
      padW, padH, adjW, adjH, nGroup, noBias, initMethod)
  }
}

(
      nInputPlane: Int,
      nOutputPlane: Int,
      kW: Int,
      kH: Int,
      dW: Int = 1,
      dH: Int = 1,
      padW: Int = 0,
      padH: Int = 0,
      adjW: Int = 0,
      adjH: Int = 0,
      nGroup: Int = 1,
      noBias: Boolean = false,
      initMethod: InitializationMethod = Default
  )
SpatialFullConvolution ['n_input_plane', 'n_output_plane', 'kw', 'kh', 'dw', 'dh', 'pad_w', 'pad_h', 'adj_w', 'adj_h', 'n_group', 'no_bias', 'init_method'] ['NoD', 'NoD', 'NoD', 'NoD', '1', '1', '0', '0', '0', '0', '1', 'False', 'default']
SpatialMaxPooling.scala
object SpatialMaxPooling {
  def apply[@specialized(Float, Double) T: ClassTag](
      kW: Int,
      kH: Int,
      dW: Int,
      dH: Int,
      padW: Int = 0,
      padH: Int = 0)(implicit ev: TensorNumeric[T]): SpatialMaxPooling[T] = {
    new SpatialMaxPooling[T](kW, kH, dW, dH, padW, padH)
  }
}

(
      kW: Int,
      kH: Int,
      dW: Int,
      dH: Int,
      padW: Int = 0,
      padH: Int = 0)
SpatialMaxPooling ['kw', 'kh', 'dw', 'dh', 'pad_w', 'pad_h'] ['NoD', 'NoD', 'NoD', 'NoD', '0', '0']
SpatialShareConvolution.scala
object SpatialShareConvolution {
  def apply[@specialized(Float, Double) T: ClassTag](
    nInputPlane: Int,
    nOutputPlane: Int,
    kernelW: Int,
    kernelH: Int,
    strideW: Int = 1,
    strideH: Int = 1,
    padW: Int = 0,
    padH: Int = 0,
    nGroup: Int = 1,
    propagateBack: Boolean = true,
    initMethod: InitializationMethod = Default)
    (implicit ev: TensorNumeric[T]): SpatialShareConvolution[T] = {
    new SpatialShareConvolution[T](nInputPlane, nOutputPlane, kernelW, kernelH,
      strideW, strideH, padW, padH, nGroup, propagateBack, initMethod)
  }
}

(
    nInputPlane: Int,
    nOutputPlane: Int,
    kernelW: Int,
    kernelH: Int,
    strideW: Int = 1,
    strideH: Int = 1,
    padW: Int = 0,
    padH: Int = 0,
    nGroup: Int = 1,
    propagateBack: Boolean = true,
    initMethod: InitializationMethod = Default)
SpatialShareConvolution ['n_input_plane', 'n_output_plane', 'kernel_w', 'kernel_h', 'stride_w', 'stride_h', 'pad_w', 'pad_h', 'n_group', 'propagate_back', 'init_method'] ['NoD', 'NoD', 'NoD', 'NoD', '1', '1', '0', '0', '1', 'True', 'default']
SpatialSubtractiveNormalization.scala
object SpatialSubtractiveNormalization {
  def apply[@specialized(Float, Double) T: ClassTag](
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null)(
      implicit ev: TensorNumeric[T]) : SpatialSubtractiveNormalization[T] = {
    new SpatialSubtractiveNormalization[T](nInputPlane, kernel)
  }
}

(
      nInputPlane: Int = 1,
      kernel: Tensor[T] = null)
SpatialSubtractiveNormalization ['n_input_plane', 'kernel'] ['NoD', '1']
SpatialZeroPadding.scala
object SpatialZeroPadding {
  def apply[@specialized(Float, Double) T: ClassTag](
      padLeft: Int,
      padRight: Int,
      padTop: Int,
      padBottom: Int)(implicit ev: TensorNumeric[T]) : SpatialZeroPadding[T] = {
    new SpatialZeroPadding[T](padLeft, padRight, padTop, padBottom)
  }
}

(
      padLeft: Int,
      padRight: Int,
      padTop: Int,
      padBottom: Int)
SpatialZeroPadding ['pad_left', 'pad_right', 'pad_top', 'pad_bottom'] ['NoD', 'NoD', 'NoD', 'NoD']
Sqrt.scala
object Sqrt {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Sqrt[T] = {
    new Sqrt[T]()
  }
}

()
Sqrt [] []
Square.scala
object Square {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Square[T] = {
    new Square[T]()
  }
}

()
Square [] []
Squeeze.scala
object Squeeze {
  def apply[@specialized(Float, Double) T: ClassTag](
      dim : Int = Int.MinValue,
      numInputDims: Int = Int.MinValue)(implicit ev: TensorNumeric[T]) : Squeeze[T] = {
    new Squeeze[T](dim, numInputDims)
  }
}

(
      dim : Int = Int.MinValue,
      numInputDims: Int = Int.MinValue)
Squeeze ['num_input_dims'] ['Int.MinValue']
Sum.scala
object Sum {
  def apply[@specialized(Float, Double) T: ClassTag](
      dimension: Int = 1,
      nInputDims: Int = -1,
      sizeAverage: Boolean = false)(implicit ev: TensorNumeric[T]) : Sum[T] = {
    new Sum[T](dimension, nInputDims, sizeAverage)
  }
}

(
      dimension: Int = 1,
      nInputDims: Int = -1,
      sizeAverage: Boolean = false)
Sum ['dimension', 'n_input_dims', 'size_average'] ['1', '-1', 'False']
Tanh.scala
object Tanh {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : Tanh[T] = {
    new Tanh[T]()
  }
}


()
Tanh [] []
TanhShrink.scala
object TanhShrink {
  def apply[@specialized(Float, Double) T: ClassTag]()
      (implicit ev: TensorNumeric[T]) : TanhShrink[T] = {
    new TanhShrink[T]()
  }
}

()
TanhShrink [] []
Threshold.scala
object Threshold {
  def apply[@specialized(Float, Double) T: ClassTag](
      th: Double = 1e-6,
      v: Double = 0.0,
      ip: Boolean = false)(implicit ev: TensorNumeric[T]) : Threshold[T] = {
    new Threshold[T](th, v, ip)
  }
}

(
      th: Double = 1e-6,
      v: Double = 0.0,
      ip: Boolean = false)
Threshold ['th', 'v', 'ip'] ['1e-6', '0.0', 'False']
TimeDistributed.scala
object TimeDistributed {
  def apply[@specialized(Float, Double) T: ClassTag]()
  (implicit ev: TensorNumeric[T]): TimeDistributed[T] = {
    new TimeDistributed[T]()
  }
}

()
TimeDistributed [] []
TimeDistributedCriterion.scala
object TimeDistributedCriterion {
  def apply[@specialized(Float, Double) T: ClassTag](
    critrn: TensorCriterion[T] = null)
  (implicit ev: TensorNumeric[T]) : TimeDistributedCriterion[T] = {
    new TimeDistributedCriterion[T](critrn)
  }
}

(
    critrn: TensorCriterion[T] = null)
TimeDistributedCriterion ['critrn'] ['NoD']
Transpose.scala
object Transpose {
  def apply[@specialized(Float, Double) T: ClassTag](
      permutations: Array[(Int, Int)])(implicit ev: TensorNumeric[T]) : Transpose[T] = {
    new Transpose[T](permutations)
  }
}

(
      permutations: Array[(Int, Int)
Transpose ['permutations'] ['NoD']
Unsqueeze.scala
object Unsqueeze {
  def apply[@specialized(Float, Double) T: ClassTag](
      pos: Int,
      numInputDims: Int = Int.MinValue)(implicit ev: TensorNumeric[T]) : Unsqueeze[T] = {
    new Unsqueeze[T](pos, numInputDims)
  }
}

(
      pos: Int,
      numInputDims: Int = Int.MinValue)
Unsqueeze ['pos', 'num_input_dims'] ['NoD', 'Int.MinValue']
Utils.scala
object Utils {
  /**
   * This method recursively keep the shape of the source table `t2` and
   * set the elements of each tensor to zero, saving the result on the destination
   * table `t1`
   * Notice that `t1` and `t2` can only contain tables or tensors
   * @param t1 is the destination table
   * @param t2 is the source table
   * @return
   */
  def zeroTableCopy[T : ClassTag](t1: Table, t2: Table)(
    implicit ev: TensorNumeric[T]): Table = {
    t2.foreach { case ((k: Any, v: Any)) =>
      if (v.isInstanceOf[Table]) {
        t1.update(k, zeroTableCopy(if (t1.contains(k)) t1(k) else T(), t2(k)))
      } else {
        require(v.isInstanceOf[Tensor[T]], "Input can only consist of Tensor or Table")
        val tensorV = v.asInstanceOf[Tensor[T]]
        if (!t1.contains(k)) {
          t1.update(k, tensorV.clone().zero())
        } else {
          t1[Tensor[T]](k).resizeAs(tensorV)
          t1[Tensor[T]](k).zero()
        }
      }
    }
    t1.foreach { case ((k: Any, v: Any)) =>
      if (!t2.contains(k)) {
        t1.update(k, null)
      }
    }

    t1
  }

  /**
   * Resize table target as table src.
 *
   * @param target
   * @param src
   */
  def recursiveResizeAs[T : ClassTag](target : Activity, src: Activity)(
    implicit ev: TensorNumeric[T]): Activity = {
    var result: Activity = null
    if (src.isInstanceOf[Table]) {
      val srcTable = src.toTable
      result = if (null == target) {
        T()
      } else if (target.isInstanceOf[Tensor[T]]) {
        T(target)
      } else {
        target
      }

      val resultTable = result.toTable
      var i = 1
      while (i <= src.toTable.length()) {
        if (resultTable.contains(i)) {
          resultTable(i) = recursiveResizeAs(resultTable(i), srcTable(i))
        } else {
          resultTable(i) = recursiveResizeAs(null, srcTable(i))
        }
        i += 1
      }
      while (i <= resultTable.length()) {
        resultTable.remove(i)
        i += 1
      }
    } else if (src.isInstanceOf[Tensor[T]]) {
      result = if (target.isInstanceOf[Tensor[T]]) {
        target
      } else {
        Tensor[T]()
      }
      result.toTensor[T].resizeAs(src.toTensor)
    }
    result
  }

  /**
   * Apply function 'func' on all tensor in the table.
 *
   * @param x
   * @param func
   */
  def recursiveTensorApply1[T](x: Activity, func: Tensor[T] => Tensor[T])(
    implicit ev: TensorNumeric[T]): Unit = {
    require(x.isInstanceOf[Activity],
      s"expecting tensors or tables thereof. Got ${x} instead"
    )
    if (x.isInstanceOf[Table]) {
      var i = 1
      while (i <= x.toTable.length()) {
        recursiveTensorApply1(x.toTable(i), func)
        i += 1
      }
    } else {
      func(x.toTensor[T])
    }
  }

  /**
   * Apply function 'func' on each tensor in table x and table y recursively.
   *
   * Table x should have the same size with table y.
   *
   * @param x
   * @param y
   * @param func
   * @return
   */
  def recursiveTensorApply2[T](x: Activity, y: Activity,
    func: (Tensor[T], Tensor[T]) => Tensor[T])(implicit ev: TensorNumeric[T]): Activity = {
    if (y.isInstanceOf[Tensor[T]] && x.isInstanceOf[Tensor[T]]) {
      require(x.toTensor[T].nElement() == y.toTensor[T].nElement(),
        "x, y should have the same size")
      func(x.toTensor[T], y.toTensor[T])
    } else {
      require(x.isInstanceOf[Table] && y.isInstanceOf[Table], "x, y should have the same size")
      require(x.toTable.length() == y.toTable.length(), "x, y should have the same size")
      var i = 1
      while (i <= x.toTable.length()) {
        recursiveTensorApply2[T](x.toTable(i), y.toTable(i), func)
        i += 1
      }
    }
    x
  }

  /**
   * Apply a add operation on table x and table y one by one.
   * y := y + alpha * x
   *
   * Table x should have the same size with y.
   *
   * @param y
   * @param alpha
   * @param x
   * @tparam T: Float or Double
   * @return y
   */
  def recursiveAdd[T](y: Activity, alpha: Double = 1.0, x: Activity )(
    implicit ev: TensorNumeric[T]): Activity = {
    recursiveTensorApply2[T](y, x, (t1, t2) => t1.add(ev.fromType[Double](alpha), t2))
    y
  }

  /**
   * copy table x's tensor to table y.
   *
   * Table x should have the same size with y.
   *
   * @param y
   * @param x
   * @tparam T: Float or Double
   * @return y
   */
  def recursiveCopy[T](y: Activity, x: Activity )(
    implicit ev: TensorNumeric[T]): Activity = {
    recursiveTensorApply2[T](y, x, (t1, t2) => t1.copy(t2))
    y
  }

  /**
   * Fill the value to each Tensor in the table recursively
 *
   * @param x
   * @param value
   */
  def recursiveFill[T](x: Activity, value : Double)(
    implicit ev: TensorNumeric[T]): Unit = {
    recursiveTensorApply1[T](x, t => t.fill(ev.fromType[Double](value)))
  }

  /**
   * get all modules and map by name
   *
   * @param model
   * @tparam T
   * @return
   */
  def getNamedModules[T](model: Module[T]): Map[String, Module[T]] = {
    var namedModules: Map[String, Module[T]] = Map()
    def getModules(module: Module[T]): Unit = {
      module match {
        case m: Container[_, _, T] =>
          for (m <- module.asInstanceOf[Container[_, _, T]].modules) getModules(m)
        case _ => namedModules += (module.getName() -> module)
      }
    }
    getModules(model)
    namedModules
  }

  /**
   * copy src's parameters and running status to dst
   * @param src source model
   * @param dst destination model
   */
  def copyModule[T](src: Module[T], dst: Module[T]): Module[T] = {
    // copy parameters
    val srcParameters = src.getParameters()._1
    val dstParameters = dst.getParameters()._1
    require(srcParameters.size(1) == dstParameters.size(1),
      s"$src and $dst is not the same type.")
    dstParameters.copy(srcParameters)
    // copy running status
    dst.copyStatus(src)
    dst
  }

  /**
   * get the inner loop size and outer loop size given a pivot dim
   * @param pivotDim is the dim whose value larger than 1
   * @return inner loop size and outer loop size
   */
  private[nn] def getInnerOuterNum[T](pivotDim: Int, data: Tensor[T]): (Int, Int) = {
    var k = 1
    var outerNum = 1
    while (k < pivotDim) {
      outerNum *= data.size(k)
      k += 1
    }
    var innerNum = 1
    k = pivotDim + 1
    while (k <= data.dim()) {
      innerNum *= data.size(k)
      k += 1
    }
    (innerNum, outerNum)
  }

  /**
   * if there is only one dim of size > 1, return this dim(count from 1)
   * else return -1
   * e.g. (1, 2, 1, 1) returns 1, (1, 2, 3, 1) returns -1, and (1, 1, 1, 1) returns -1
   * @param size size of tensor
   * @return (the only dim whose value > 1) else (-1)
   */
  private[nn] def getOnlyDimGtOne(size: Array[Int]): Int = {
    var i = 0
    var count = 0
    var pivot = 0
    while (i < size.length) {
      if (size(i) > 1) {
        count += 1
        pivot = i + 1
      }
      i += 1
    }
    if (count == 1) pivot else -1
  }
}

(
    implicit ev: TensorNumeric[T])
Utils ['ev'] ['NoD']
View.scala
object View {
  def apply[@specialized(Float, Double) T: ClassTag](
      sizes: Int*)(implicit ev: TensorNumeric[T]) : View[T] = {
    new View[T](sizes.toArray)
  }

  def apply[@specialized(Float, Double) T: ClassTag](
      sizes: Array[Int])(implicit ev: TensorNumeric[T]) : View[T] = {
    new View[T](sizes)
  }
}

(
      sizes: Int*)
View ['sizes'] ['NoD']
abstractnn
['Abs.scala', 'AbsCriterion.scala', 'Add.scala', 'AddConstant.scala', 'BCECriterion.scala', 'BatchNormalization.scala', 'Bilinear.scala', 'Bottle.scala', 'CAdd.scala', 'CAddTable.scala', 'CDivTable.scala', 'CMaxTable.scala', 'CMinTable.scala', 'CMul.scala', 'CMulTable.scala', 'CSubTable.scala', 'Clamp.scala', 'ClassNLLCriterion.scala', 'ClassSimplexCriterion.scala', 'Concat.scala', 'ConcatTable.scala', 'Contiguous.scala', 'Copy.scala', 'Cosine.scala', 'CosineDistance.scala', 'CosineEmbeddingCriterion.scala', 'CriterionTable.scala', 'CrossEntropyCriterion.scala', 'DistKLDivCriterion.scala', 'DotProduct.scala', 'Dropout.scala', 'ELU.scala', 'Echo.scala', 'ErrorInfo.scala', 'Euclidean.scala', 'Exp.scala', 'FlattenTable.scala', 'GradientReversal.scala', 'HardShrink.scala', 'HardTanh.scala', 'HingeEmbeddingCriterion.scala', 'Identity.scala', 'Index.scala', 'InferReshape.scala', 'JoinTable.scala', 'L1Cost.scala', 'L1HingeEmbeddingCriterion.scala', 'L1Penalty.scala', 'LeakyReLU.scala', 'Linear.scala', 'Log.scala', 'LogSigmoid.scala', 'LogSoftMax.scala', 'LookupTable.scala', 'MM.scala', 'MSECriterion.scala', 'MV.scala', 'MapTable.scala', 'MarginCriterion.scala', 'MarginRankingCriterion.scala', 'MaskedSelect.scala', 'Max.scala', 'Mean.scala', 'Min.scala', 'MixtureTable.scala', 'Module.scala', 'Mul.scala', 'MulConstant.scala', 'MultiCriterion.scala', 'MultiLabelMarginCriterion.scala', 'MultiLabelSoftMarginCriterion.scala', 'MultiMarginCriterion.scala', 'NNPrimitive.scala', 'Narrow.scala', 'NarrowTable.scala', 'Normalize.scala', 'PReLU.scala', 'Padding.scala', 'PairwiseDistance.scala', 'ParallelCriterion.scala', 'ParallelTable.scala', 'Power.scala', 'RNN.scala', 'RReLU.scala', 'ReLU.scala', 'ReLU6.scala', 'Recurrent.scala', 'Replicate.scala', 'Reshape.scala', 'RoiPooling.scala', 'Scale.scala', 'Select.scala', 'Sequential.scala', 'Sigmoid.scala', 'SmoothL1Criterion.scala', 'SmoothL1CriterionWithWeights.scala', 'SoftMax.scala', 'SoftMin.scala', 'SoftPlus.scala', 'SoftShrink.scala', 'SoftSign.scala', 'SoftmaxWithCriterion.scala', 'SpatialAveragePooling.scala', 'SpatialBatchNormalization.scala', 'SpatialContrastiveNormalization.scala', 'SpatialConvolution.scala', 'SpatialConvolutionMap.scala', 'SpatialCrossMapLRN.scala', 'SpatialDilatedConvolution.scala', 'SpatialDivisiveNormalization.scala', 'SpatialFullConvolution.scala', 'SpatialMaxPooling.scala', 'SpatialShareConvolution.scala', 'SpatialSubtractiveNormalization.scala', 'SpatialZeroPadding.scala', 'Sqrt.scala', 'Square.scala', 'Squeeze.scala', 'Sum.scala', 'Tanh.scala', 'TanhShrink.scala', 'Threshold.scala', 'TimeDistributed.scala', 'TimeDistributedCriterion.scala', 'Transpose.scala', 'Unsqueeze.scala', 'Utils.scala', 'View.scala'] 128
