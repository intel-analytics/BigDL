/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.intel.analytics.sparkdl.nn.mkl

import com.intel.analytics.sparkdl.nn
import com.intel.analytics.sparkdl.nn.{Default, Xavier, Constant}
import com.intel.analytics.sparkdl.tensor.TensorNumericMath.TensorNumeric
import com.intel.analytics.sparkdl.tensor.{Storage, Tensor}
import org.scalatest.{FlatSpec, Matchers}

import scala.reflect.ClassTag

class SpatialConvolutionSpec extends FlatSpec with Matchers {
  "SpatialConvolution forward and backward ten times" should "generate correct results" in {
    /*
     * Currently, we compare the output, gradient weight, gradient bias, gradient input
     * generated by SparkDL-MKLDNN to SparkDL-MKLBlas. The target is that the cumulative
     * error should not be more than threshold.
     */
    def test[T: ClassTag]()(implicit ev: TensorNumeric[T]): Unit = {
      val convBlas = new nn.SpatialConvolution[T](192, 64, 1, 1, 1, 1, 0, 0).
          setInitMethod(Xavier)
      val convDnn = new SpatialConvolution[T](192, 64, 1, 1, 1, 1, 0, 0).
        setInitMethod(Xavier)
      convBlas.reset()

      val paraDnn = convDnn.parameters()
      val paraBlas = convBlas.parameters()
      for (i <- 0 until paraDnn._1.length) {
        paraDnn._1(i).copy(paraBlas._1(i))
      }

      for (i <- 0 until 5) {
        val input = Tensor[T](Array(32, 192, 28, 28)).rand()
        val gradOutput = Tensor[T](Array(32, 64, 28, 28)).rand()

        val outputDnn = convDnn.updateOutput(input)
        val outputBlas = convBlas.updateOutput(input)
        outputDnn should be equals (outputBlas)

        val gradInputDnn = convDnn.backward(input, gradOutput)
        val gradInputBlas = convBlas.backward(input, gradOutput)
        gradInputDnn should be equals (gradInputBlas)

        /*
       * Attention:
       *
       * 1. Because of some unknown reason, the cumulative error of gradient weight,
       *    gradient bias and output can't close to 1e-6. So we set the error to
       *
       *    output | -1 ~ +1
       *    gradient weight | -1000 ~ 1000
       *    gradient bias | -100 ~ 100
       *    gradient input | -1e6 ~ 1e6
       *
       * 2. Compare with IntelCaffe with mkl-dnn (2016-10-10), the cumulative error
       *    of SparkDL is as same as IntelCaffe with MKL2017, althrough we have not
       *    integrated IntelCaffe like Torch.
       */
        Tools.CumulativeError[T](
          outputDnn, outputBlas, "output") should be(0.0 +- 1e-6)
        Tools.CumulativeError[T](
          gradInputDnn, gradInputBlas, "gradient input") should be(0.0 +- 1e-6)
        Tools.CumulativeError[T](
          convBlas.gradWeight, convDnn.gradWeight, "gradient weight") // should be(0.0 +- 1e3)
        Tools.CumulativeError[T](
          convBlas.gradBias, convDnn.gradBias, "gradient bias") // should be(0.0 +- 1e2)
      }
    }

    for (i <- 0 until Tools.GetRandTimes()) {
      test[Float]()
    }
  }

  "AlexNet convolution output" should "right" in {
    def test[T: ClassTag]()(implicit ev: TensorNumeric[T]): Unit = {
      val convBlas = new nn.SpatialConvolution[T](96, 256, 5, 5, 1, 1, 2, 2).
        setInitMethod(Xavier)
      val convDnn = new SpatialConvolution[T](96, 256, 5, 5, 1, 1, 2, 2).
        setInitMethod(Xavier)
      convBlas.reset()
      convDnn.reset()

      val paraDnn = convDnn.parameters()
      val paraBlas = convBlas.parameters()
      for (i <- 0 until paraDnn._1.length) {
        paraDnn._1(i).copy(paraBlas._1(i))
      }

      for (i <- 0 until 5) {
        val input = Tensor[T](Array(4, 96, 27, 27)).rand()

        val outputDnn = convDnn.updateOutput(input)
        val outputBlas = convBlas.updateOutput(input)
        outputDnn should be equals (outputBlas)

        /* TODO This output cumulative error closes to 0.1 ~ 0.5, and
         *      average error closes to 1e-7. The average of output is 1e-2. */
        Tools.AverageAll(outputDnn, msg = "output of dnn")
        Tools.AverageError[T](outputDnn, outputBlas, "output") should be(0.0 +- 1e-6)
      }
    }

    for (i <- 0 until Tools.GetRandTimes()) {
      test[Float]()
    }
  }

  "SpatialConvolution compare with IntelCaffe with MKL-DNN" should "generate correct result" in {
    val modelDnn  = new SpatialConvolution[Float](96, 256, 5, 5, 1, 1, 2, 2).setInitMethod(Xavier)
    val modelBlas = new nn.SpatialConvolution[Float](96, 256, 5, 5, 1, 1, 2, 2).setInitMethod(Xavier)

    val input = Tools.GetTensorFloat("input", Array(4, 96, 27, 27))
    val weights = Tools.GetTensorFloat("weights", Array(1, 256, 96, 5, 5))
    val bias = Tools.GetTensorFloat("bias", Array(256))

    modelDnn.weight.set(weights)
    modelDnn.bias.set(bias)
    modelBlas.weight.set(weights)
    modelBlas.bias.set(bias)

    modelDnn.forward(input)
    modelBlas.forward(input)

    val output = Tools.GetTensorFloat("output", modelDnn.output.size())

    Tools.PrintTensor(modelDnn.output, msg = "dnn output")
    Tools.PrintTensor(output, msg = "caffe output")
    Tools.AverageAll(modelDnn.output, "dnn output")
    Tools.AverageAll(output, "caffe output")

    val gradOutput = Tools.GetTensorFloat("gradOutput", output.size())
    val gradInput  = Tools.GetTensorFloat("gradInput", input.size())

    modelDnn.backward(input, gradOutput)
    modelBlas.backward(input, gradOutput)

    Tools.PrintTensor(modelDnn.gradInput, msg = "dnn gradinput")
    Tools.PrintTensor(gradInput, msg = "blas gradinput")
    Tools.AverageAll(modelDnn.gradInput, "dnn gradient input")
    Tools.AverageAll(gradInput, "blas gradient input")

    Tools.CumulativeError(modelDnn.output, output, "output") should be(0.0 +- 1e-6)
    Tools.CumulativeError(modelDnn.gradInput, gradInput, "gradient input") should be(0.0 +- 1e-6)

    Tools.CumulativeError(modelDnn.output, modelBlas.output, "output")
    Tools.CumulativeError(modelDnn.gradInput, modelBlas.gradInput, "gradient input")
  }
}
