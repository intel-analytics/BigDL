/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.intel.analytics.sparkdl.nn.mkl

import com.intel.analytics.sparkdl.nn
import com.intel.analytics.sparkdl.nn.{Default, Xavier, Constant}
import com.intel.analytics.sparkdl.tensor.TensorNumericMath.TensorNumeric
import com.intel.analytics.sparkdl.tensor.{Storage, Tensor}
import org.scalatest.{FlatSpec, Matchers}

import scala.reflect.ClassTag

class SpatialConvolutionSpec extends FlatSpec with Matchers {
  "SpatialConvolution forward and backward ten times" should "generate correct results" in {
    /*
     * Currently, we compare the output, gradient weight, gradient bias, gradient input
     * generated by SparkDL-MKLDNN to SparkDL-MKLBlas. The target is that the cumulative
     * error should not be more than threshold.
     */
    def test[T: ClassTag]()(implicit ev: TensorNumeric[T]): Unit = {
      val convBlas = new nn.SpatialConvolution[T](192, 64, 1, 1, 1, 1, 0, 0).
          setInitMethod(Xavier)
      val convDnn = new SpatialConvolution[T](192, 64, 1, 1, 1, 1, 0, 0).
        setInitMethod(Xavier)
      convBlas.reset()

      val paraDnn = convDnn.parameters()
      val paraBlas = convBlas.parameters()
      for (i <- 0 until paraDnn._1.length) {
        paraDnn._1(i).copy(paraBlas._1(i))
      }

      val input = Tensor[T](Array(32, 192, 28, 28)).rand()
      val gradOutput = Tensor[T](Array(32, 64, 28, 28)).rand()

      val outputDnn = convDnn.updateOutput(input)
      val outputBlas = convBlas.updateOutput(input)
      outputDnn should be equals (outputBlas)

      val gradInputDnn = convDnn.backward(input, gradOutput)
      val gradInputBlas = convBlas.backward(input, gradOutput)
      gradInputDnn should be equals (gradInputBlas)

      /*
       * Attention:
       *
       * 1. Because of some unknown reason, the cumulative error of gradient weight,
       *    gradient bias and output can't close to 1e-6. So we set the error to
       *
       *    output | -1 ~ +1
       *    gradient weight | -1000 ~ 1000
       *    gradient bias | -100 ~ 100
       *    gradient input | -1e6 ~ 1e6
       *
       * 2. Compare with IntelCaffe with mkl-dnn (2016-10-10), the cumulative error
       *    of SparkDL is as same as IntelCaffe with MKL2017, althrough we have not
       *    integrated IntelCaffe like Torch.
       */
      Tools.CumulativeError[T](
        outputDnn,outputBlas, "output") should be(0.0 +- 1)
      Tools.CumulativeError[T](
        gradInputDnn, gradInputBlas, "gradient input") should be(0.0 +- 1e-6)
      Tools.CumulativeError[T](
        convBlas.gradWeight, convDnn.gradWeight, "gradient weight") should be(0.0 +- 1e3)
      Tools.CumulativeError[T](
        convBlas.gradBias, convDnn.gradBias, "gradient bias") should be(0.0 +- 1e2)
    }

    for (i <- 0 until Tools.GetRandTimes()) {
      test[Float]()
    }
  }
}
